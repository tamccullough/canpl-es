{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todd McCullough\n",
    "# 2020\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game      team    p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1  \\\n",
      "0   I1  Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48   \n",
      "1   I1  York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70   \n",
      "\n",
      "    p10  p11  p12  p13  r  s  \n",
      "0  0.42  0.0  0.0  0.0  2  1  \n",
      "1  0.47  0.0  0.0  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')\n",
    "print(results.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pump_it_up(results)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "'''y = db.pop('s')\n",
    "db.pop('r')'''\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
       "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
       "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
       "\n",
       "   p13  r  s  \n",
       "0  0.0  2  1  \n",
       "1  0.0  2  1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
      "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
      "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
      "\n",
      "   p13  r  s  \n",
      "0  0.0  2  1  \n",
      "1  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  \\\n",
       "12  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0   \n",
       "43  0.79  0.91  0.75  0.37  0.33  0.87  0.66  0.66  0.62  0.73  0.64  0.0   \n",
       "\n",
       "    p12  p13  r  s  \n",
       "12  0.0  0.0  3  3  \n",
       "43  0.0  0.0  3  3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high = X[X['s'] > 2]\n",
    "high.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_y = high.pop('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
       "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
       "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
       "\n",
       "   p13  r  s  \n",
       "0  0.0  2  1  \n",
       "1  0.0  2  1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = X[X['s'] <= 2]\n",
    "low.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y = low.pop('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 15) (3152, 15)\n"
     ]
    }
   ],
   "source": [
    "print(high.shape,low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_X_train, h_X_test, h_y_train, h_y_test = train_test_split(high, h_y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_X_train, l_X_test, l_y_train, l_y_test = train_test_split(low, l_y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_X_train = pd.concat([l_X_train,h_X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_X_test = pd.concat([l_X_test,h_X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_X_train.shape[0] + l_X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y_train = pd.concat([l_y_train,h_y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y_test = pd.concat([l_y_test,h_y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_y_train.shape[0] + l_y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1556"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_y_test.shape[0] + l_X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = l_X_train, l_X_test, l_y_train, l_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3105, 15)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(778, 15)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.pop('r')\n",
    "X_test.pop('r')\n",
    "X_train.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model\n",
      "\n",
      "RMSE:  1.236319605381782\n",
      "\n",
      "Score 1.19\n",
      "\n",
      "Decision Tree Regression Model\n",
      "\n",
      "RMSE:  1.3045779271254694\n",
      "\n",
      "Score -10.02\n",
      "\n",
      "Random Forest Regression Model\n",
      "\n",
      "RMSE:  1.2723227667684853\n",
      "\n",
      "Score -4.65\n",
      "\n",
      "Voting Regressor Model\n",
      "\n",
      "RMSE:  1.2482340232385531\n",
      "\n",
      "Score -0.72\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model\n",
    "def linearRegression():\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "lr = linearRegression()\n",
    "\n",
    "print('\\nLinear Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n",
    "print('\\nScore',round(lr.score(X_test, y_test)*100,2))\n",
    "\n",
    "#DecisionTreeRegressor\n",
    "def decisionTree():\n",
    "    model = DecisionTreeRegressor(criterion='mse', splitter='random', max_depth=8, max_features='log2')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "dt = decisionTree()\n",
    "\n",
    "print('\\nDecision Tree Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test, dt.predict(X_test))))\n",
    "print('\\nScore',round(dt.score(X_test, y_test)*100,2))\n",
    "\n",
    "#Random Forest Regression\n",
    "def forestRegression():\n",
    "    model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "rf = forestRegression()\n",
    "\n",
    "print('\\nRandom Forest Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,rf.predict(X_test))))\n",
    "print('\\nScore',round(rf.score(X_test, y_test)*100,2))\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "vr = VotingRegressor(estimators=[('lr', lr), ('dt', dt), ('rf', rf)])\n",
    "vr = vr.fit(X_train, y_train)\n",
    "\n",
    "print('\\nVoting Regressor Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,vr.predict(X_test))))\n",
    "print('\\nScore',round(vr.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasSequential():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                optimizer = tf.keras.optimizers.RMSprop(0.1),\n",
    "                metrics = ['mae', 'mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,185\n",
      "Trainable params: 5,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ks = kerasSequential()\n",
    "print(ks.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2484 samples, validate on 621 samples\n",
      "Epoch 1/450\n",
      "2484/2484 [==============================] - 1s 541us/sample - loss: 4545.4927 - mae: 16.1353 - mse: 4545.4932 - val_loss: 10.5876 - val_mae: 3.1032 - val_mse: 10.5876\n",
      "Epoch 2/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.9820 - mae: 0.8417 - mse: 0.9820 - val_loss: 8.0529 - val_mae: 2.6826 - val_mse: 8.0529\n",
      "Epoch 3/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.6525 - mae: 0.6789 - mse: 0.6525 - val_loss: 6.4753 - val_mae: 2.3834 - val_mse: 6.4753\n",
      "Epoch 4/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6059 - mae: 0.6418 - mse: 0.6059 - val_loss: 6.1776 - val_mae: 2.3241 - val_mse: 6.1776\n",
      "Epoch 5/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6029 - mae: 0.6359 - mse: 0.6029 - val_loss: 6.2491 - val_mae: 2.3394 - val_mse: 6.2491\n",
      "Epoch 6/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.6148 - mae: 0.6434 - mse: 0.6148 - val_loss: 9.3069 - val_mae: 2.9009 - val_mse: 9.3069\n",
      "Epoch 7/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.6856 - mae: 0.7001 - mse: 0.6856 - val_loss: 4.5210 - val_mae: 1.9593 - val_mse: 4.5210\n",
      "Epoch 8/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.6742 - mae: 0.6935 - mse: 0.6742 - val_loss: 6.4175 - val_mae: 2.3737 - val_mse: 6.4175\n",
      "Epoch 9/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.6337 - mae: 0.6594 - mse: 0.6337 - val_loss: 5.9279 - val_mae: 2.2747 - val_mse: 5.9279\n",
      "Epoch 10/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6579 - mae: 0.6739 - mse: 0.6579 - val_loss: 7.5343 - val_mae: 2.5913 - val_mse: 7.5343\n",
      "Epoch 11/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6070 - mae: 0.6260 - mse: 0.6070 - val_loss: 5.2406 - val_mae: 2.1281 - val_mse: 5.2406\n",
      "Epoch 12/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6075 - mae: 0.6242 - mse: 0.6075 - val_loss: 6.2179 - val_mae: 2.3337 - val_mse: 6.2179\n",
      "Epoch 13/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.0019 - mae: 0.7657 - mse: 1.0019 - val_loss: 6.4948 - val_mae: 2.3892 - val_mse: 6.4948\n",
      "Epoch 14/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5993 - mae: 0.6088 - mse: 0.5993 - val_loss: 6.1413 - val_mae: 2.3183 - val_mse: 6.1413\n",
      "Epoch 15/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.7840 - mae: 0.6759 - mse: 0.7840 - val_loss: 6.5236 - val_mae: 2.3951 - val_mse: 6.5236\n",
      "Epoch 16/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5997 - mae: 0.6133 - mse: 0.5997 - val_loss: 6.3794 - val_mae: 2.3658 - val_mse: 6.3794\n",
      "Epoch 17/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6022 - mae: 0.6171 - mse: 0.6022 - val_loss: 5.8644 - val_mae: 2.2618 - val_mse: 5.8644\n",
      "Epoch 18/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6106 - mae: 0.6249 - mse: 0.6106 - val_loss: 6.9696 - val_mae: 2.4837 - val_mse: 6.9696\n",
      "Epoch 19/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.6031 - mae: 0.6210 - mse: 0.6031 - val_loss: 5.7118 - val_mae: 2.2299 - val_mse: 5.7118\n",
      "Epoch 20/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.6275 - mae: 0.6365 - mse: 0.6275 - val_loss: 6.2232 - val_mae: 2.3348 - val_mse: 6.2232\n",
      "Epoch 21/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.6049 - mae: 0.6217 - mse: 0.6049 - val_loss: 6.5127 - val_mae: 2.3929 - val_mse: 6.5127\n",
      "Epoch 22/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 0.6025 - mae: 0.6232 - mse: 0.6025 - val_loss: 6.9631 - val_mae: 2.4824 - val_mse: 6.9632\n",
      "Epoch 23/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.6028 - mae: 0.6192 - mse: 0.6028 - val_loss: 6.0794 - val_mae: 2.3058 - val_mse: 6.0794\n",
      "Epoch 24/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.6035 - mae: 0.6227 - mse: 0.6035 - val_loss: 5.3167 - val_mae: 2.1449 - val_mse: 5.3167\n",
      "Epoch 25/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.6082 - mae: 0.6288 - mse: 0.6082 - val_loss: 5.4837 - val_mae: 2.1813 - val_mse: 5.4837\n",
      "Epoch 26/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.6063 - mae: 0.6241 - mse: 0.6063 - val_loss: 6.2025 - val_mae: 2.3306 - val_mse: 6.2025\n",
      "Epoch 27/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.6035 - mae: 0.6167 - mse: 0.6035 - val_loss: 5.8240 - val_mae: 2.2534 - val_mse: 5.8240\n",
      "Epoch 28/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.6049 - mae: 0.6222 - mse: 0.6049 - val_loss: 6.2179 - val_mae: 2.3337 - val_mse: 6.2179\n",
      "Epoch 29/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.6044 - mae: 0.6254 - mse: 0.6044 - val_loss: 6.3598 - val_mae: 2.3619 - val_mse: 6.3598\n",
      "Epoch 30/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 0.6027 - mae: 0.6162 - mse: 0.6027 - val_loss: 6.4522 - val_mae: 2.3806 - val_mse: 6.4522\n",
      "Epoch 31/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.6066 - mae: 0.6218 - mse: 0.6066 - val_loss: 6.6360 - val_mae: 2.4177 - val_mse: 6.6360\n",
      "Epoch 32/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6048 - mae: 0.6182 - mse: 0.6048 - val_loss: 6.6349 - val_mae: 2.4175 - val_mse: 6.6349\n",
      "Epoch 33/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 0.6554 - mae: 0.6449 - mse: 0.6554 - val_loss: 6.0501 - val_mae: 2.2999 - val_mse: 6.0501\n",
      "Epoch 34/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6136 - mae: 0.6334 - mse: 0.6136 - val_loss: 5.8428 - val_mae: 2.2573 - val_mse: 5.8428\n",
      "Epoch 35/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6035 - mae: 0.6240 - mse: 0.6035 - val_loss: 7.1822 - val_mae: 2.5248 - val_mse: 7.1822\n",
      "Epoch 36/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.6002 - mae: 0.6203 - mse: 0.6002 - val_loss: 6.5104 - val_mae: 2.3924 - val_mse: 6.5104\n",
      "Epoch 37/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.6046 - mae: 0.6205 - mse: 0.6046 - val_loss: 6.1301 - val_mae: 2.3161 - val_mse: 6.1301\n",
      "Epoch 38/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6030 - mae: 0.6205 - mse: 0.6030 - val_loss: 5.7280 - val_mae: 2.2333 - val_mse: 5.7280\n",
      "Epoch 39/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6048 - mae: 0.6205 - mse: 0.6048 - val_loss: 6.0531 - val_mae: 2.3005 - val_mse: 6.0531\n",
      "Epoch 40/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.6117 - mae: 0.6214 - mse: 0.6117 - val_loss: 6.4126 - val_mae: 2.3725 - val_mse: 6.4126\n",
      "Epoch 41/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6022 - mae: 0.6173 - mse: 0.6022 - val_loss: 7.2801 - val_mae: 2.5434 - val_mse: 7.2801\n",
      "Epoch 42/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.6050 - mae: 0.6204 - mse: 0.6050 - val_loss: 6.5816 - val_mae: 2.4068 - val_mse: 6.5816\n",
      "Epoch 43/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 0.6023 - mae: 0.6215 - mse: 0.6023 - val_loss: 6.2905 - val_mae: 2.3482 - val_mse: 6.2905\n",
      "Epoch 44/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.6049 - mae: 0.6211 - mse: 0.6049 - val_loss: 6.4845 - val_mae: 2.3871 - val_mse: 6.4845\n",
      "Epoch 45/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.6004 - mae: 0.6187 - mse: 0.6004 - val_loss: 6.4624 - val_mae: 2.3826 - val_mse: 6.4624\n",
      "Epoch 46/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.6044 - mae: 0.6250 - mse: 0.6044 - val_loss: 6.5901 - val_mae: 2.4085 - val_mse: 6.5901\n",
      "Epoch 47/450\n",
      "2484/2484 [==============================] - 0s 101us/sample - loss: 0.6013 - mae: 0.6203 - mse: 0.6013 - val_loss: 6.2223 - val_mae: 2.3346 - val_mse: 6.2223\n",
      "Epoch 48/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 95us/sample - loss: 0.6025 - mae: 0.6183 - mse: 0.6025 - val_loss: 6.4019 - val_mae: 2.3703 - val_mse: 6.4019\n",
      "Epoch 49/450\n",
      "2484/2484 [==============================] - 0s 82us/sample - loss: 0.6035 - mae: 0.6226 - mse: 0.6035 - val_loss: 6.2005 - val_mae: 2.3302 - val_mse: 6.2005\n",
      "Epoch 50/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.6017 - mae: 0.6181 - mse: 0.6017 - val_loss: 7.5953 - val_mae: 2.6026 - val_mse: 7.5953\n",
      "Epoch 51/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6061 - mae: 0.6203 - mse: 0.6061 - val_loss: 6.3927 - val_mae: 2.3684 - val_mse: 6.3927\n",
      "Epoch 52/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.6068 - mae: 0.6222 - mse: 0.6068 - val_loss: 6.0254 - val_mae: 2.2949 - val_mse: 6.0254\n",
      "Epoch 53/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.5980 - mae: 0.6185 - mse: 0.5980 - val_loss: 5.4179 - val_mae: 2.1670 - val_mse: 5.4179\n",
      "Epoch 54/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.6076 - mae: 0.6235 - mse: 0.6076 - val_loss: 6.0367 - val_mae: 2.2971 - val_mse: 6.0367\n",
      "Epoch 55/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.6053 - mae: 0.6200 - mse: 0.6053 - val_loss: 5.8336 - val_mae: 2.2554 - val_mse: 5.8336\n",
      "Epoch 56/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6028 - mae: 0.6154 - mse: 0.6028 - val_loss: 6.8816 - val_mae: 2.4665 - val_mse: 6.8816\n",
      "Epoch 57/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6080 - mae: 0.6234 - mse: 0.6080 - val_loss: 5.8027 - val_mae: 2.2489 - val_mse: 5.8027\n",
      "Epoch 58/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6063 - mae: 0.6213 - mse: 0.6063 - val_loss: 6.0462 - val_mae: 2.2991 - val_mse: 6.0462\n",
      "Epoch 59/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6058 - mae: 0.6242 - mse: 0.6058 - val_loss: 5.7113 - val_mae: 2.2298 - val_mse: 5.7113\n",
      "Epoch 60/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.6028 - mae: 0.6220 - mse: 0.6028 - val_loss: 6.6054 - val_mae: 2.4116 - val_mse: 6.6054\n",
      "Epoch 61/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 0.6034 - mae: 0.6202 - mse: 0.6034 - val_loss: 6.2904 - val_mae: 2.3482 - val_mse: 6.2904\n",
      "Epoch 62/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 0.6012 - mae: 0.6180 - mse: 0.6012 - val_loss: 5.4126 - val_mae: 2.1659 - val_mse: 5.4126\n",
      "Epoch 63/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6033 - mae: 0.6241 - mse: 0.6033 - val_loss: 6.2770 - val_mae: 2.3455 - val_mse: 6.2770\n",
      "Epoch 64/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6093 - mae: 0.6248 - mse: 0.6093 - val_loss: 6.2006 - val_mae: 2.3303 - val_mse: 6.2006\n",
      "Epoch 65/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.6207 - mae: 0.6318 - mse: 0.6207 - val_loss: 7.0215 - val_mae: 2.4938 - val_mse: 7.0215\n",
      "Epoch 66/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6043 - mae: 0.6188 - mse: 0.6043 - val_loss: 6.6611 - val_mae: 2.4228 - val_mse: 6.6611\n",
      "Epoch 67/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.6042 - mae: 0.6195 - mse: 0.6042 - val_loss: 6.8553 - val_mae: 2.4614 - val_mse: 6.8553\n",
      "Epoch 68/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.6019 - mae: 0.6210 - mse: 0.6019 - val_loss: 6.7505 - val_mae: 2.4405 - val_mse: 6.7505\n",
      "Epoch 69/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.6038 - mae: 0.6246 - mse: 0.6038 - val_loss: 6.1326 - val_mae: 2.3166 - val_mse: 6.1326\n",
      "Epoch 70/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.6016 - mae: 0.6261 - mse: 0.6016 - val_loss: 6.5674 - val_mae: 2.4041 - val_mse: 6.5674\n",
      "Epoch 71/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.6015 - mae: 0.6153 - mse: 0.6015 - val_loss: 5.8829 - val_mae: 2.2656 - val_mse: 5.8829\n",
      "Epoch 72/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6094 - mae: 0.6253 - mse: 0.6094 - val_loss: 6.4077 - val_mae: 2.3716 - val_mse: 6.4077\n",
      "Epoch 73/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6129 - mae: 0.6253 - mse: 0.6129 - val_loss: 5.9707 - val_mae: 2.2837 - val_mse: 5.9707\n",
      "Epoch 74/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.6013 - mae: 0.6225 - mse: 0.6013 - val_loss: 6.7585 - val_mae: 2.4424 - val_mse: 6.7585\n",
      "Epoch 75/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.6107 - mae: 0.6257 - mse: 0.6107 - val_loss: 6.5471 - val_mae: 2.3998 - val_mse: 6.5471\n",
      "Epoch 76/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.6027 - mae: 0.6137 - mse: 0.6027 - val_loss: 6.4336 - val_mae: 2.3768 - val_mse: 6.4336\n",
      "Epoch 77/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 0.6033 - mae: 0.6166 - mse: 0.6033 - val_loss: 6.1172 - val_mae: 2.3135 - val_mse: 6.1172\n",
      "Epoch 78/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6001 - mae: 0.6178 - mse: 0.6001 - val_loss: 6.4401 - val_mae: 2.3782 - val_mse: 6.4401\n",
      "Epoch 79/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6346 - mae: 0.6376 - mse: 0.6346 - val_loss: 6.0993 - val_mae: 2.3100 - val_mse: 6.0993\n",
      "Epoch 80/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6015 - mae: 0.6225 - mse: 0.6015 - val_loss: 5.8188 - val_mae: 2.2528 - val_mse: 5.8188\n",
      "Epoch 81/450\n",
      "2484/2484 [==============================] - ETA: 0s - loss: 0.6019 - mae: 0.6206 - mse: 0.601 - 0s 49us/sample - loss: 0.6013 - mae: 0.6216 - mse: 0.6013 - val_loss: 5.9664 - val_mae: 2.2830 - val_mse: 5.9664\n",
      "Epoch 82/450\n",
      "2484/2484 [==============================] - 0s 93us/sample - loss: 0.6019 - mae: 0.6221 - mse: 0.6019 - val_loss: 6.4990 - val_mae: 2.3902 - val_mse: 6.4990\n",
      "Epoch 83/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.6036 - mae: 0.6153 - mse: 0.6036 - val_loss: 5.6649 - val_mae: 2.2200 - val_mse: 5.6649\n",
      "Epoch 84/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.6075 - mae: 0.6246 - mse: 0.6075 - val_loss: 6.3402 - val_mae: 2.3583 - val_mse: 6.3402\n",
      "Epoch 85/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5986 - mae: 0.6194 - mse: 0.5986 - val_loss: 6.1387 - val_mae: 2.3181 - val_mse: 6.1387\n",
      "Epoch 86/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.6009 - mae: 0.6193 - mse: 0.6009 - val_loss: 6.8537 - val_mae: 2.4610 - val_mse: 6.8537\n",
      "Epoch 87/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.6013 - mae: 0.6249 - mse: 0.6013 - val_loss: 6.4849 - val_mae: 2.3873 - val_mse: 6.4849\n",
      "Epoch 88/450\n",
      "2484/2484 [==============================] - 0s 103us/sample - loss: 0.6023 - mae: 0.6228 - mse: 0.6023 - val_loss: 6.5031 - val_mae: 2.3910 - val_mse: 6.5031\n",
      "Epoch 89/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.6016 - mae: 0.6189 - mse: 0.6016 - val_loss: 6.2988 - val_mae: 2.3499 - val_mse: 6.2988\n",
      "Epoch 90/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.6031 - mae: 0.6221 - mse: 0.6031 - val_loss: 6.4774 - val_mae: 2.3858 - val_mse: 6.4774\n",
      "Epoch 91/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.5975 - mae: 0.6183 - mse: 0.5975 - val_loss: 7.2674 - val_mae: 2.5393 - val_mse: 7.2674\n",
      "Epoch 92/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 0.5974 - mae: 0.6168 - mse: 0.5974 - val_loss: 6.5180 - val_mae: 2.3940 - val_mse: 6.5180\n",
      "Epoch 93/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.5983 - mae: 0.6198 - mse: 0.5983 - val_loss: 6.0601 - val_mae: 2.3019 - val_mse: 6.0601\n",
      "Epoch 94/450\n",
      "2484/2484 [==============================] - 0s 102us/sample - loss: 0.6007 - mae: 0.6191 - mse: 0.6007 - val_loss: 7.0066 - val_mae: 2.4886 - val_mse: 7.0066\n",
      "Epoch 95/450\n",
      "2484/2484 [==============================] - 0s 80us/sample - loss: 0.6039 - mae: 0.6187 - mse: 0.6039 - val_loss: 6.9482 - val_mae: 2.4796 - val_mse: 6.9482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5992 - mae: 0.6156 - mse: 0.5992 - val_loss: 5.6159 - val_mae: 2.2067 - val_mse: 5.6159\n",
      "Epoch 97/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 0.5995 - mae: 0.6192 - mse: 0.5995 - val_loss: 6.8066 - val_mae: 2.4518 - val_mse: 6.8066\n",
      "Epoch 98/450\n",
      "2484/2484 [==============================] - 0s 86us/sample - loss: 0.5981 - mae: 0.6160 - mse: 0.5981 - val_loss: 5.7705 - val_mae: 2.2424 - val_mse: 5.7705\n",
      "Epoch 99/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 0.6014 - mae: 0.6272 - mse: 0.6014 - val_loss: 5.6787 - val_mae: 2.2230 - val_mse: 5.6787\n",
      "Epoch 100/450\n",
      "2484/2484 [==============================] - 0s 123us/sample - loss: 0.5995 - mae: 0.6230 - mse: 0.5995 - val_loss: 5.8751 - val_mae: 2.2641 - val_mse: 5.8751\n",
      "Epoch 101/450\n",
      "2484/2484 [==============================] - 0s 106us/sample - loss: 0.5980 - mae: 0.6167 - mse: 0.5980 - val_loss: 6.3229 - val_mae: 2.3546 - val_mse: 6.3229\n",
      "Epoch 102/450\n",
      "2484/2484 [==============================] - 0s 118us/sample - loss: 0.6019 - mae: 0.6211 - mse: 0.6019 - val_loss: 7.1228 - val_mae: 2.5134 - val_mse: 7.1228\n",
      "Epoch 103/450\n",
      "2484/2484 [==============================] - 0s 115us/sample - loss: 0.6015 - mae: 0.6215 - mse: 0.6015 - val_loss: 6.9784 - val_mae: 2.4855 - val_mse: 6.9784\n",
      "Epoch 104/450\n",
      "2484/2484 [==============================] - 0s 82us/sample - loss: 0.5972 - mae: 0.6193 - mse: 0.5972 - val_loss: 6.0321 - val_mae: 2.2963 - val_mse: 6.0321\n",
      "Epoch 105/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 0.6040 - mae: 0.6221 - mse: 0.6040 - val_loss: 6.6076 - val_mae: 2.4121 - val_mse: 6.6076\n",
      "Epoch 106/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 0.5959 - mae: 0.6186 - mse: 0.5959 - val_loss: 7.0975 - val_mae: 2.5085 - val_mse: 7.0975\n",
      "Epoch 107/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 0.6005 - mae: 0.6213 - mse: 0.6005 - val_loss: 6.6144 - val_mae: 2.4128 - val_mse: 6.6144\n",
      "Epoch 108/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 0.5989 - mae: 0.6155 - mse: 0.5989 - val_loss: 6.1556 - val_mae: 2.3209 - val_mse: 6.1556\n",
      "Epoch 109/450\n",
      "2484/2484 [==============================] - 0s 85us/sample - loss: 0.5977 - mae: 0.6179 - mse: 0.5977 - val_loss: 6.7855 - val_mae: 2.4477 - val_mse: 6.7855\n",
      "Epoch 110/450\n",
      "2484/2484 [==============================] - 0s 82us/sample - loss: 0.5962 - mae: 0.6155 - mse: 0.5962 - val_loss: 5.8939 - val_mae: 2.2667 - val_mse: 5.8939\n",
      "Epoch 111/450\n",
      "2484/2484 [==============================] - 0s 152us/sample - loss: 0.5945 - mae: 0.6159 - mse: 0.5945 - val_loss: 5.9648 - val_mae: 2.2754 - val_mse: 5.9648\n",
      "Epoch 112/450\n",
      "2484/2484 [==============================] - 0s 93us/sample - loss: 0.6021 - mae: 0.6237 - mse: 0.6021 - val_loss: 5.8831 - val_mae: 2.2659 - val_mse: 5.8831\n",
      "Epoch 113/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5980 - mae: 0.6160 - mse: 0.5980 - val_loss: 7.0720 - val_mae: 2.5036 - val_mse: 7.0720\n",
      "Epoch 114/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5981 - mae: 0.6206 - mse: 0.5981 - val_loss: 6.1419 - val_mae: 2.3185 - val_mse: 6.1419\n",
      "Epoch 115/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.5988 - mae: 0.6224 - mse: 0.5988 - val_loss: 5.9742 - val_mae: 2.2844 - val_mse: 5.9742\n",
      "Epoch 116/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5987 - mae: 0.6210 - mse: 0.5987 - val_loss: 7.0651 - val_mae: 2.5023 - val_mse: 7.0651\n",
      "Epoch 117/450\n",
      "2484/2484 [==============================] - ETA: 0s - loss: 0.5958 - mae: 0.6140 - mse: 0.595 - 0s 57us/sample - loss: 0.6020 - mae: 0.6210 - mse: 0.6020 - val_loss: 6.8462 - val_mae: 2.4595 - val_mse: 6.8462\n",
      "Epoch 118/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.6020 - mae: 0.6230 - mse: 0.6020 - val_loss: 5.3939 - val_mae: 2.1619 - val_mse: 5.3939\n",
      "Epoch 119/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.5962 - mae: 0.6188 - mse: 0.5962 - val_loss: 6.3676 - val_mae: 2.3636 - val_mse: 6.3676\n",
      "Epoch 120/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.6010 - mae: 0.6198 - mse: 0.6010 - val_loss: 5.9380 - val_mae: 2.2771 - val_mse: 5.9380\n",
      "Epoch 121/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5973 - mae: 0.6190 - mse: 0.5973 - val_loss: 5.9971 - val_mae: 2.2881 - val_mse: 5.9971\n",
      "Epoch 122/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5987 - mae: 0.6229 - mse: 0.5987 - val_loss: 6.8122 - val_mae: 2.4529 - val_mse: 6.8122\n",
      "Epoch 123/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5989 - mae: 0.6197 - mse: 0.5989 - val_loss: 6.6715 - val_mae: 2.4249 - val_mse: 6.6715\n",
      "Epoch 124/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5955 - mae: 0.6156 - mse: 0.5955 - val_loss: 7.0113 - val_mae: 2.4841 - val_mse: 7.0113\n",
      "Epoch 125/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 0.5992 - mae: 0.6218 - mse: 0.5992 - val_loss: 6.5743 - val_mae: 2.4017 - val_mse: 6.5743\n",
      "Epoch 126/450\n",
      "2484/2484 [==============================] - 0s 92us/sample - loss: 0.6010 - mae: 0.6203 - mse: 0.6010 - val_loss: 6.7826 - val_mae: 2.4461 - val_mse: 6.7826\n",
      "Epoch 127/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 0.5980 - mae: 0.6192 - mse: 0.5980 - val_loss: 6.1581 - val_mae: 2.3184 - val_mse: 6.1581\n",
      "Epoch 128/450\n",
      "2484/2484 [==============================] - 0s 134us/sample - loss: 0.6009 - mae: 0.6189 - mse: 0.6009 - val_loss: 6.5158 - val_mae: 2.3935 - val_mse: 6.5158\n",
      "Epoch 129/450\n",
      "2484/2484 [==============================] - 0s 134us/sample - loss: 0.6027 - mae: 0.6184 - mse: 0.6027 - val_loss: 5.7206 - val_mae: 2.2318 - val_mse: 5.7206\n",
      "Epoch 130/450\n",
      "2484/2484 [==============================] - 0s 153us/sample - loss: 0.5985 - mae: 0.6197 - mse: 0.5985 - val_loss: 7.0590 - val_mae: 2.5011 - val_mse: 7.0590\n",
      "Epoch 131/450\n",
      "2484/2484 [==============================] - 0s 133us/sample - loss: 0.6055 - mae: 0.6228 - mse: 0.6055 - val_loss: 5.7251 - val_mae: 2.2328 - val_mse: 5.7252\n",
      "Epoch 132/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.5961 - mae: 0.6237 - mse: 0.5961 - val_loss: 6.3172 - val_mae: 2.3534 - val_mse: 6.3172\n",
      "Epoch 133/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.5964 - mae: 0.6155 - mse: 0.5964 - val_loss: 6.5033 - val_mae: 2.3910 - val_mse: 6.5033\n",
      "Epoch 134/450\n",
      "2484/2484 [==============================] - 0s 117us/sample - loss: 0.6028 - mae: 0.6206 - mse: 0.6028 - val_loss: 6.9309 - val_mae: 2.4758 - val_mse: 6.9309\n",
      "Epoch 135/450\n",
      "2484/2484 [==============================] - 0s 102us/sample - loss: 0.6003 - mae: 0.6144 - mse: 0.6003 - val_loss: 6.3501 - val_mae: 2.3600 - val_mse: 6.3501\n",
      "Epoch 136/450\n",
      "2484/2484 [==============================] - 0s 105us/sample - loss: 0.5981 - mae: 0.6179 - mse: 0.5981 - val_loss: 6.8993 - val_mae: 2.4666 - val_mse: 6.8993\n",
      "Epoch 137/450\n",
      "2484/2484 [==============================] - 0s 94us/sample - loss: 0.6004 - mae: 0.6202 - mse: 0.6004 - val_loss: 6.0192 - val_mae: 2.2938 - val_mse: 6.0192\n",
      "Epoch 138/450\n",
      "2484/2484 [==============================] - 0s 79us/sample - loss: 0.6002 - mae: 0.6151 - mse: 0.6002 - val_loss: 7.0138 - val_mae: 2.4920 - val_mse: 7.0138\n",
      "Epoch 139/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.5988 - mae: 0.6164 - mse: 0.5988 - val_loss: 5.7466 - val_mae: 2.2369 - val_mse: 5.7466\n",
      "Epoch 140/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5983 - mae: 0.6171 - mse: 0.5983 - val_loss: 5.8184 - val_mae: 2.2498 - val_mse: 5.8184\n",
      "Epoch 141/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.5991 - mae: 0.6197 - mse: 0.5991 - val_loss: 6.6454 - val_mae: 2.4193 - val_mse: 6.6454\n",
      "Epoch 142/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5991 - mae: 0.6153 - mse: 0.5991 - val_loss: 6.3101 - val_mae: 2.3522 - val_mse: 6.3101\n",
      "Epoch 143/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.5996 - mae: 0.6164 - mse: 0.5996 - val_loss: 6.1374 - val_mae: 2.3177 - val_mse: 6.1374\n",
      "Epoch 144/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.5951 - mae: 0.6197 - mse: 0.5951 - val_loss: 6.4577 - val_mae: 2.3817 - val_mse: 6.4577\n",
      "Epoch 145/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5985 - mae: 0.6140 - mse: 0.5985 - val_loss: 6.3573 - val_mae: 2.3612 - val_mse: 6.3573\n",
      "Epoch 146/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.5984 - mae: 0.6227 - mse: 0.5984 - val_loss: 6.4850 - val_mae: 2.3872 - val_mse: 6.4850\n",
      "Epoch 147/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6016 - mae: 0.6196 - mse: 0.6016 - val_loss: 6.9197 - val_mae: 2.4738 - val_mse: 6.9197\n",
      "Epoch 148/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.6033 - mae: 0.6256 - mse: 0.6033 - val_loss: 5.9438 - val_mae: 2.2783 - val_mse: 5.9438\n",
      "Epoch 149/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5948 - mae: 0.6148 - mse: 0.5948 - val_loss: 5.8321 - val_mae: 2.2552 - val_mse: 5.8321\n",
      "Epoch 150/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6015 - mae: 0.6210 - mse: 0.6015 - val_loss: 6.0078 - val_mae: 2.2913 - val_mse: 6.0078\n",
      "Epoch 151/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 0.5999 - mae: 0.6234 - mse: 0.5999 - val_loss: 6.0631 - val_mae: 2.3026 - val_mse: 6.0631\n",
      "Epoch 152/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5930 - mae: 0.6119 - mse: 0.5930 - val_loss: 7.4541 - val_mae: 2.5740 - val_mse: 7.4541\n",
      "Epoch 153/450\n",
      "2484/2484 [==============================] - 0s 109us/sample - loss: 0.6038 - mae: 0.6176 - mse: 0.6038 - val_loss: 6.7169 - val_mae: 2.4327 - val_mse: 6.7169\n",
      "Epoch 154/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.5977 - mae: 0.6136 - mse: 0.5977 - val_loss: 5.8925 - val_mae: 2.2675 - val_mse: 5.8925\n",
      "Epoch 155/450\n",
      "2484/2484 [==============================] - 0s 88us/sample - loss: 0.5995 - mae: 0.6201 - mse: 0.5995 - val_loss: 5.9669 - val_mae: 2.2830 - val_mse: 5.9669\n",
      "Epoch 156/450\n",
      "2484/2484 [==============================] - 0s 102us/sample - loss: 0.5987 - mae: 0.6203 - mse: 0.5987 - val_loss: 6.3819 - val_mae: 2.3646 - val_mse: 6.3819\n",
      "Epoch 157/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 0.5990 - mae: 0.6209 - mse: 0.5990 - val_loss: 6.7747 - val_mae: 2.4454 - val_mse: 6.7747\n",
      "Epoch 158/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5965 - mae: 0.6173 - mse: 0.5965 - val_loss: 6.2344 - val_mae: 2.3369 - val_mse: 6.2344\n",
      "Epoch 159/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.6027 - mae: 0.6212 - mse: 0.6027 - val_loss: 6.0076 - val_mae: 2.2907 - val_mse: 6.0076\n",
      "Epoch 160/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.5970 - mae: 0.6123 - mse: 0.5970 - val_loss: 5.5727 - val_mae: 2.2001 - val_mse: 5.5727\n",
      "Epoch 161/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.5932 - mae: 0.6218 - mse: 0.5932 - val_loss: 6.5429 - val_mae: 2.3990 - val_mse: 6.5429\n",
      "Epoch 162/450\n",
      "2484/2484 [==============================] - 0s 115us/sample - loss: 0.5968 - mae: 0.6139 - mse: 0.5968 - val_loss: 6.6712 - val_mae: 2.4248 - val_mse: 6.6712\n",
      "Epoch 163/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6008 - mae: 0.6209 - mse: 0.6008 - val_loss: 6.0412 - val_mae: 2.2931 - val_mse: 6.0412\n",
      "Epoch 164/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.5977 - mae: 0.6189 - mse: 0.5977 - val_loss: 6.5394 - val_mae: 2.3978 - val_mse: 6.5394\n",
      "Epoch 165/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5998 - mae: 0.6167 - mse: 0.5998 - val_loss: 7.2173 - val_mae: 2.5314 - val_mse: 7.2173\n",
      "Epoch 166/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 0.5984 - mae: 0.6156 - mse: 0.5984 - val_loss: 5.9097 - val_mae: 2.2705 - val_mse: 5.9097\n",
      "Epoch 167/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.5969 - mae: 0.6170 - mse: 0.5969 - val_loss: 6.9436 - val_mae: 2.4753 - val_mse: 6.9436\n",
      "Epoch 168/450\n",
      "2484/2484 [==============================] - 0s 86us/sample - loss: 0.6035 - mae: 0.6213 - mse: 0.6035 - val_loss: 6.3422 - val_mae: 2.3573 - val_mse: 6.3422\n",
      "Epoch 169/450\n",
      "2484/2484 [==============================] - 0s 125us/sample - loss: 0.5945 - mae: 0.6111 - mse: 0.5945 - val_loss: 6.2243 - val_mae: 2.3326 - val_mse: 6.2243\n",
      "Epoch 170/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 0.5969 - mae: 0.6141 - mse: 0.5969 - val_loss: 5.3933 - val_mae: 2.1613 - val_mse: 5.3933\n",
      "Epoch 171/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.5988 - mae: 0.6194 - mse: 0.5988 - val_loss: 6.5103 - val_mae: 2.3872 - val_mse: 6.5103\n",
      "Epoch 172/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5959 - mae: 0.6159 - mse: 0.5959 - val_loss: 6.5015 - val_mae: 2.3901 - val_mse: 6.5015\n",
      "Epoch 173/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5990 - mae: 0.6175 - mse: 0.5990 - val_loss: 6.6016 - val_mae: 2.4020 - val_mse: 6.6016\n",
      "Epoch 174/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6073 - mae: 0.6215 - mse: 0.6073 - val_loss: 6.5122 - val_mae: 2.3926 - val_mse: 6.5122\n",
      "Epoch 175/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5944 - mae: 0.6126 - mse: 0.5944 - val_loss: 6.8477 - val_mae: 2.4598 - val_mse: 6.8477\n",
      "Epoch 176/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5997 - mae: 0.6186 - mse: 0.5997 - val_loss: 6.0769 - val_mae: 2.3053 - val_mse: 6.0769\n",
      "Epoch 177/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.5945 - mae: 0.6162 - mse: 0.5945 - val_loss: 6.1679 - val_mae: 2.3232 - val_mse: 6.1679\n",
      "Epoch 178/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.6019 - mae: 0.6218 - mse: 0.6019 - val_loss: 6.3977 - val_mae: 2.3695 - val_mse: 6.3977\n",
      "Epoch 179/450\n",
      "2484/2484 [==============================] - 0s 111us/sample - loss: 0.5995 - mae: 0.6152 - mse: 0.5995 - val_loss: 7.1319 - val_mae: 2.5152 - val_mse: 7.1319\n",
      "Epoch 180/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.5988 - mae: 0.6196 - mse: 0.5988 - val_loss: 5.5079 - val_mae: 2.1866 - val_mse: 5.5079\n",
      "Epoch 181/450\n",
      "2484/2484 [==============================] - 0s 95us/sample - loss: 0.5974 - mae: 0.6146 - mse: 0.5974 - val_loss: 5.6608 - val_mae: 2.2182 - val_mse: 5.6608\n",
      "Epoch 182/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 0.5996 - mae: 0.6206 - mse: 0.5996 - val_loss: 6.1602 - val_mae: 2.3219 - val_mse: 6.1602\n",
      "Epoch 183/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6028 - mae: 0.6199 - mse: 0.6028 - val_loss: 6.8843 - val_mae: 2.4662 - val_mse: 6.8843\n",
      "Epoch 184/450\n",
      "2484/2484 [==============================] - 0s 82us/sample - loss: 0.5941 - mae: 0.6175 - mse: 0.5941 - val_loss: 6.7791 - val_mae: 2.4456 - val_mse: 6.7791\n",
      "Epoch 185/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.6004 - mae: 0.6183 - mse: 0.6004 - val_loss: 6.2495 - val_mae: 2.3399 - val_mse: 6.2495\n",
      "Epoch 186/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.6040 - mae: 0.6189 - mse: 0.6040 - val_loss: 6.3974 - val_mae: 2.3694 - val_mse: 6.3974\n",
      "Epoch 187/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5969 - mae: 0.6177 - mse: 0.5969 - val_loss: 5.9272 - val_mae: 2.2742 - val_mse: 5.9272\n",
      "Epoch 188/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5987 - mae: 0.6156 - mse: 0.5987 - val_loss: 6.1799 - val_mae: 2.3258 - val_mse: 6.1799\n",
      "Epoch 189/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.5989 - mae: 0.6203 - mse: 0.5989 - val_loss: 6.4511 - val_mae: 2.3802 - val_mse: 6.4511\n",
      "Epoch 190/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5974 - mae: 0.6195 - mse: 0.5974 - val_loss: 7.1662 - val_mae: 2.5209 - val_mse: 7.1662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 0.5966 - mae: 0.6155 - mse: 0.5966 - val_loss: 5.4736 - val_mae: 2.1791 - val_mse: 5.4736\n",
      "Epoch 192/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.5951 - mae: 0.6178 - mse: 0.5951 - val_loss: 5.6323 - val_mae: 2.2131 - val_mse: 5.6323\n",
      "Epoch 193/450\n",
      "2484/2484 [==============================] - 0s 79us/sample - loss: 0.6004 - mae: 0.6183 - mse: 0.6004 - val_loss: 6.5833 - val_mae: 2.4070 - val_mse: 6.5833\n",
      "Epoch 194/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.5953 - mae: 0.6177 - mse: 0.5953 - val_loss: 6.5327 - val_mae: 2.3968 - val_mse: 6.5327\n",
      "Epoch 195/450\n",
      "2484/2484 [==============================] - 0s 79us/sample - loss: 0.5970 - mae: 0.6168 - mse: 0.5970 - val_loss: 7.5319 - val_mae: 2.5722 - val_mse: 7.5319\n",
      "Epoch 196/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.6015 - mae: 0.6232 - mse: 0.6015 - val_loss: 6.4380 - val_mae: 2.3775 - val_mse: 6.4380\n",
      "Epoch 197/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5966 - mae: 0.6166 - mse: 0.5966 - val_loss: 6.0134 - val_mae: 2.2919 - val_mse: 6.0134\n",
      "Epoch 198/450\n",
      "2484/2484 [==============================] - 0s 80us/sample - loss: 0.5998 - mae: 0.6173 - mse: 0.5998 - val_loss: 6.4205 - val_mae: 2.3730 - val_mse: 6.4205\n",
      "Epoch 199/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 0.5956 - mae: 0.6149 - mse: 0.5956 - val_loss: 6.4167 - val_mae: 2.3722 - val_mse: 6.4167\n",
      "Epoch 200/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 0.5967 - mae: 0.6184 - mse: 0.5967 - val_loss: 6.7697 - val_mae: 2.4444 - val_mse: 6.7697\n",
      "Epoch 201/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5976 - mae: 0.6185 - mse: 0.5976 - val_loss: 6.2749 - val_mae: 2.3440 - val_mse: 6.2749\n",
      "Epoch 202/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.5919 - mae: 0.6125 - mse: 0.5919 - val_loss: 7.9235 - val_mae: 2.6536 - val_mse: 7.9235\n",
      "Epoch 203/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.5981 - mae: 0.6162 - mse: 0.5981 - val_loss: 6.5303 - val_mae: 2.3955 - val_mse: 6.5303\n",
      "Epoch 204/450\n",
      "2484/2484 [==============================] - 0s 92us/sample - loss: 0.5984 - mae: 0.6187 - mse: 0.5984 - val_loss: 6.6164 - val_mae: 2.4133 - val_mse: 6.6164\n",
      "Epoch 205/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5980 - mae: 0.6140 - mse: 0.5980 - val_loss: 7.1813 - val_mae: 2.5230 - val_mse: 7.1813\n",
      "Epoch 206/450\n",
      "2484/2484 [==============================] - 0s 86us/sample - loss: 0.5960 - mae: 0.6120 - mse: 0.5960 - val_loss: 5.7455 - val_mae: 2.2356 - val_mse: 5.7455\n",
      "Epoch 207/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.5940 - mae: 0.6192 - mse: 0.5940 - val_loss: 6.5592 - val_mae: 2.4020 - val_mse: 6.5592\n",
      "Epoch 208/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5987 - mae: 0.6192 - mse: 0.5987 - val_loss: 6.2474 - val_mae: 2.3322 - val_mse: 6.2474\n",
      "Epoch 209/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.5980 - mae: 0.6201 - mse: 0.5980 - val_loss: 5.8728 - val_mae: 2.2635 - val_mse: 5.8728\n",
      "Epoch 210/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.6011 - mae: 0.6177 - mse: 0.6011 - val_loss: 6.0433 - val_mae: 2.2981 - val_mse: 6.0433\n",
      "Epoch 211/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.5958 - mae: 0.6150 - mse: 0.5958 - val_loss: 6.0461 - val_mae: 2.2988 - val_mse: 6.0461\n",
      "Epoch 212/450\n",
      "2484/2484 [==============================] - 0s 88us/sample - loss: 0.5966 - mae: 0.6177 - mse: 0.5966 - val_loss: 7.3980 - val_mae: 2.5614 - val_mse: 7.3980\n",
      "Epoch 213/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.6004 - mae: 0.6193 - mse: 0.6004 - val_loss: 6.2678 - val_mae: 2.3416 - val_mse: 6.2678\n",
      "Epoch 214/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.6009 - mae: 0.6190 - mse: 0.6009 - val_loss: 5.6211 - val_mae: 2.2109 - val_mse: 5.6211\n",
      "Epoch 215/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.5983 - mae: 0.6200 - mse: 0.5983 - val_loss: 6.1813 - val_mae: 2.3263 - val_mse: 6.1813\n",
      "Epoch 216/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.5969 - mae: 0.6159 - mse: 0.5969 - val_loss: 6.9745 - val_mae: 2.4847 - val_mse: 6.9745\n",
      "Epoch 217/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.6018 - mae: 0.6220 - mse: 0.6018 - val_loss: 6.5432 - val_mae: 2.3978 - val_mse: 6.5432\n",
      "Epoch 218/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 0.6018 - mae: 0.6183 - mse: 0.6018 - val_loss: 6.7146 - val_mae: 2.4333 - val_mse: 6.7146\n",
      "Epoch 219/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 0.6021 - mae: 0.6221 - mse: 0.6021 - val_loss: 6.7495 - val_mae: 2.4401 - val_mse: 6.7495\n",
      "Epoch 220/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 0.5984 - mae: 0.6146 - mse: 0.5984 - val_loss: 5.7939 - val_mae: 2.2469 - val_mse: 5.7939\n",
      "Epoch 221/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.6001 - mae: 0.6175 - mse: 0.6001 - val_loss: 6.5960 - val_mae: 2.4096 - val_mse: 6.5960\n",
      "Epoch 222/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.5998 - mae: 0.6182 - mse: 0.5998 - val_loss: 6.6014 - val_mae: 2.4108 - val_mse: 6.6014\n",
      "Epoch 223/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 0.6014 - mae: 0.6150 - mse: 0.6014 - val_loss: 6.5691 - val_mae: 2.4008 - val_mse: 6.5691\n",
      "Epoch 224/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 0.5999 - mae: 0.6205 - mse: 0.5999 - val_loss: 6.0691 - val_mae: 2.3037 - val_mse: 6.0691\n",
      "Epoch 225/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 0.5967 - mae: 0.6192 - mse: 0.5967 - val_loss: 6.2540 - val_mae: 2.3406 - val_mse: 6.2540\n",
      "Epoch 226/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.6026 - mae: 0.6187 - mse: 0.6026 - val_loss: 6.1070 - val_mae: 2.3113 - val_mse: 6.1070\n",
      "Epoch 227/450\n",
      "2484/2484 [==============================] - 0s 82us/sample - loss: 0.5953 - mae: 0.6131 - mse: 0.5953 - val_loss: 6.1967 - val_mae: 2.3295 - val_mse: 6.1967\n",
      "Epoch 228/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.6013 - mae: 0.6246 - mse: 0.6013 - val_loss: 5.4889 - val_mae: 2.1819 - val_mse: 5.4889\n",
      "Epoch 229/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.5953 - mae: 0.6128 - mse: 0.5953 - val_loss: 5.4604 - val_mae: 2.1763 - val_mse: 5.4604\n",
      "Epoch 230/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.5983 - mae: 0.6208 - mse: 0.5983 - val_loss: 6.4119 - val_mae: 2.3720 - val_mse: 6.4119\n",
      "Epoch 231/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.6075 - mae: 0.6229 - mse: 0.6075 - val_loss: 7.0274 - val_mae: 2.4943 - val_mse: 7.0274\n",
      "Epoch 232/450\n",
      "2484/2484 [==============================] - 0s 96us/sample - loss: 0.6019 - mae: 0.6217 - mse: 0.6019 - val_loss: 6.4959 - val_mae: 2.3872 - val_mse: 6.4959\n",
      "Epoch 233/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 0.5970 - mae: 0.6181 - mse: 0.5970 - val_loss: 6.5645 - val_mae: 2.4033 - val_mse: 6.5645\n",
      "Epoch 234/450\n",
      "2484/2484 [==============================] - 0s 92us/sample - loss: 0.5955 - mae: 0.6145 - mse: 0.5955 - val_loss: 7.0605 - val_mae: 2.5007 - val_mse: 7.0605\n",
      "Epoch 235/450\n",
      "2484/2484 [==============================] - 0s 101us/sample - loss: 0.6046 - mae: 0.6222 - mse: 0.6046 - val_loss: 5.9091 - val_mae: 2.2710 - val_mse: 5.9091\n",
      "Epoch 236/450\n",
      "2484/2484 [==============================] - 0s 119us/sample - loss: 0.5973 - mae: 0.6161 - mse: 0.5973 - val_loss: 6.6185 - val_mae: 2.4124 - val_mse: 6.6185\n",
      "Epoch 237/450\n",
      "2484/2484 [==============================] - 0s 113us/sample - loss: 0.5967 - mae: 0.6140 - mse: 0.5967 - val_loss: 5.7518 - val_mae: 2.2376 - val_mse: 5.7518\n",
      "Epoch 238/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5960 - mae: 0.6200 - mse: 0.5960 - val_loss: 6.8879 - val_mae: 2.4648 - val_mse: 6.8879\n",
      "Epoch 239/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.5990 - mae: 0.6192 - mse: 0.5990 - val_loss: 5.9045 - val_mae: 2.2691 - val_mse: 5.9045\n",
      "Epoch 240/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.6012 - mae: 0.6194 - mse: 0.6012 - val_loss: 6.2049 - val_mae: 2.3308 - val_mse: 6.2049\n",
      "Epoch 241/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 0.5995 - mae: 0.6132 - mse: 0.5995 - val_loss: 6.3966 - val_mae: 2.3693 - val_mse: 6.3966\n",
      "Epoch 242/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5974 - mae: 0.6156 - mse: 0.5974 - val_loss: 6.7370 - val_mae: 2.4371 - val_mse: 6.7370\n",
      "Epoch 243/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5937 - mae: 0.6131 - mse: 0.5937 - val_loss: 5.6343 - val_mae: 2.2131 - val_mse: 5.6343\n",
      "Epoch 244/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.6017 - mae: 0.6187 - mse: 0.6017 - val_loss: 6.2507 - val_mae: 2.3400 - val_mse: 6.2507\n",
      "Epoch 245/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5960 - mae: 0.6215 - mse: 0.5960 - val_loss: 6.6469 - val_mae: 2.4199 - val_mse: 6.6469\n",
      "Epoch 246/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5995 - mae: 0.6250 - mse: 0.5995 - val_loss: 6.5124 - val_mae: 2.3923 - val_mse: 6.5124\n",
      "Epoch 247/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5955 - mae: 0.6168 - mse: 0.5955 - val_loss: 6.0152 - val_mae: 2.2892 - val_mse: 6.0152\n",
      "Epoch 248/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6032 - mae: 0.6208 - mse: 0.6032 - val_loss: 6.3368 - val_mae: 2.3574 - val_mse: 6.3368\n",
      "Epoch 249/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 0.6006 - mae: 0.6177 - mse: 0.6006 - val_loss: 5.7381 - val_mae: 2.2355 - val_mse: 5.7381\n",
      "Epoch 250/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5947 - mae: 0.6198 - mse: 0.5947 - val_loss: 6.8903 - val_mae: 2.4632 - val_mse: 6.8903\n",
      "Epoch 251/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5962 - mae: 0.6164 - mse: 0.5962 - val_loss: 5.5196 - val_mae: 2.1838 - val_mse: 5.5196\n",
      "Epoch 252/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5990 - mae: 0.6180 - mse: 0.5990 - val_loss: 6.7063 - val_mae: 2.4282 - val_mse: 6.7063\n",
      "Epoch 253/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6006 - mae: 0.6211 - mse: 0.6006 - val_loss: 6.5749 - val_mae: 2.4051 - val_mse: 6.5749\n",
      "Epoch 254/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5970 - mae: 0.6155 - mse: 0.5970 - val_loss: 6.7635 - val_mae: 2.4429 - val_mse: 6.7635\n",
      "Epoch 255/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 0.5946 - mae: 0.6142 - mse: 0.5946 - val_loss: 6.1452 - val_mae: 2.3180 - val_mse: 6.1452\n",
      "Epoch 256/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5965 - mae: 0.6159 - mse: 0.5965 - val_loss: 6.2084 - val_mae: 2.3319 - val_mse: 6.2084\n",
      "Epoch 257/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5982 - mae: 0.6204 - mse: 0.5982 - val_loss: 6.2810 - val_mae: 2.3464 - val_mse: 6.2810\n",
      "Epoch 258/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 0.5960 - mae: 0.6120 - mse: 0.5960 - val_loss: 6.2375 - val_mae: 2.3338 - val_mse: 6.2375\n",
      "Epoch 259/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 0.5970 - mae: 0.6219 - mse: 0.5970 - val_loss: 6.3933 - val_mae: 2.3686 - val_mse: 6.3933\n",
      "Epoch 260/450\n",
      "2484/2484 [==============================] - 0s 79us/sample - loss: 0.5986 - mae: 0.6154 - mse: 0.5986 - val_loss: 6.2388 - val_mae: 2.3380 - val_mse: 6.2388\n",
      "Epoch 261/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5982 - mae: 0.6156 - mse: 0.5982 - val_loss: 6.0034 - val_mae: 2.2893 - val_mse: 6.0034\n",
      "Epoch 262/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.6003 - mae: 0.6198 - mse: 0.6003 - val_loss: 6.3765 - val_mae: 2.3653 - val_mse: 6.3765\n",
      "Epoch 263/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.5943 - mae: 0.6174 - mse: 0.5943 - val_loss: 6.7793 - val_mae: 2.4463 - val_mse: 6.7793\n",
      "Epoch 264/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 0.5989 - mae: 0.6171 - mse: 0.5989 - val_loss: 5.8640 - val_mae: 2.2618 - val_mse: 5.8640\n",
      "Epoch 265/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.6004 - mae: 0.6210 - mse: 0.6004 - val_loss: 6.0779 - val_mae: 2.3055 - val_mse: 6.0779\n",
      "Epoch 266/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 0.5969 - mae: 0.6174 - mse: 0.5969 - val_loss: 6.4245 - val_mae: 2.3743 - val_mse: 6.4245\n",
      "Epoch 267/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5983 - mae: 0.6132 - mse: 0.5983 - val_loss: 5.7977 - val_mae: 2.2479 - val_mse: 5.7977\n",
      "Epoch 268/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5990 - mae: 0.6207 - mse: 0.5990 - val_loss: 6.3396 - val_mae: 2.3535 - val_mse: 6.3396\n",
      "Epoch 269/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5944 - mae: 0.6151 - mse: 0.5944 - val_loss: 6.3103 - val_mae: 2.3498 - val_mse: 6.3103\n",
      "Epoch 270/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6010 - mae: 0.6224 - mse: 0.6010 - val_loss: 6.3085 - val_mae: 2.3518 - val_mse: 6.3085\n",
      "Epoch 271/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.5970 - mae: 0.6141 - mse: 0.5970 - val_loss: 5.8029 - val_mae: 2.2483 - val_mse: 5.8029\n",
      "Epoch 272/450\n",
      "2484/2484 [==============================] - 0s 97us/sample - loss: 0.6032 - mae: 0.6239 - mse: 0.6032 - val_loss: 6.2081 - val_mae: 2.3300 - val_mse: 6.2081\n",
      "Epoch 273/450\n",
      "2484/2484 [==============================] - ETA: 0s - loss: 0.5974 - mae: 0.6227 - mse: 0.597 - 0s 79us/sample - loss: 0.5940 - mae: 0.6170 - mse: 0.5940 - val_loss: 5.1345 - val_mae: 2.1043 - val_mse: 5.1345\n",
      "Epoch 274/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 0.6049 - mae: 0.6208 - mse: 0.6049 - val_loss: 7.0207 - val_mae: 2.4932 - val_mse: 7.0207\n",
      "Epoch 275/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.5949 - mae: 0.6146 - mse: 0.5949 - val_loss: 7.2827 - val_mae: 2.5437 - val_mse: 7.2827\n",
      "Epoch 276/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.6002 - mae: 0.6200 - mse: 0.6002 - val_loss: 5.7800 - val_mae: 2.2442 - val_mse: 5.7800\n",
      "Epoch 277/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.5983 - mae: 0.6169 - mse: 0.5983 - val_loss: 6.5979 - val_mae: 2.4101 - val_mse: 6.5979\n",
      "Epoch 278/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 0.5980 - mae: 0.6189 - mse: 0.5980 - val_loss: 6.6568 - val_mae: 2.4219 - val_mse: 6.6568\n",
      "Epoch 279/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5965 - mae: 0.6133 - mse: 0.5965 - val_loss: 6.9961 - val_mae: 2.4865 - val_mse: 6.9961\n",
      "Epoch 280/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 0.6009 - mae: 0.6218 - mse: 0.6009 - val_loss: 6.3253 - val_mae: 2.3533 - val_mse: 6.3253\n",
      "Epoch 281/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5928 - mae: 0.6130 - mse: 0.5928 - val_loss: 7.2568 - val_mae: 2.5388 - val_mse: 7.2568\n",
      "Epoch 282/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6012 - mae: 0.6209 - mse: 0.6012 - val_loss: 7.3101 - val_mae: 2.5478 - val_mse: 7.3101\n",
      "Epoch 283/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.5956 - mae: 0.6175 - mse: 0.5956 - val_loss: 5.9951 - val_mae: 2.2883 - val_mse: 5.9951\n",
      "Epoch 284/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5955 - mae: 0.6203 - mse: 0.5955 - val_loss: 5.6924 - val_mae: 2.2259 - val_mse: 5.6924\n",
      "Epoch 285/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6018 - mae: 0.6230 - mse: 0.6018 - val_loss: 6.2735 - val_mae: 2.3437 - val_mse: 6.2735\n",
      "Epoch 286/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 0.5959 - mae: 0.6216 - mse: 0.5959 - val_loss: 7.0215 - val_mae: 2.4935 - val_mse: 7.0215\n",
      "Epoch 287/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.5996 - mae: 0.6206 - mse: 0.5996 - val_loss: 6.5899 - val_mae: 2.4084 - val_mse: 6.5899\n",
      "Epoch 288/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.5997 - mae: 0.6208 - mse: 0.5997 - val_loss: 5.9530 - val_mae: 2.2801 - val_mse: 5.9530\n",
      "Epoch 289/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5981 - mae: 0.6188 - mse: 0.5981 - val_loss: 6.2119 - val_mae: 2.3315 - val_mse: 6.2119\n",
      "Epoch 290/450\n",
      "2484/2484 [==============================] - 0s 95us/sample - loss: 0.5997 - mae: 0.6188 - mse: 0.5997 - val_loss: 5.9974 - val_mae: 2.2884 - val_mse: 5.9974\n",
      "Epoch 291/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5983 - mae: 0.6189 - mse: 0.5983 - val_loss: 6.6299 - val_mae: 2.4159 - val_mse: 6.6299\n",
      "Epoch 292/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5953 - mae: 0.6143 - mse: 0.5953 - val_loss: 6.5294 - val_mae: 2.3952 - val_mse: 6.5294\n",
      "Epoch 293/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 0.5945 - mae: 0.6172 - mse: 0.5945 - val_loss: 6.9003 - val_mae: 2.4697 - val_mse: 6.9003\n",
      "Epoch 294/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.6018 - mae: 0.6198 - mse: 0.6018 - val_loss: 6.3861 - val_mae: 2.3669 - val_mse: 6.3861\n",
      "Epoch 295/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.5958 - mae: 0.6163 - mse: 0.5958 - val_loss: 6.4111 - val_mae: 2.3720 - val_mse: 6.4111\n",
      "Epoch 296/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.5993 - mae: 0.6184 - mse: 0.5993 - val_loss: 5.8463 - val_mae: 2.2578 - val_mse: 5.8463\n",
      "Epoch 297/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5996 - mae: 0.6196 - mse: 0.5996 - val_loss: 5.9958 - val_mae: 2.2890 - val_mse: 5.9958\n",
      "Epoch 298/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5992 - mae: 0.6229 - mse: 0.5992 - val_loss: 6.1202 - val_mae: 2.3141 - val_mse: 6.1202\n",
      "Epoch 299/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 0.5959 - mae: 0.6135 - mse: 0.5959 - val_loss: 6.3992 - val_mae: 2.3698 - val_mse: 6.3992\n",
      "Epoch 300/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.6004 - mae: 0.6204 - mse: 0.6004 - val_loss: 5.8281 - val_mae: 2.2544 - val_mse: 5.8281\n",
      "Epoch 301/450\n",
      "2484/2484 [==============================] - 0s 109us/sample - loss: 0.6022 - mae: 0.6237 - mse: 0.6022 - val_loss: 6.1639 - val_mae: 2.3208 - val_mse: 6.1639\n",
      "Epoch 302/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.5966 - mae: 0.6175 - mse: 0.5966 - val_loss: 6.3039 - val_mae: 2.3507 - val_mse: 6.3039\n",
      "Epoch 303/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5955 - mae: 0.6163 - mse: 0.5955 - val_loss: 6.5620 - val_mae: 2.4028 - val_mse: 6.5620\n",
      "Epoch 304/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.5959 - mae: 0.6179 - mse: 0.5959 - val_loss: 5.3020 - val_mae: 2.1417 - val_mse: 5.3020\n",
      "Epoch 305/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6022 - mae: 0.6262 - mse: 0.6022 - val_loss: 6.5277 - val_mae: 2.3955 - val_mse: 6.5277\n",
      "Epoch 306/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5932 - mae: 0.6116 - mse: 0.5932 - val_loss: 6.2116 - val_mae: 2.3317 - val_mse: 6.2116\n",
      "Epoch 307/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5970 - mae: 0.6215 - mse: 0.5970 - val_loss: 5.8186 - val_mae: 2.2524 - val_mse: 5.8186\n",
      "Epoch 308/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.5937 - mae: 0.6104 - mse: 0.5937 - val_loss: 4.9889 - val_mae: 2.0714 - val_mse: 4.9889\n",
      "Epoch 309/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.6023 - mae: 0.6226 - mse: 0.6023 - val_loss: 6.8774 - val_mae: 2.4654 - val_mse: 6.8774\n",
      "Epoch 310/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.5981 - mae: 0.6180 - mse: 0.5981 - val_loss: 6.0847 - val_mae: 2.3067 - val_mse: 6.0847\n",
      "Epoch 311/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5953 - mae: 0.6166 - mse: 0.5953 - val_loss: 6.2397 - val_mae: 2.3381 - val_mse: 6.2397\n",
      "Epoch 312/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5955 - mae: 0.6214 - mse: 0.5955 - val_loss: 6.0297 - val_mae: 2.2953 - val_mse: 6.0297\n",
      "Epoch 313/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.5958 - mae: 0.6138 - mse: 0.5958 - val_loss: 7.2630 - val_mae: 2.5401 - val_mse: 7.2630\n",
      "Epoch 314/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5984 - mae: 0.6196 - mse: 0.5984 - val_loss: 7.3188 - val_mae: 2.5500 - val_mse: 7.3188\n",
      "Epoch 315/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 0.6044 - mae: 0.6225 - mse: 0.6044 - val_loss: 6.2739 - val_mae: 2.3450 - val_mse: 6.2739\n",
      "Epoch 316/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 0.5964 - mae: 0.6174 - mse: 0.5964 - val_loss: 6.2124 - val_mae: 2.3326 - val_mse: 6.2124\n",
      "Epoch 317/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5952 - mae: 0.6150 - mse: 0.5952 - val_loss: 6.5054 - val_mae: 2.3909 - val_mse: 6.5054\n",
      "Epoch 318/450\n",
      "2484/2484 [==============================] - 0s 94us/sample - loss: 0.6010 - mae: 0.6197 - mse: 0.6010 - val_loss: 7.1324 - val_mae: 2.5146 - val_mse: 7.1324\n",
      "Epoch 319/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.6022 - mae: 0.6281 - mse: 0.6022 - val_loss: 7.1845 - val_mae: 2.5237 - val_mse: 7.1845\n",
      "Epoch 320/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.5974 - mae: 0.6149 - mse: 0.5974 - val_loss: 6.5061 - val_mae: 2.3895 - val_mse: 6.5061\n",
      "Epoch 321/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5982 - mae: 0.6181 - mse: 0.5982 - val_loss: 6.4686 - val_mae: 2.3837 - val_mse: 6.4686\n",
      "Epoch 322/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.6002 - mae: 0.6155 - mse: 0.6002 - val_loss: 6.7411 - val_mae: 2.4382 - val_mse: 6.7411\n",
      "Epoch 323/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5959 - mae: 0.6142 - mse: 0.5959 - val_loss: 5.7133 - val_mae: 2.2291 - val_mse: 5.7133\n",
      "Epoch 324/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6004 - mae: 0.6199 - mse: 0.6004 - val_loss: 6.7478 - val_mae: 2.4390 - val_mse: 6.7478\n",
      "Epoch 325/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5980 - mae: 0.6179 - mse: 0.5980 - val_loss: 5.8014 - val_mae: 2.2487 - val_mse: 5.8014\n",
      "Epoch 326/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 0.5975 - mae: 0.6141 - mse: 0.5975 - val_loss: 6.2644 - val_mae: 2.3416 - val_mse: 6.2644\n",
      "Epoch 327/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6015 - mae: 0.6217 - mse: 0.6015 - val_loss: 5.8976 - val_mae: 2.2687 - val_mse: 5.8976\n",
      "Epoch 328/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5992 - mae: 0.6150 - mse: 0.5992 - val_loss: 5.6780 - val_mae: 2.2228 - val_mse: 5.6780\n",
      "Epoch 329/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.5977 - mae: 0.6179 - mse: 0.5977 - val_loss: 5.7787 - val_mae: 2.2439 - val_mse: 5.7787\n",
      "Epoch 330/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.5934 - mae: 0.6184 - mse: 0.5934 - val_loss: 6.5893 - val_mae: 2.4068 - val_mse: 6.5893\n",
      "Epoch 331/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.5962 - mae: 0.6143 - mse: 0.5962 - val_loss: 7.0894 - val_mae: 2.5067 - val_mse: 7.0894\n",
      "Epoch 332/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5966 - mae: 0.6165 - mse: 0.5966 - val_loss: 6.2265 - val_mae: 2.3335 - val_mse: 6.2265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5974 - mae: 0.6209 - mse: 0.5974 - val_loss: 5.9728 - val_mae: 2.2841 - val_mse: 5.9728\n",
      "Epoch 334/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5946 - mae: 0.6136 - mse: 0.5946 - val_loss: 6.3092 - val_mae: 2.3520 - val_mse: 6.3092\n",
      "Epoch 335/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5986 - mae: 0.6196 - mse: 0.5986 - val_loss: 5.7487 - val_mae: 2.2338 - val_mse: 5.7487\n",
      "Epoch 336/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.5995 - mae: 0.6199 - mse: 0.5995 - val_loss: 5.7583 - val_mae: 2.2397 - val_mse: 5.7583\n",
      "Epoch 337/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 0.6003 - mae: 0.6269 - mse: 0.6003 - val_loss: 5.8445 - val_mae: 2.2574 - val_mse: 5.8445\n",
      "Epoch 338/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 0.5953 - mae: 0.6166 - mse: 0.5953 - val_loss: 6.2155 - val_mae: 2.3322 - val_mse: 6.2155\n",
      "Epoch 339/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.5966 - mae: 0.6165 - mse: 0.5966 - val_loss: 6.1901 - val_mae: 2.3282 - val_mse: 6.1901\n",
      "Epoch 340/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 0.5938 - mae: 0.6161 - mse: 0.5938 - val_loss: 6.6486 - val_mae: 2.4202 - val_mse: 6.6486\n",
      "Epoch 341/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6031 - mae: 0.6229 - mse: 0.6031 - val_loss: 6.7344 - val_mae: 2.4374 - val_mse: 6.7344\n",
      "Epoch 342/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5958 - mae: 0.6146 - mse: 0.5958 - val_loss: 6.7462 - val_mae: 2.4393 - val_mse: 6.7462\n",
      "Epoch 343/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.5992 - mae: 0.6202 - mse: 0.5992 - val_loss: 5.6658 - val_mae: 2.2202 - val_mse: 5.6658\n",
      "Epoch 344/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6037 - mae: 0.6275 - mse: 0.6037 - val_loss: 6.2489 - val_mae: 2.3400 - val_mse: 6.2489\n",
      "Epoch 345/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5985 - mae: 0.6126 - mse: 0.5985 - val_loss: 5.9314 - val_mae: 2.2739 - val_mse: 5.9314\n",
      "Epoch 346/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5961 - mae: 0.6150 - mse: 0.5961 - val_loss: 6.7581 - val_mae: 2.4421 - val_mse: 6.7581\n",
      "Epoch 347/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6000 - mae: 0.6179 - mse: 0.6000 - val_loss: 6.0585 - val_mae: 2.3017 - val_mse: 6.0585\n",
      "Epoch 348/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 0.5978 - mae: 0.6153 - mse: 0.5978 - val_loss: 6.1316 - val_mae: 2.3156 - val_mse: 6.1316\n",
      "Epoch 349/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.6014 - mae: 0.6241 - mse: 0.6014 - val_loss: 6.3733 - val_mae: 2.3647 - val_mse: 6.3733\n",
      "Epoch 350/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5967 - mae: 0.6139 - mse: 0.5967 - val_loss: 6.8619 - val_mae: 2.4602 - val_mse: 6.8619\n",
      "Epoch 351/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5982 - mae: 0.6181 - mse: 0.5982 - val_loss: 6.1881 - val_mae: 2.3276 - val_mse: 6.1881\n",
      "Epoch 352/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5972 - mae: 0.6148 - mse: 0.5972 - val_loss: 6.9822 - val_mae: 2.4832 - val_mse: 6.9822\n",
      "Epoch 353/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5990 - mae: 0.6152 - mse: 0.5990 - val_loss: 6.1437 - val_mae: 2.3189 - val_mse: 6.1437\n",
      "Epoch 354/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.5986 - mae: 0.6216 - mse: 0.5986 - val_loss: 6.7058 - val_mae: 2.4315 - val_mse: 6.7058\n",
      "Epoch 355/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5965 - mae: 0.6170 - mse: 0.5965 - val_loss: 6.5028 - val_mae: 2.3904 - val_mse: 6.5028\n",
      "Epoch 356/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.5943 - mae: 0.6160 - mse: 0.5943 - val_loss: 5.8850 - val_mae: 2.2661 - val_mse: 5.8850\n",
      "Epoch 357/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 0.6012 - mae: 0.6195 - mse: 0.6012 - val_loss: 6.9975 - val_mae: 2.4871 - val_mse: 6.9975\n",
      "Epoch 358/450\n",
      "2484/2484 [==============================] - 0s 93us/sample - loss: 0.5992 - mae: 0.6168 - mse: 0.5992 - val_loss: 5.9440 - val_mae: 2.2782 - val_mse: 5.9440\n",
      "Epoch 359/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 0.5979 - mae: 0.6168 - mse: 0.5979 - val_loss: 5.6458 - val_mae: 2.2157 - val_mse: 5.6458\n",
      "Epoch 360/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.6023 - mae: 0.6228 - mse: 0.6023 - val_loss: 5.6921 - val_mae: 2.2258 - val_mse: 5.6921\n",
      "Epoch 361/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 0.5980 - mae: 0.6148 - mse: 0.5980 - val_loss: 5.5203 - val_mae: 2.1888 - val_mse: 5.5203\n",
      "Epoch 362/450\n",
      "2484/2484 [==============================] - 0s 80us/sample - loss: 0.6007 - mae: 0.6206 - mse: 0.6007 - val_loss: 6.8717 - val_mae: 2.4641 - val_mse: 6.8717\n",
      "Epoch 363/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 0.5918 - mae: 0.6212 - mse: 0.5918 - val_loss: 7.1254 - val_mae: 2.5136 - val_mse: 7.1254\n",
      "Epoch 364/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.5941 - mae: 0.6172 - mse: 0.5941 - val_loss: 5.9011 - val_mae: 2.2689 - val_mse: 5.9011\n",
      "Epoch 365/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 0.5972 - mae: 0.6194 - mse: 0.5972 - val_loss: 6.0091 - val_mae: 2.2916 - val_mse: 6.0091\n",
      "Epoch 366/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5955 - mae: 0.6149 - mse: 0.5955 - val_loss: 5.9889 - val_mae: 2.2875 - val_mse: 5.9889\n",
      "Epoch 367/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.6002 - mae: 0.6243 - mse: 0.6002 - val_loss: 6.2639 - val_mae: 2.3422 - val_mse: 6.2639\n",
      "Epoch 368/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.5975 - mae: 0.6169 - mse: 0.5975 - val_loss: 5.8546 - val_mae: 2.2582 - val_mse: 5.8546\n",
      "Epoch 369/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.6008 - mae: 0.6235 - mse: 0.6008 - val_loss: 7.0704 - val_mae: 2.5000 - val_mse: 7.0704\n",
      "Epoch 370/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5976 - mae: 0.6172 - mse: 0.5976 - val_loss: 6.2142 - val_mae: 2.3331 - val_mse: 6.2142\n",
      "Epoch 371/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 0.5989 - mae: 0.6194 - mse: 0.5989 - val_loss: 5.9309 - val_mae: 2.2756 - val_mse: 5.9309\n",
      "Epoch 372/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 0.6021 - mae: 0.6219 - mse: 0.6021 - val_loss: 6.5443 - val_mae: 2.3984 - val_mse: 6.5443\n",
      "Epoch 373/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5962 - mae: 0.6144 - mse: 0.5962 - val_loss: 6.9770 - val_mae: 2.4840 - val_mse: 6.9770\n",
      "Epoch 374/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6006 - mae: 0.6232 - mse: 0.6006 - val_loss: 6.6169 - val_mae: 2.4121 - val_mse: 6.6169\n",
      "Epoch 375/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.5986 - mae: 0.6159 - mse: 0.5986 - val_loss: 6.0568 - val_mae: 2.2990 - val_mse: 6.0568\n",
      "Epoch 376/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.6007 - mae: 0.6217 - mse: 0.6007 - val_loss: 6.4215 - val_mae: 2.3737 - val_mse: 6.4215\n",
      "Epoch 377/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5926 - mae: 0.6146 - mse: 0.5926 - val_loss: 5.6535 - val_mae: 2.2177 - val_mse: 5.6535\n",
      "Epoch 378/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.6002 - mae: 0.6225 - mse: 0.6002 - val_loss: 5.7780 - val_mae: 2.2438 - val_mse: 5.7780\n",
      "Epoch 379/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5954 - mae: 0.6165 - mse: 0.5954 - val_loss: 6.1129 - val_mae: 2.3124 - val_mse: 6.1129\n",
      "Epoch 380/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.6031 - mae: 0.6247 - mse: 0.6031 - val_loss: 5.4389 - val_mae: 2.1716 - val_mse: 5.4389\n",
      "Epoch 381/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 0.6024 - mae: 0.6225 - mse: 0.6024 - val_loss: 6.5326 - val_mae: 2.3957 - val_mse: 6.5326\n",
      "Epoch 382/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6052 - mae: 0.6217 - mse: 0.6052 - val_loss: 6.2616 - val_mae: 2.3423 - val_mse: 6.2616\n",
      "Epoch 383/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.5960 - mae: 0.6119 - mse: 0.5960 - val_loss: 6.6532 - val_mae: 2.4191 - val_mse: 6.6532\n",
      "Epoch 384/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5970 - mae: 0.6197 - mse: 0.5970 - val_loss: 6.2878 - val_mae: 2.3477 - val_mse: 6.2878\n",
      "Epoch 385/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5991 - mae: 0.6159 - mse: 0.5991 - val_loss: 6.2015 - val_mae: 2.3300 - val_mse: 6.2015\n",
      "Epoch 386/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 0.5969 - mae: 0.6146 - mse: 0.5969 - val_loss: 6.0619 - val_mae: 2.3021 - val_mse: 6.0619\n",
      "Epoch 387/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 0.5966 - mae: 0.6164 - mse: 0.5966 - val_loss: 6.5303 - val_mae: 2.3940 - val_mse: 6.5303\n",
      "Epoch 388/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5977 - mae: 0.6163 - mse: 0.5977 - val_loss: 6.7807 - val_mae: 2.4465 - val_mse: 6.7807\n",
      "Epoch 389/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.5939 - mae: 0.6184 - mse: 0.5939 - val_loss: 6.6220 - val_mae: 2.4141 - val_mse: 6.6220\n",
      "Epoch 390/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.5966 - mae: 0.6201 - mse: 0.5966 - val_loss: 5.6912 - val_mae: 2.2256 - val_mse: 5.6912\n",
      "Epoch 391/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 0.5973 - mae: 0.6182 - mse: 0.5973 - val_loss: 6.0545 - val_mae: 2.3009 - val_mse: 6.0545\n",
      "Epoch 392/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 0.5938 - mae: 0.6159 - mse: 0.5938 - val_loss: 7.1847 - val_mae: 2.5159 - val_mse: 7.1847\n",
      "Epoch 393/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.5960 - mae: 0.6202 - mse: 0.5960 - val_loss: 6.2383 - val_mae: 2.3339 - val_mse: 6.2383\n",
      "Epoch 394/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 0.5975 - mae: 0.6154 - mse: 0.5975 - val_loss: 6.6768 - val_mae: 2.4237 - val_mse: 6.6768\n",
      "Epoch 395/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 0.5974 - mae: 0.6114 - mse: 0.5974 - val_loss: 6.9469 - val_mae: 2.4792 - val_mse: 6.9469\n",
      "Epoch 396/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5955 - mae: 0.6181 - mse: 0.5955 - val_loss: 7.3634 - val_mae: 2.5469 - val_mse: 7.3634\n",
      "Epoch 397/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5945 - mae: 0.6155 - mse: 0.5945 - val_loss: 6.6993 - val_mae: 2.4299 - val_mse: 6.6993\n",
      "Epoch 398/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5971 - mae: 0.6154 - mse: 0.5971 - val_loss: 5.9740 - val_mae: 2.2844 - val_mse: 5.9740\n",
      "Epoch 399/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 0.5971 - mae: 0.6158 - mse: 0.5971 - val_loss: 7.1559 - val_mae: 2.5178 - val_mse: 7.1559\n",
      "Epoch 400/450\n",
      "2484/2484 [==============================] - 0s 78us/sample - loss: 0.5943 - mae: 0.6197 - mse: 0.5943 - val_loss: 6.1683 - val_mae: 2.3230 - val_mse: 6.1683\n",
      "Epoch 401/450\n",
      "2484/2484 [==============================] - 0s 86us/sample - loss: 0.5962 - mae: 0.6230 - mse: 0.5962 - val_loss: 6.5230 - val_mae: 2.3906 - val_mse: 6.5230\n",
      "Epoch 402/450\n",
      "2484/2484 [==============================] - 0s 86us/sample - loss: 0.5994 - mae: 0.6180 - mse: 0.5994 - val_loss: 6.0146 - val_mae: 2.2924 - val_mse: 6.0146\n",
      "Epoch 403/450\n",
      "2484/2484 [==============================] - 0s 95us/sample - loss: 0.5968 - mae: 0.6158 - mse: 0.5968 - val_loss: 6.2246 - val_mae: 2.3351 - val_mse: 6.2246\n",
      "Epoch 404/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 0.5955 - mae: 0.6143 - mse: 0.5955 - val_loss: 6.7367 - val_mae: 2.4354 - val_mse: 6.7367\n",
      "Epoch 405/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.6016 - mae: 0.6229 - mse: 0.6016 - val_loss: 6.2096 - val_mae: 2.3322 - val_mse: 6.2096\n",
      "Epoch 406/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.5982 - mae: 0.6176 - mse: 0.5982 - val_loss: 6.4155 - val_mae: 2.3727 - val_mse: 6.4155\n",
      "Epoch 407/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.5973 - mae: 0.6161 - mse: 0.5973 - val_loss: 5.4908 - val_mae: 2.1828 - val_mse: 5.4908\n",
      "Epoch 408/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 0.6023 - mae: 0.6225 - mse: 0.6023 - val_loss: 6.6974 - val_mae: 2.4294 - val_mse: 6.6974\n",
      "Epoch 409/450\n",
      "2484/2484 [==============================] - 0s 108us/sample - loss: 0.5931 - mae: 0.6143 - mse: 0.5931 - val_loss: 5.8699 - val_mae: 2.2622 - val_mse: 5.8699\n",
      "Epoch 410/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 0.5937 - mae: 0.6165 - mse: 0.5937 - val_loss: 6.5278 - val_mae: 2.3958 - val_mse: 6.5278\n",
      "Epoch 411/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 0.6052 - mae: 0.6219 - mse: 0.6052 - val_loss: 6.3599 - val_mae: 2.3620 - val_mse: 6.3599\n",
      "Epoch 412/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 0.5947 - mae: 0.6116 - mse: 0.5947 - val_loss: 6.7273 - val_mae: 2.4342 - val_mse: 6.7273\n",
      "Epoch 413/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 0.5960 - mae: 0.6185 - mse: 0.5960 - val_loss: 5.6590 - val_mae: 2.2181 - val_mse: 5.6590\n",
      "Epoch 414/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5973 - mae: 0.6207 - mse: 0.5973 - val_loss: 6.4202 - val_mae: 2.3736 - val_mse: 6.4202\n",
      "Epoch 415/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.5981 - mae: 0.6200 - mse: 0.5981 - val_loss: 6.4899 - val_mae: 2.3879 - val_mse: 6.4899\n",
      "Epoch 416/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5943 - mae: 0.6139 - mse: 0.5943 - val_loss: 7.1996 - val_mae: 2.5276 - val_mse: 7.1996\n",
      "Epoch 417/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 0.6014 - mae: 0.6188 - mse: 0.6014 - val_loss: 6.6829 - val_mae: 2.4268 - val_mse: 6.6829\n",
      "Epoch 418/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.5985 - mae: 0.6195 - mse: 0.5985 - val_loss: 5.7656 - val_mae: 2.2412 - val_mse: 5.7656\n",
      "Epoch 419/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5974 - mae: 0.6127 - mse: 0.5974 - val_loss: 5.2989 - val_mae: 2.1410 - val_mse: 5.2989\n",
      "Epoch 420/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.5993 - mae: 0.6195 - mse: 0.5993 - val_loss: 6.7522 - val_mae: 2.4295 - val_mse: 6.7522\n",
      "Epoch 421/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.6006 - mae: 0.6161 - mse: 0.6006 - val_loss: 7.0759 - val_mae: 2.5028 - val_mse: 7.0759\n",
      "Epoch 422/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 0.5952 - mae: 0.6129 - mse: 0.5952 - val_loss: 6.1733 - val_mae: 2.3243 - val_mse: 6.1733\n",
      "Epoch 423/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 0.5996 - mae: 0.6188 - mse: 0.5996 - val_loss: 5.7111 - val_mae: 2.2298 - val_mse: 5.7111\n",
      "Epoch 424/450\n",
      "2484/2484 [==============================] - 0s 92us/sample - loss: 0.5985 - mae: 0.6177 - mse: 0.5985 - val_loss: 6.2283 - val_mae: 2.3359 - val_mse: 6.2283\n",
      "Epoch 425/450\n",
      "2484/2484 [==============================] - 0s 88us/sample - loss: 0.5951 - mae: 0.6165 - mse: 0.5951 - val_loss: 6.8371 - val_mae: 2.4577 - val_mse: 6.8371\n",
      "Epoch 426/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 0.5999 - mae: 0.6187 - mse: 0.5999 - val_loss: 6.6130 - val_mae: 2.4131 - val_mse: 6.6130\n",
      "Epoch 427/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 0.5960 - mae: 0.6158 - mse: 0.5960 - val_loss: 7.3093 - val_mae: 2.5455 - val_mse: 7.3093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.6050 - mae: 0.6259 - mse: 0.6050 - val_loss: 6.2283 - val_mae: 2.3359 - val_mse: 6.2283\n",
      "Epoch 429/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 0.5932 - mae: 0.6142 - mse: 0.5932 - val_loss: 6.7912 - val_mae: 2.4467 - val_mse: 6.7912\n",
      "Epoch 430/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 0.5963 - mae: 0.6180 - mse: 0.5963 - val_loss: 6.3961 - val_mae: 2.3688 - val_mse: 6.3961\n",
      "Epoch 431/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 0.5978 - mae: 0.6187 - mse: 0.5978 - val_loss: 5.7977 - val_mae: 2.2479 - val_mse: 5.7977\n",
      "Epoch 432/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.5994 - mae: 0.6175 - mse: 0.5994 - val_loss: 6.6487 - val_mae: 2.4183 - val_mse: 6.6487\n",
      "Epoch 433/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 0.5923 - mae: 0.6205 - mse: 0.5923 - val_loss: 6.3335 - val_mae: 2.3563 - val_mse: 6.3335\n",
      "Epoch 434/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 0.6004 - mae: 0.6215 - mse: 0.6004 - val_loss: 6.5875 - val_mae: 2.4008 - val_mse: 6.5875\n",
      "Epoch 435/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5954 - mae: 0.6142 - mse: 0.5954 - val_loss: 6.8259 - val_mae: 2.4543 - val_mse: 6.8259\n",
      "Epoch 436/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.6033 - mae: 0.6203 - mse: 0.6033 - val_loss: 6.5208 - val_mae: 2.3943 - val_mse: 6.5208\n",
      "Epoch 437/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 0.5994 - mae: 0.6145 - mse: 0.5994 - val_loss: 6.3840 - val_mae: 2.3646 - val_mse: 6.3840\n",
      "Epoch 438/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.5962 - mae: 0.6169 - mse: 0.5962 - val_loss: 5.6283 - val_mae: 2.2083 - val_mse: 5.6283\n",
      "Epoch 439/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5996 - mae: 0.6192 - mse: 0.5996 - val_loss: 6.8888 - val_mae: 2.4666 - val_mse: 6.8888\n",
      "Epoch 440/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 0.5965 - mae: 0.6176 - mse: 0.5965 - val_loss: 5.8815 - val_mae: 2.2563 - val_mse: 5.8815\n",
      "Epoch 441/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 0.6009 - mae: 0.6212 - mse: 0.6009 - val_loss: 5.7552 - val_mae: 2.2388 - val_mse: 5.7552\n",
      "Epoch 442/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 0.5983 - mae: 0.6183 - mse: 0.5983 - val_loss: 5.6335 - val_mae: 2.2132 - val_mse: 5.6335\n",
      "Epoch 443/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 0.5982 - mae: 0.6139 - mse: 0.5982 - val_loss: 6.2629 - val_mae: 2.3360 - val_mse: 6.2629\n",
      "Epoch 444/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 0.5927 - mae: 0.6193 - mse: 0.5927 - val_loss: 7.2176 - val_mae: 2.5309 - val_mse: 7.2176\n",
      "Epoch 445/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 0.5974 - mae: 0.6211 - mse: 0.5974 - val_loss: 6.6227 - val_mae: 2.4112 - val_mse: 6.6227\n",
      "Epoch 446/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.5978 - mae: 0.6149 - mse: 0.5978 - val_loss: 5.9051 - val_mae: 2.2695 - val_mse: 5.9051\n",
      "Epoch 447/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 0.5954 - mae: 0.6142 - mse: 0.5954 - val_loss: 5.5612 - val_mae: 2.1979 - val_mse: 5.5612\n",
      "Epoch 448/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 0.6004 - mae: 0.6164 - mse: 0.6004 - val_loss: 6.3327 - val_mae: 2.3307 - val_mse: 6.3327\n",
      "Epoch 449/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 0.6023 - mae: 0.6201 - mse: 0.6023 - val_loss: 7.1759 - val_mae: 2.5222 - val_mse: 7.1759\n",
      "Epoch 450/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 0.5973 - mae: 0.6192 - mse: 0.5973 - val_loss: 5.9971 - val_mae: 2.2857 - val_mse: 5.9971\n"
     ]
    }
   ],
   "source": [
    "trained_weight = ks.get_weights()[0]\n",
    "trained_bias = ks.get_weights()[1]\n",
    "\n",
    "EPOCHS = 450\n",
    "history = ks.fit(X_train,\n",
    "                 y_train,\n",
    "                 epochs = EPOCHS,\n",
    "                 batch_size = 128,\n",
    "                 validation_split = 0.2,\n",
    "                 verbose = 1)\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist['mse']\n",
    "epochs = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    plt.plot(feature.tolist(), label.tolist(), c='r')\n",
    "    # Render the scatter plot and the red line.\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.97, mse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature = X['all'].copy()\n",
    "#label = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_the_model(trained_weight, trained_bias, feature, label)\n",
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              loss         mae          mse    val_loss     val_mae  \\\n",
      "count   450.000000  450.000000   450.000000  450.000000  450.000000   \n",
      "mean     10.702289    0.655160    10.702290    6.357828    2.359509   \n",
      "std     214.248179    0.731521   214.248201    0.548966    0.107965   \n",
      "min       0.591830    0.608798     0.591830    4.520980    1.959320   \n",
      "25%       0.596810    0.616190     0.596810    5.998913    2.289028   \n",
      "50%       0.599018    0.618903     0.599018    6.333093    2.354081   \n",
      "75%       0.601862    0.621454     0.601862    6.648630    2.419782   \n",
      "max    4545.492701   16.135311  4545.493164   10.587625    3.103235   \n",
      "\n",
      "          val_mse  \n",
      "count  450.000000  \n",
      "mean     6.357828  \n",
      "std      0.548966  \n",
      "min      4.520980  \n",
      "25%      5.998913  \n",
      "50%      6.333093  \n",
      "75%      6.648630  \n",
      "max     10.587625  \n"
     ]
    }
   ],
   "source": [
    "print(hist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(a,b):\n",
    "    #print('pred :',a,'actual :',b)\n",
    "    if a == b:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.030552001390400704\n",
      "dt 0.11312758360760354\n",
      "rf 0.490246160220455\n",
      "vr 0.26288883802891905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ks_test = ks.evaluate(X_train, y_train,verbose=0)\\nprint('ks',ks_test[1])\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('lr',lr.score(X_train, y_train))\n",
    "print('dt',dt.score(X_train, y_train))\n",
    "print('rf',rf.score(X_train, y_train))\n",
    "print('vr',vr.score(X_train, y_train))\n",
    "'''ks_test = ks.evaluate(X_train, y_train,verbose=0)\n",
    "print('ks',ks_test[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_pred_test(result,num):\n",
    "    score = check(result,y_test.loc[num])\n",
    "    return score\n",
    "\n",
    "def model_pred_test(model):\n",
    "    pred = []\n",
    "    numbers = X_test.index\n",
    "    random_nums = random.choices(numbers, k=50)\n",
    "    for i in random_nums:\n",
    "        p = X_test.loc[i].tolist()\n",
    "        result = model.predict([p]).flatten().round()\n",
    "        prediction = cycle_pred_test(result,i)\n",
    "        pred.append(prediction)\n",
    "    dz = pd.DataFrame(pred)\n",
    "    #df = pd.DataFrame(prob)\n",
    "    c = str(float(dz.sum().values / 50))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(index=range(10),columns=['vr','dt','rf','ks'])\n",
    "test_results = test_results.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    v_s = model_pred_test(vr)\n",
    "    test_results.at[i,'vr'] = v_s\n",
    "    d_s = model_pred_test(dt)\n",
    "    test_results.at[i,'dt'] = d_s\n",
    "    r_s = model_pred_test(rf)\n",
    "    test_results.at[i,'rf'] = r_s\n",
    "    k_s = model_pred_test(ks)\n",
    "    test_results.at[i,'ks'] = k_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vr</th>\n",
       "      <th>dt</th>\n",
       "      <th>rf</th>\n",
       "      <th>ks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.25600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.088969</td>\n",
       "      <td>0.064944</td>\n",
       "      <td>0.041366</td>\n",
       "      <td>0.04402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.18000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.22500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.29500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.32000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              vr         dt         rf        ks\n",
       "count  10.000000  10.000000  10.000000  10.00000\n",
       "mean    0.334000   0.278000   0.310000   0.25600\n",
       "std     0.088969   0.064944   0.041366   0.04402\n",
       "min     0.180000   0.200000   0.260000   0.18000\n",
       "25%     0.280000   0.240000   0.280000   0.22500\n",
       "50%     0.350000   0.250000   0.300000   0.25000\n",
       "75%     0.390000   0.340000   0.355000   0.29500\n",
       "max     0.460000   0.380000   0.360000   0.32000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.iloc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    e = e[0]\n",
    "    if e < 1:\n",
    "        e = 0\n",
    "    elif e < 2:\n",
    "        e = 1\n",
    "    return e\n",
    "\n",
    "def model_pred_test(model):\n",
    "    b = []\n",
    "    prob = []\n",
    "    random_nums = np.random.randint(low=1, high=58, size=(20))\n",
    "    for i in random_nums:\n",
    "        prob.append(cycle_prob_test(i,model))\n",
    "    df = pd.DataFrame(prob)\n",
    "    df = df.values\n",
    "    print('scores :\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_score_regressor.sav'\n",
    "pickle.dump(vr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import cpl_main as cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forge FC Cavalry FC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todd/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "#model_pred_test(cpl_classifier_model)\n",
    "\n",
    "# home side\n",
    "q1 = schedule.iloc[0]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[0]['away']\n",
    "print(q1,q2)\n",
    "\n",
    "compare = cpl.get_team_comparison(results_brief,q1,q2)\n",
    "\n",
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)\n",
    "\n",
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "game_info\n",
    "\n",
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)\n",
    "\n",
    "home_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "away_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.33\n"
     ]
    }
   ],
   "source": [
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db\n",
    "\n",
    "q1_roster = get_overall_roster(home_roster)\n",
    "#print(q1_roster)\n",
    "\n",
    "q2_roster = get_overall_roster(away_roster)\n",
    "#print(q2_roster)\n",
    "\n",
    "def roster_regressor_pred(model,array):\n",
    "    prediction = model.predict([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    return df\n",
    "\n",
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)\n",
    "print(home_win, away_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37\n",
      "0.34\n",
      "\n",
      " Forge FC \n",
      "win probability:  0.37\n",
      "\n",
      " Cavalry FC \n",
      "win probability:  0.34\n",
      "\n",
      "Draw probability:  0.29\n"
     ]
    }
   ],
   "source": [
    "classifier = 'models/cpl_roster_classifier.sav'\n",
    "cpl_classifier_model = pickle.load(open(classifier, 'rb'))\n",
    "\n",
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(home_win_new)\n",
    "print(away_win_new)\n",
    "\n",
    "print('\\n',q1,'\\nwin probability: ', round(home_win_new,2))\n",
    "\n",
    "print('\\n',q2,'\\nwin probability: ', round(away_win_new,2))\n",
    "\n",
    "print('\\nDraw probability: ', round(draw_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/cpl_score_regressor.sav'\n",
    "cpl_score_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_score_prediction(model,q1_roster,q2_roster,home_win_new,away_win_new):\n",
    "\n",
    "    def roster_pred(model,array):\n",
    "        prediction = model.predict([array]).flatten()\n",
    "        return prediction\n",
    "\n",
    "    def final_score_fix(home_score,away_score,home_win_new,away_win_new):\n",
    "        if home_win_new > away_win_new and home_score < away_score: # fix the score prediction - if the probability of home win > away win and score doesn't reflect it\n",
    "            old_home = home_score\n",
    "            home_score = away_score # change the predicted score to reflect that\n",
    "            away_score = old_home\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score > away_score: # else the probability of home win < away win\n",
    "            old_away = away_score\n",
    "            away_score = home_score # change the predicted score to reflect that\n",
    "            home_score = away_score\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new > away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        else:\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "\n",
    "    def score(num): #improve this later for greater predictions\n",
    "        new_score = int(round(num,0)) # convert the float value to int and round it\n",
    "        return new_score\n",
    "\n",
    "    q1_pred = roster_pred(model,q1_roster)\n",
    "    q1_s = score(q1_pred[0])\n",
    "    q2_pred = roster_pred(model,q2_roster)\n",
    "    q2_s = score(q2_pred[0])\n",
    "    home_score, away_score, home_win_new, away_win_new = final_score_fix(q1_s, q2_s,home_win_new,away_win_new)\n",
    "    return home_score,away_score, home_win_new, away_win_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 15 is different from 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-9b4632a2a2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhome_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maway_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhome_win_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maway_win_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_final_score_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpl_score_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq1_roster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2_roster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhome_win_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maway_win_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'home'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhome_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'away'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maway_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-4038dfae14a3>\u001b[0m in \u001b[0;36mget_final_score_prediction\u001b[0;34m(model, q1_roster, q2_roster, home_win_new, away_win_new)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mq1_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroster_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq1_roster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mq1_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mq2_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroster_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2_roster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-4038dfae14a3>\u001b[0m in \u001b[0;36mroster_pred\u001b[0;34m(model, array)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mroster_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m    425\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         return np.average(self._predict(X), axis=1,\n\u001b[0m\u001b[1;32m    427\u001b[0m                           weights=self._weights_not_none)\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 209\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 15 is different from 14)"
     ]
    }
   ],
   "source": [
    "home_score, away_score, home_win_new, away_win_new = get_final_score_prediction(cpl_score_model,q1_roster,q2_roster,home_win_new,away_win_new)\n",
    "print('home',home_score,'away', away_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = q1_roster.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = l.values.tolist()\n",
    "l = l[0]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "average = statistics.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4421428516507149"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-136c17fc3cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "l.extend(average)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
