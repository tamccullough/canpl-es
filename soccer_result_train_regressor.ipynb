{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todd McCullough\n",
    "# 2020\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game      team    p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1  \\\n",
      "0   I1  Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48   \n",
      "1   I1  York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70   \n",
      "\n",
      "    p10  p11  p12  p13  r  s  \n",
      "0  0.42  0.0  0.0  0.0  2  1  \n",
      "1  0.47  0.0  0.0  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')\n",
    "print(results.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pump_it_up(results)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "y = db.pop('s')\n",
    "db.pop('r')\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
      "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
      "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
      "\n",
      "   p13  \n",
      "0  0.0  \n",
      "1  0.0  \n"
     ]
    }
   ],
   "source": [
    "print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model\n",
      "\n",
      "RMSE:  1.22284599419792\n",
      "\n",
      "Score 1.72\n",
      "\n",
      "Decision Tree Regression Model\n",
      "\n",
      "RMSE:  1.2530288819180666\n",
      "\n",
      "Score -3.19\n",
      "\n",
      "Random Forest Regression Model\n",
      "\n",
      "RMSE:  1.2241685812507441\n",
      "\n",
      "Score 1.51\n",
      "\n",
      "Voting Regressor Model\n",
      "\n",
      "RMSE:  1.220778810855711\n",
      "\n",
      "Score 2.05\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model\n",
    "def linearRegression():\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "lr = linearRegression()\n",
    "\n",
    "print('\\nLinear Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n",
    "print('\\nScore',round(lr.score(X_test, y_test)*100,2))\n",
    "\n",
    "#DecisionTreeRegressor\n",
    "def decisionTree():\n",
    "    model = DecisionTreeRegressor(criterion='mse', splitter='random', max_depth=8, max_features='log2')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "dt = decisionTree()\n",
    "\n",
    "print('\\nDecision Tree Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test, dt.predict(X_test))))\n",
    "print('\\nScore',round(dt.score(X_test, y_test)*100,2))\n",
    "\n",
    "#Random Forest Regression\n",
    "def forestRegression():\n",
    "    model = RandomForestRegressor(n_estimators = 500, random_state = 42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "rf = forestRegression()\n",
    "\n",
    "print('\\nRandom Forest Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,rf.predict(X_test))))\n",
    "print('\\nScore',round(rf.score(X_test, y_test)*100,2))\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "vr = VotingRegressor(estimators=[('lr', lr), ('dt', dt), ('rf', rf)])\n",
    "vr = vr.fit(X_train, y_train)\n",
    "\n",
    "print('\\nVoting Regressor Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,vr.predict(X_test))))\n",
    "print('\\nScore',round(vr.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasSequential():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                optimizer = tf.keras.optimizers.RMSprop(0.1),\n",
    "                metrics = ['mae', 'mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,185\n",
      "Trainable params: 5,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ks = kerasSequential()\n",
    "print(ks.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2174 samples, validate on 544 samples\n",
      "Epoch 1/450\n",
      "2174/2174 [==============================] - 1s 625us/sample - loss: 3103.4846 - mae: 14.4831 - mse: 3103.4851 - val_loss: 1.5962 - val_mae: 1.0061 - val_mse: 1.5962\n",
      "Epoch 2/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5693 - mae: 1.0037 - mse: 1.5693 - val_loss: 1.7614 - val_mae: 1.1044 - val_mse: 1.7614\n",
      "Epoch 3/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5765 - mae: 1.0071 - mse: 1.5765 - val_loss: 1.7787 - val_mae: 0.9651 - val_mse: 1.7787\n",
      "Epoch 4/450\n",
      "2174/2174 [==============================] - 0s 94us/sample - loss: 1.5750 - mae: 1.0080 - mse: 1.5750 - val_loss: 1.6101 - val_mae: 0.9969 - val_mse: 1.6101\n",
      "Epoch 5/450\n",
      "2174/2174 [==============================] - 0s 87us/sample - loss: 1.5982 - mae: 1.0088 - mse: 1.5982 - val_loss: 1.7806 - val_mae: 0.9626 - val_mse: 1.7806\n",
      "Epoch 6/450\n",
      "2174/2174 [==============================] - 0s 83us/sample - loss: 1.6020 - mae: 1.0088 - mse: 1.6020 - val_loss: 2.1016 - val_mae: 1.0725 - val_mse: 2.1016\n",
      "Epoch 7/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.6348 - mae: 1.0297 - mse: 1.6348 - val_loss: 1.6207 - val_mae: 1.0026 - val_mse: 1.6207\n",
      "Epoch 8/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.6304 - mae: 1.0352 - mse: 1.6304 - val_loss: 1.6128 - val_mae: 1.0227 - val_mse: 1.6128\n",
      "Epoch 9/450\n",
      "2174/2174 [==============================] - 0s 67us/sample - loss: 1.5784 - mae: 1.0226 - mse: 1.5784 - val_loss: 1.6979 - val_mae: 1.0881 - val_mse: 1.6979\n",
      "Epoch 10/450\n",
      "2174/2174 [==============================] - 0s 71us/sample - loss: 1.6087 - mae: 1.0180 - mse: 1.6087 - val_loss: 1.6801 - val_mae: 1.0826 - val_mse: 1.6801\n",
      "Epoch 11/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.6227 - mae: 1.0179 - mse: 1.6227 - val_loss: 1.6132 - val_mae: 1.0257 - val_mse: 1.6132\n",
      "Epoch 12/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.8010 - mae: 1.0790 - mse: 1.8010 - val_loss: 1.6156 - val_mae: 1.0285 - val_mse: 1.6156\n",
      "Epoch 13/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.6120 - mae: 1.0325 - mse: 1.6120 - val_loss: 1.6185 - val_mae: 1.0125 - val_mse: 1.6185\n",
      "Epoch 14/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5407 - mae: 1.0147 - mse: 1.5407 - val_loss: 1.6311 - val_mae: 0.9996 - val_mse: 1.6311\n",
      "Epoch 15/450\n",
      "2174/2174 [==============================] - 0s 70us/sample - loss: 1.5551 - mae: 1.0193 - mse: 1.5551 - val_loss: 1.6197 - val_mae: 1.0117 - val_mse: 1.6197\n",
      "Epoch 16/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5541 - mae: 1.0165 - mse: 1.5541 - val_loss: 1.6312 - val_mae: 1.0549 - val_mse: 1.6312\n",
      "Epoch 17/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5366 - mae: 1.0164 - mse: 1.5366 - val_loss: 1.6153 - val_mae: 1.0364 - val_mse: 1.6153\n",
      "Epoch 18/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5369 - mae: 1.0128 - mse: 1.5369 - val_loss: 1.6161 - val_mae: 1.0159 - val_mse: 1.6161\n",
      "Epoch 19/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5379 - mae: 1.0089 - mse: 1.5379 - val_loss: 1.6245 - val_mae: 1.0493 - val_mse: 1.6245\n",
      "Epoch 20/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5354 - mae: 1.0144 - mse: 1.5354 - val_loss: 1.6136 - val_mae: 1.0302 - val_mse: 1.6136\n",
      "Epoch 21/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5339 - mae: 1.0165 - mse: 1.5339 - val_loss: 1.6135 - val_mae: 1.0264 - val_mse: 1.6135\n",
      "Epoch 22/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5284 - mae: 1.0120 - mse: 1.5284 - val_loss: 1.6693 - val_mae: 1.0769 - val_mse: 1.6693\n",
      "Epoch 23/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5286 - mae: 1.0180 - mse: 1.5286 - val_loss: 1.6161 - val_mae: 1.0170 - val_mse: 1.6161\n",
      "Epoch 24/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5272 - mae: 1.0137 - mse: 1.5272 - val_loss: 1.6147 - val_mae: 1.0346 - val_mse: 1.6147\n",
      "Epoch 25/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5325 - mae: 1.0165 - mse: 1.5325 - val_loss: 1.6204 - val_mae: 1.0448 - val_mse: 1.6204\n",
      "Epoch 26/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5271 - mae: 1.0139 - mse: 1.5271 - val_loss: 1.6407 - val_mae: 1.0620 - val_mse: 1.6407\n",
      "Epoch 27/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5301 - mae: 1.0182 - mse: 1.5301 - val_loss: 1.6144 - val_mae: 1.0212 - val_mse: 1.6144\n",
      "Epoch 28/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5337 - mae: 1.0166 - mse: 1.5337 - val_loss: 1.6261 - val_mae: 1.0510 - val_mse: 1.6261\n",
      "Epoch 29/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5314 - mae: 1.0197 - mse: 1.5314 - val_loss: 1.6136 - val_mae: 1.0261 - val_mse: 1.6136\n",
      "Epoch 30/450\n",
      "2174/2174 [==============================] - 0s 74us/sample - loss: 1.5238 - mae: 1.0097 - mse: 1.5238 - val_loss: 1.6668 - val_mae: 1.0758 - val_mse: 1.6668\n",
      "Epoch 31/450\n",
      "2174/2174 [==============================] - 0s 86us/sample - loss: 1.5286 - mae: 1.0191 - mse: 1.5286 - val_loss: 1.6214 - val_mae: 1.0089 - val_mse: 1.6214\n",
      "Epoch 32/450\n",
      "2174/2174 [==============================] - 0s 71us/sample - loss: 1.5312 - mae: 1.0169 - mse: 1.5312 - val_loss: 1.6240 - val_mae: 1.0060 - val_mse: 1.6240\n",
      "Epoch 33/450\n",
      "2174/2174 [==============================] - 0s 91us/sample - loss: 1.5360 - mae: 1.0163 - mse: 1.5360 - val_loss: 1.6148 - val_mae: 1.0348 - val_mse: 1.6148\n",
      "Epoch 34/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5254 - mae: 1.0158 - mse: 1.5254 - val_loss: 1.6154 - val_mae: 1.0366 - val_mse: 1.6154\n",
      "Epoch 35/450\n",
      "2174/2174 [==============================] - 0s 101us/sample - loss: 1.5331 - mae: 1.0151 - mse: 1.5331 - val_loss: 1.6535 - val_mae: 1.0694 - val_mse: 1.6535\n",
      "Epoch 36/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5270 - mae: 1.0164 - mse: 1.5270 - val_loss: 1.6513 - val_mae: 1.0682 - val_mse: 1.6513\n",
      "Epoch 37/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5261 - mae: 1.0128 - mse: 1.5261 - val_loss: 1.6480 - val_mae: 1.0664 - val_mse: 1.6480\n",
      "Epoch 38/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5329 - mae: 1.0204 - mse: 1.5329 - val_loss: 1.6150 - val_mae: 1.0194 - val_mse: 1.6150\n",
      "Epoch 39/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5329 - mae: 1.0179 - mse: 1.5329 - val_loss: 1.6140 - val_mae: 1.0229 - val_mse: 1.6140\n",
      "Epoch 40/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5293 - mae: 1.0151 - mse: 1.5293 - val_loss: 1.6172 - val_mae: 1.0148 - val_mse: 1.6172\n",
      "Epoch 41/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5276 - mae: 1.0157 - mse: 1.5276 - val_loss: 1.6189 - val_mae: 1.0428 - val_mse: 1.6189\n",
      "Epoch 42/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5277 - mae: 1.0156 - mse: 1.5277 - val_loss: 1.6395 - val_mae: 1.0612 - val_mse: 1.6395\n",
      "Epoch 43/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5260 - mae: 1.0149 - mse: 1.5260 - val_loss: 1.6165 - val_mae: 1.0389 - val_mse: 1.6165\n",
      "Epoch 44/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5284 - mae: 1.0152 - mse: 1.5284 - val_loss: 1.6150 - val_mae: 1.0354 - val_mse: 1.6150\n",
      "Epoch 45/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5263 - mae: 1.0168 - mse: 1.5263 - val_loss: 1.6147 - val_mae: 1.0204 - val_mse: 1.6147\n",
      "Epoch 46/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5343 - mae: 1.0206 - mse: 1.5343 - val_loss: 1.6152 - val_mae: 1.0359 - val_mse: 1.6152\n",
      "Epoch 47/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5300 - mae: 1.0156 - mse: 1.5300 - val_loss: 1.6175 - val_mae: 1.0406 - val_mse: 1.6175\n",
      "Epoch 48/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5322 - mae: 1.0146 - mse: 1.5322 - val_loss: 1.6373 - val_mae: 1.0598 - val_mse: 1.6373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5394 - mae: 1.0221 - mse: 1.5394 - val_loss: 1.6156 - val_mae: 1.0179 - val_mse: 1.6156\n",
      "Epoch 50/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5264 - mae: 1.0139 - mse: 1.5264 - val_loss: 1.6305 - val_mae: 1.0548 - val_mse: 1.6305\n",
      "Epoch 51/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5362 - mae: 1.0181 - mse: 1.5362 - val_loss: 1.6278 - val_mae: 1.0525 - val_mse: 1.6278\n",
      "Epoch 52/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5347 - mae: 1.0187 - mse: 1.5347 - val_loss: 1.6203 - val_mae: 1.0447 - val_mse: 1.6203\n",
      "Epoch 53/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5276 - mae: 1.0156 - mse: 1.5276 - val_loss: 1.6391 - val_mae: 1.0609 - val_mse: 1.6391\n",
      "Epoch 54/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5292 - mae: 1.0184 - mse: 1.5292 - val_loss: 1.6248 - val_mae: 1.0052 - val_mse: 1.6248\n",
      "Epoch 55/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5310 - mae: 1.0123 - mse: 1.5310 - val_loss: 1.6405 - val_mae: 1.0619 - val_mse: 1.6405\n",
      "Epoch 56/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5283 - mae: 1.0155 - mse: 1.5283 - val_loss: 1.6329 - val_mae: 1.0566 - val_mse: 1.6329\n",
      "Epoch 57/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5330 - mae: 1.0184 - mse: 1.5330 - val_loss: 1.6188 - val_mae: 1.0426 - val_mse: 1.6188\n",
      "Epoch 58/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5249 - mae: 1.0177 - mse: 1.5249 - val_loss: 1.6402 - val_mae: 0.9932 - val_mse: 1.6402\n",
      "Epoch 59/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5338 - mae: 1.0130 - mse: 1.5338 - val_loss: 1.6316 - val_mae: 1.0556 - val_mse: 1.6316\n",
      "Epoch 60/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5268 - mae: 1.0175 - mse: 1.5268 - val_loss: 1.6360 - val_mae: 1.0588 - val_mse: 1.6360\n",
      "Epoch 61/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5360 - mae: 1.0201 - mse: 1.5360 - val_loss: 1.6157 - val_mae: 1.0373 - val_mse: 1.6157\n",
      "Epoch 62/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5294 - mae: 1.0171 - mse: 1.5294 - val_loss: 1.6170 - val_mae: 1.0398 - val_mse: 1.6170\n",
      "Epoch 63/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5244 - mae: 1.0145 - mse: 1.5244 - val_loss: 1.6279 - val_mae: 1.0526 - val_mse: 1.6279\n",
      "Epoch 64/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5369 - mae: 1.0179 - mse: 1.5369 - val_loss: 1.6178 - val_mae: 1.0411 - val_mse: 1.6178\n",
      "Epoch 65/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5350 - mae: 1.0183 - mse: 1.5350 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 66/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5327 - mae: 1.0169 - mse: 1.5327 - val_loss: 1.6211 - val_mae: 1.0457 - val_mse: 1.6211\n",
      "Epoch 67/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5277 - mae: 1.0183 - mse: 1.5277 - val_loss: 1.6149 - val_mae: 1.0196 - val_mse: 1.6149\n",
      "Epoch 68/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5259 - mae: 1.0140 - mse: 1.5259 - val_loss: 1.6142 - val_mae: 1.0327 - val_mse: 1.6142\n",
      "Epoch 69/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5337 - mae: 1.0160 - mse: 1.5337 - val_loss: 1.6242 - val_mae: 1.0491 - val_mse: 1.6242\n",
      "Epoch 70/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5300 - mae: 1.0172 - mse: 1.5300 - val_loss: 1.6295 - val_mae: 1.0539 - val_mse: 1.6295\n",
      "Epoch 71/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5255 - mae: 1.0174 - mse: 1.5255 - val_loss: 1.6399 - val_mae: 0.9934 - val_mse: 1.6399\n",
      "Epoch 72/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5332 - mae: 1.0138 - mse: 1.5332 - val_loss: 1.6153 - val_mae: 1.0363 - val_mse: 1.6153\n",
      "Epoch 73/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5332 - mae: 1.0189 - mse: 1.5332 - val_loss: 1.6155 - val_mae: 1.0367 - val_mse: 1.6155\n",
      "Epoch 74/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5243 - mae: 1.0140 - mse: 1.5243 - val_loss: 1.6137 - val_mae: 1.0304 - val_mse: 1.6137\n",
      "Epoch 75/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5331 - mae: 1.0175 - mse: 1.5331 - val_loss: 1.6176 - val_mae: 1.0408 - val_mse: 1.6176\n",
      "Epoch 76/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5292 - mae: 1.0158 - mse: 1.5292 - val_loss: 1.6294 - val_mae: 1.0538 - val_mse: 1.6294\n",
      "Epoch 77/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5269 - mae: 1.0189 - mse: 1.5269 - val_loss: 1.6216 - val_mae: 1.0087 - val_mse: 1.6216\n",
      "Epoch 78/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5271 - mae: 1.0107 - mse: 1.5271 - val_loss: 1.6416 - val_mae: 1.0626 - val_mse: 1.6416\n",
      "Epoch 79/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5276 - mae: 1.0168 - mse: 1.5276 - val_loss: 1.6483 - val_mae: 1.0666 - val_mse: 1.6483\n",
      "Epoch 80/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5317 - mae: 1.0151 - mse: 1.5317 - val_loss: 1.6709 - val_mae: 1.0776 - val_mse: 1.6709\n",
      "Epoch 81/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5313 - mae: 1.0187 - mse: 1.5313 - val_loss: 1.6139 - val_mae: 1.0313 - val_mse: 1.6139\n",
      "Epoch 82/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5262 - mae: 1.0149 - mse: 1.5262 - val_loss: 1.6136 - val_mae: 1.0293 - val_mse: 1.6136\n",
      "Epoch 83/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5333 - mae: 1.0173 - mse: 1.5333 - val_loss: 1.6209 - val_mae: 1.0454 - val_mse: 1.6209\n",
      "Epoch 84/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5285 - mae: 1.0153 - mse: 1.5285 - val_loss: 1.6143 - val_mae: 1.0334 - val_mse: 1.6143\n",
      "Epoch 85/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5308 - mae: 1.0157 - mse: 1.5308 - val_loss: 1.6243 - val_mae: 1.0492 - val_mse: 1.6243\n",
      "Epoch 86/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5304 - mae: 1.0179 - mse: 1.5304 - val_loss: 1.6196 - val_mae: 1.0438 - val_mse: 1.6196\n",
      "Epoch 87/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5359 - mae: 1.0176 - mse: 1.5359 - val_loss: 1.6136 - val_mae: 1.0290 - val_mse: 1.6136\n",
      "Epoch 88/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5273 - mae: 1.0163 - mse: 1.5273 - val_loss: 1.6163 - val_mae: 1.0165 - val_mse: 1.6163\n",
      "Epoch 89/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5305 - mae: 1.0146 - mse: 1.5305 - val_loss: 1.6173 - val_mae: 1.0404 - val_mse: 1.6173\n",
      "Epoch 90/450\n",
      "2174/2174 [==============================] - 0s 74us/sample - loss: 1.5251 - mae: 1.0123 - mse: 1.5251 - val_loss: 1.6153 - val_mae: 1.0362 - val_mse: 1.6153\n",
      "Epoch 91/450\n",
      "2174/2174 [==============================] - 0s 113us/sample - loss: 1.5310 - mae: 1.0143 - mse: 1.5310 - val_loss: 1.6296 - val_mae: 1.0540 - val_mse: 1.6296\n",
      "Epoch 92/450\n",
      "2174/2174 [==============================] - 0s 78us/sample - loss: 1.5275 - mae: 1.0164 - mse: 1.5275 - val_loss: 1.6180 - val_mae: 1.0414 - val_mse: 1.6180\n",
      "Epoch 93/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5310 - mae: 1.0206 - mse: 1.5310 - val_loss: 1.6372 - val_mae: 0.9952 - val_mse: 1.6372\n",
      "Epoch 94/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5270 - mae: 1.0131 - mse: 1.5270 - val_loss: 1.6141 - val_mae: 1.0322 - val_mse: 1.6141\n",
      "Epoch 95/450\n",
      "2174/2174 [==============================] - 0s 70us/sample - loss: 1.5300 - mae: 1.0149 - mse: 1.5300 - val_loss: 1.6423 - val_mae: 1.0630 - val_mse: 1.6423\n",
      "Epoch 96/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5230 - mae: 1.0170 - mse: 1.5230 - val_loss: 1.6187 - val_mae: 1.0425 - val_mse: 1.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5338 - mae: 1.0149 - mse: 1.5338 - val_loss: 1.6313 - val_mae: 1.0554 - val_mse: 1.6313\n",
      "Epoch 98/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5250 - mae: 1.0169 - mse: 1.5250 - val_loss: 1.6136 - val_mae: 1.0283 - val_mse: 1.6136\n",
      "Epoch 99/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5306 - mae: 1.0189 - mse: 1.5306 - val_loss: 1.6135 - val_mae: 1.0274 - val_mse: 1.6135\n",
      "Epoch 100/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5340 - mae: 1.0150 - mse: 1.5340 - val_loss: 1.6327 - val_mae: 1.0565 - val_mse: 1.6327\n",
      "Epoch 101/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5262 - mae: 1.0164 - mse: 1.5262 - val_loss: 1.6135 - val_mae: 1.0269 - val_mse: 1.6135\n",
      "Epoch 102/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5286 - mae: 1.0143 - mse: 1.5286 - val_loss: 1.6320 - val_mae: 1.0559 - val_mse: 1.6320\n",
      "Epoch 103/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5316 - mae: 1.0155 - mse: 1.5316 - val_loss: 1.6167 - val_mae: 1.0392 - val_mse: 1.6167\n",
      "Epoch 104/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5302 - mae: 1.0159 - mse: 1.5302 - val_loss: 1.6209 - val_mae: 1.0455 - val_mse: 1.6209\n",
      "Epoch 105/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5265 - mae: 1.0178 - mse: 1.5265 - val_loss: 1.6384 - val_mae: 0.9944 - val_mse: 1.6384\n",
      "Epoch 106/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5329 - mae: 1.0151 - mse: 1.5329 - val_loss: 1.6139 - val_mae: 1.0237 - val_mse: 1.6139\n",
      "Epoch 107/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5309 - mae: 1.0172 - mse: 1.5309 - val_loss: 1.6163 - val_mae: 1.0164 - val_mse: 1.6163\n",
      "Epoch 108/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5298 - mae: 1.0141 - mse: 1.5298 - val_loss: 1.6183 - val_mae: 1.0419 - val_mse: 1.6183\n",
      "Epoch 109/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5329 - mae: 1.0193 - mse: 1.5329 - val_loss: 1.6172 - val_mae: 1.0147 - val_mse: 1.6172\n",
      "Epoch 110/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5369 - mae: 1.0169 - mse: 1.5369 - val_loss: 1.6138 - val_mae: 1.0306 - val_mse: 1.6138\n",
      "Epoch 111/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5297 - mae: 1.0142 - mse: 1.5297 - val_loss: 1.6387 - val_mae: 1.0607 - val_mse: 1.6387\n",
      "Epoch 112/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5326 - mae: 1.0200 - mse: 1.5326 - val_loss: 1.6141 - val_mae: 1.0226 - val_mse: 1.6141\n",
      "Epoch 113/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5291 - mae: 1.0142 - mse: 1.5291 - val_loss: 1.6139 - val_mae: 1.0314 - val_mse: 1.6139\n",
      "Epoch 114/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5250 - mae: 1.0121 - mse: 1.5250 - val_loss: 1.6651 - val_mae: 1.0751 - val_mse: 1.6651\n",
      "Epoch 115/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5262 - mae: 1.0167 - mse: 1.5262 - val_loss: 1.6137 - val_mae: 1.0298 - val_mse: 1.6137\n",
      "Epoch 116/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5291 - mae: 1.0151 - mse: 1.5291 - val_loss: 1.6438 - val_mae: 1.0639 - val_mse: 1.6438\n",
      "Epoch 117/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5261 - mae: 1.0191 - mse: 1.5261 - val_loss: 1.6213 - val_mae: 1.0090 - val_mse: 1.6213\n",
      "Epoch 118/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5360 - mae: 1.0183 - mse: 1.5360 - val_loss: 1.6153 - val_mae: 1.0362 - val_mse: 1.6153\n",
      "Epoch 119/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5264 - mae: 1.0154 - mse: 1.5264 - val_loss: 1.6140 - val_mae: 1.0228 - val_mse: 1.6140\n",
      "Epoch 120/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5295 - mae: 1.0141 - mse: 1.5295 - val_loss: 1.6195 - val_mae: 1.0436 - val_mse: 1.6195\n",
      "Epoch 121/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5282 - mae: 1.0165 - mse: 1.5282 - val_loss: 1.6136 - val_mae: 1.0290 - val_mse: 1.6136\n",
      "Epoch 122/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5306 - mae: 1.0164 - mse: 1.5306 - val_loss: 1.6180 - val_mae: 1.0415 - val_mse: 1.6180\n",
      "Epoch 123/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5318 - mae: 1.0192 - mse: 1.5318 - val_loss: 1.6136 - val_mae: 1.0282 - val_mse: 1.6136\n",
      "Epoch 124/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5300 - mae: 1.0157 - mse: 1.5300 - val_loss: 1.6184 - val_mae: 1.0421 - val_mse: 1.6184\n",
      "Epoch 125/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5293 - mae: 1.0166 - mse: 1.5293 - val_loss: 1.6144 - val_mae: 1.0335 - val_mse: 1.6144\n",
      "Epoch 126/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5310 - mae: 1.0157 - mse: 1.5310 - val_loss: 1.6138 - val_mae: 1.0311 - val_mse: 1.6138\n",
      "Epoch 127/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5332 - mae: 1.0160 - mse: 1.5332 - val_loss: 1.6189 - val_mae: 1.0428 - val_mse: 1.6189\n",
      "Epoch 128/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5281 - mae: 1.0162 - mse: 1.5281 - val_loss: 1.6326 - val_mae: 1.0564 - val_mse: 1.6326\n",
      "Epoch 129/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5270 - mae: 1.0154 - mse: 1.5270 - val_loss: 1.6251 - val_mae: 1.0500 - val_mse: 1.6251\n",
      "Epoch 130/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5295 - mae: 1.0173 - mse: 1.5295 - val_loss: 1.6136 - val_mae: 1.0265 - val_mse: 1.6136\n",
      "Epoch 131/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5281 - mae: 1.0149 - mse: 1.5281 - val_loss: 1.6136 - val_mae: 1.0283 - val_mse: 1.6136\n",
      "Epoch 132/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5308 - mae: 1.0166 - mse: 1.5308 - val_loss: 1.6321 - val_mae: 1.0560 - val_mse: 1.6321\n",
      "Epoch 133/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5296 - mae: 1.0146 - mse: 1.5296 - val_loss: 1.6396 - val_mae: 1.0613 - val_mse: 1.6396\n",
      "Epoch 134/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5258 - mae: 1.0142 - mse: 1.5258 - val_loss: 1.6160 - val_mae: 1.0379 - val_mse: 1.6160\n",
      "Epoch 135/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5231 - mae: 1.0159 - mse: 1.5231 - val_loss: 1.6427 - val_mae: 0.9917 - val_mse: 1.6427\n",
      "Epoch 136/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5293 - mae: 1.0125 - mse: 1.5293 - val_loss: 1.6260 - val_mae: 1.0508 - val_mse: 1.6260\n",
      "Epoch 137/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5288 - mae: 1.0152 - mse: 1.5288 - val_loss: 1.6142 - val_mae: 1.0330 - val_mse: 1.6142\n",
      "Epoch 138/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5325 - mae: 1.0172 - mse: 1.5325 - val_loss: 1.6150 - val_mae: 1.0353 - val_mse: 1.6150\n",
      "Epoch 139/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5312 - mae: 1.0171 - mse: 1.5312 - val_loss: 1.6138 - val_mae: 1.0242 - val_mse: 1.6138\n",
      "Epoch 140/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5341 - mae: 1.0176 - mse: 1.5341 - val_loss: 1.6150 - val_mae: 1.0354 - val_mse: 1.6150\n",
      "Epoch 141/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5280 - mae: 1.0153 - mse: 1.5280 - val_loss: 1.6199 - val_mae: 1.0442 - val_mse: 1.6199\n",
      "Epoch 142/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5305 - mae: 1.0182 - mse: 1.5305 - val_loss: 1.6135 - val_mae: 1.0276 - val_mse: 1.6135\n",
      "Epoch 143/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5281 - mae: 1.0144 - mse: 1.5281 - val_loss: 1.6290 - val_mae: 1.0535 - val_mse: 1.6290\n",
      "Epoch 144/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5344 - mae: 1.0176 - mse: 1.5344 - val_loss: 1.6293 - val_mae: 1.0538 - val_mse: 1.6293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5284 - mae: 1.0159 - mse: 1.5284 - val_loss: 1.6595 - val_mae: 1.0724 - val_mse: 1.6595\n",
      "Epoch 146/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5288 - mae: 1.0190 - mse: 1.5288 - val_loss: 1.6208 - val_mae: 1.0454 - val_mse: 1.6208\n",
      "Epoch 147/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5308 - mae: 1.0163 - mse: 1.5308 - val_loss: 1.6436 - val_mae: 1.0638 - val_mse: 1.6436\n",
      "Epoch 148/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5286 - mae: 1.0191 - mse: 1.5286 - val_loss: 1.6157 - val_mae: 1.0176 - val_mse: 1.6157\n",
      "Epoch 149/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5335 - mae: 1.0184 - mse: 1.5335 - val_loss: 1.6168 - val_mae: 1.0394 - val_mse: 1.6168\n",
      "Epoch 150/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5314 - mae: 1.0183 - mse: 1.5314 - val_loss: 1.6195 - val_mae: 1.0113 - val_mse: 1.6195\n",
      "Epoch 151/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5268 - mae: 1.0135 - mse: 1.5268 - val_loss: 1.6231 - val_mae: 1.0480 - val_mse: 1.6231\n",
      "Epoch 152/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5339 - mae: 1.0179 - mse: 1.5339 - val_loss: 1.6469 - val_mae: 1.0658 - val_mse: 1.6469\n",
      "Epoch 153/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5328 - mae: 1.0185 - mse: 1.5328 - val_loss: 1.6135 - val_mae: 1.0270 - val_mse: 1.6135\n",
      "Epoch 154/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5265 - mae: 1.0174 - mse: 1.5265 - val_loss: 1.6240 - val_mae: 1.0061 - val_mse: 1.6240\n",
      "Epoch 155/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5332 - mae: 1.0159 - mse: 1.5332 - val_loss: 1.6169 - val_mae: 1.0396 - val_mse: 1.6169\n",
      "Epoch 156/450\n",
      "2174/2174 [==============================] - 0s 127us/sample - loss: 1.5270 - mae: 1.0172 - mse: 1.5270 - val_loss: 1.6136 - val_mae: 1.0254 - val_mse: 1.6136\n",
      "Epoch 157/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5316 - mae: 1.0172 - mse: 1.5316 - val_loss: 1.6135 - val_mae: 1.0281 - val_mse: 1.6135\n",
      "Epoch 158/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5279 - mae: 1.0160 - mse: 1.5279 - val_loss: 1.6377 - val_mae: 1.0600 - val_mse: 1.6377\n",
      "Epoch 159/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5299 - mae: 1.0176 - mse: 1.5299 - val_loss: 1.6467 - val_mae: 1.0656 - val_mse: 1.6467\n",
      "Epoch 160/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5315 - mae: 1.0181 - mse: 1.5315 - val_loss: 1.6136 - val_mae: 1.0291 - val_mse: 1.6136\n",
      "Epoch 161/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5323 - mae: 1.0173 - mse: 1.5323 - val_loss: 1.6311 - val_mae: 1.0553 - val_mse: 1.6311\n",
      "Epoch 162/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5288 - mae: 1.0169 - mse: 1.5288 - val_loss: 1.6221 - val_mae: 1.0468 - val_mse: 1.6221\n",
      "Epoch 163/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5244 - mae: 1.0169 - mse: 1.5244 - val_loss: 1.6136 - val_mae: 1.0293 - val_mse: 1.6136\n",
      "Epoch 164/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5338 - mae: 1.0177 - mse: 1.5338 - val_loss: 1.6157 - val_mae: 1.0373 - val_mse: 1.6157\n",
      "Epoch 165/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5304 - mae: 1.0173 - mse: 1.5304 - val_loss: 1.6191 - val_mae: 1.0119 - val_mse: 1.6191\n",
      "Epoch 166/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5307 - mae: 1.0179 - mse: 1.5307 - val_loss: 1.6188 - val_mae: 1.0123 - val_mse: 1.6188\n",
      "Epoch 167/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5324 - mae: 1.0159 - mse: 1.5324 - val_loss: 1.6156 - val_mae: 1.0369 - val_mse: 1.6156\n",
      "Epoch 168/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5258 - mae: 1.0154 - mse: 1.5258 - val_loss: 1.6159 - val_mae: 1.0377 - val_mse: 1.6159\n",
      "Epoch 169/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5329 - mae: 1.0145 - mse: 1.5329 - val_loss: 1.6375 - val_mae: 1.0599 - val_mse: 1.6375\n",
      "Epoch 170/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5293 - mae: 1.0178 - mse: 1.5293 - val_loss: 1.6152 - val_mae: 1.0361 - val_mse: 1.6152\n",
      "Epoch 171/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5319 - mae: 1.0160 - mse: 1.5319 - val_loss: 1.6249 - val_mae: 1.0498 - val_mse: 1.6249\n",
      "Epoch 172/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5303 - mae: 1.0175 - mse: 1.5303 - val_loss: 1.6174 - val_mae: 1.0405 - val_mse: 1.6174\n",
      "Epoch 173/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5246 - mae: 1.0157 - mse: 1.5246 - val_loss: 1.6150 - val_mae: 1.0195 - val_mse: 1.6150\n",
      "Epoch 174/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5286 - mae: 1.0153 - mse: 1.5286 - val_loss: 1.6362 - val_mae: 1.0590 - val_mse: 1.6362\n",
      "Epoch 175/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5305 - mae: 1.0161 - mse: 1.5305 - val_loss: 1.6145 - val_mae: 1.0340 - val_mse: 1.6145\n",
      "Epoch 176/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5329 - mae: 1.0179 - mse: 1.5329 - val_loss: 1.6135 - val_mae: 1.0281 - val_mse: 1.6135\n",
      "Epoch 177/450\n",
      "2174/2174 [==============================] - 0s 66us/sample - loss: 1.5285 - mae: 1.0164 - mse: 1.5285 - val_loss: 1.6257 - val_mae: 1.0506 - val_mse: 1.6257\n",
      "Epoch 178/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5315 - mae: 1.0160 - mse: 1.5315 - val_loss: 1.6205 - val_mae: 1.0449 - val_mse: 1.6205\n",
      "Epoch 179/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5306 - mae: 1.0181 - mse: 1.5306 - val_loss: 1.6135 - val_mae: 1.0270 - val_mse: 1.6135\n",
      "Epoch 180/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5310 - mae: 1.0174 - mse: 1.5310 - val_loss: 1.6167 - val_mae: 1.0392 - val_mse: 1.6167\n",
      "Epoch 181/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5265 - mae: 1.0171 - mse: 1.5265 - val_loss: 1.6230 - val_mae: 1.0071 - val_mse: 1.6230\n",
      "Epoch 182/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5321 - mae: 1.0162 - mse: 1.5321 - val_loss: 1.6136 - val_mae: 1.0287 - val_mse: 1.6136\n",
      "Epoch 183/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5313 - mae: 1.0123 - mse: 1.5313 - val_loss: 1.6475 - val_mae: 1.0661 - val_mse: 1.6475\n",
      "Epoch 184/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5296 - mae: 1.0179 - mse: 1.5296 - val_loss: 1.6151 - val_mae: 1.0357 - val_mse: 1.6151\n",
      "Epoch 185/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5313 - mae: 1.0147 - mse: 1.5313 - val_loss: 1.6212 - val_mae: 1.0458 - val_mse: 1.6212\n",
      "Epoch 186/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5325 - mae: 1.0180 - mse: 1.5325 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 187/450\n",
      "2174/2174 [==============================] - 0s 74us/sample - loss: 1.5357 - mae: 1.0197 - mse: 1.5357 - val_loss: 1.6141 - val_mae: 1.0325 - val_mse: 1.6141\n",
      "Epoch 188/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5275 - mae: 1.0161 - mse: 1.5275 - val_loss: 1.6137 - val_mae: 1.0304 - val_mse: 1.6137\n",
      "Epoch 189/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5330 - mae: 1.0170 - mse: 1.5330 - val_loss: 1.6136 - val_mae: 1.0292 - val_mse: 1.6136\n",
      "Epoch 190/450\n",
      "2174/2174 [==============================] - 0s 65us/sample - loss: 1.5254 - mae: 1.0145 - mse: 1.5254 - val_loss: 1.6175 - val_mae: 1.0407 - val_mse: 1.6175\n",
      "Epoch 191/450\n",
      "2174/2174 [==============================] - 0s 81us/sample - loss: 1.5283 - mae: 1.0178 - mse: 1.5283 - val_loss: 1.6162 - val_mae: 1.0167 - val_mse: 1.6162\n",
      "Epoch 192/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5276 - mae: 1.0139 - mse: 1.5276 - val_loss: 1.6312 - val_mae: 1.0553 - val_mse: 1.6312\n",
      "Epoch 193/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5285 - mae: 1.0176 - mse: 1.5285 - val_loss: 1.6137 - val_mae: 1.0245 - val_mse: 1.6137\n",
      "Epoch 194/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5276 - mae: 1.0157 - mse: 1.5276 - val_loss: 1.6142 - val_mae: 1.0222 - val_mse: 1.6142\n",
      "Epoch 195/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5326 - mae: 1.0157 - mse: 1.5326 - val_loss: 1.6181 - val_mae: 1.0416 - val_mse: 1.6181\n",
      "Epoch 196/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5323 - mae: 1.0175 - mse: 1.5323 - val_loss: 1.6314 - val_mae: 1.0555 - val_mse: 1.6314\n",
      "Epoch 197/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5308 - mae: 1.0196 - mse: 1.5308 - val_loss: 1.6137 - val_mae: 1.0300 - val_mse: 1.6137\n",
      "Epoch 198/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5404 - mae: 1.0210 - mse: 1.5404 - val_loss: 1.6187 - val_mae: 1.0124 - val_mse: 1.6187\n",
      "Epoch 199/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5267 - mae: 1.0142 - mse: 1.5267 - val_loss: 1.6163 - val_mae: 1.0384 - val_mse: 1.6163\n",
      "Epoch 200/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5299 - mae: 1.0190 - mse: 1.5299 - val_loss: 1.6136 - val_mae: 1.0282 - val_mse: 1.6136\n",
      "Epoch 201/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5354 - mae: 1.0190 - mse: 1.5354 - val_loss: 1.6138 - val_mae: 1.0311 - val_mse: 1.6138\n",
      "Epoch 202/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5300 - mae: 1.0179 - mse: 1.5300 - val_loss: 1.6135 - val_mae: 1.0277 - val_mse: 1.6135\n",
      "Epoch 203/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5305 - mae: 1.0181 - mse: 1.5305 - val_loss: 1.6146 - val_mae: 1.0206 - val_mse: 1.6146\n",
      "Epoch 204/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5273 - mae: 1.0153 - mse: 1.5273 - val_loss: 1.6138 - val_mae: 1.0305 - val_mse: 1.6138\n",
      "Epoch 205/450\n",
      "2174/2174 [==============================] - 0s 67us/sample - loss: 1.5279 - mae: 1.0133 - mse: 1.5279 - val_loss: 1.6190 - val_mae: 1.0120 - val_mse: 1.6190\n",
      "Epoch 206/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5356 - mae: 1.0161 - mse: 1.5356 - val_loss: 1.6361 - val_mae: 1.0590 - val_mse: 1.6361\n",
      "Epoch 207/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5333 - mae: 1.0181 - mse: 1.5333 - val_loss: 1.6396 - val_mae: 1.0613 - val_mse: 1.6396\n",
      "Epoch 208/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5267 - mae: 1.0166 - mse: 1.5267 - val_loss: 1.6226 - val_mae: 1.0474 - val_mse: 1.6226\n",
      "Epoch 209/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5360 - mae: 1.0214 - mse: 1.5360 - val_loss: 1.6140 - val_mae: 1.0229 - val_mse: 1.6140\n",
      "Epoch 210/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5270 - mae: 1.0144 - mse: 1.5270 - val_loss: 1.6151 - val_mae: 1.0191 - val_mse: 1.6151\n",
      "Epoch 211/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5265 - mae: 1.0144 - mse: 1.5265 - val_loss: 1.6343 - val_mae: 1.0577 - val_mse: 1.6343\n",
      "Epoch 212/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5364 - mae: 1.0191 - mse: 1.5364 - val_loss: 1.6151 - val_mae: 1.0192 - val_mse: 1.6151\n",
      "Epoch 213/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5309 - mae: 1.0154 - mse: 1.5309 - val_loss: 1.6332 - val_mae: 1.0568 - val_mse: 1.6332\n",
      "Epoch 214/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5280 - mae: 1.0143 - mse: 1.5280 - val_loss: 1.6227 - val_mae: 1.0475 - val_mse: 1.6227\n",
      "Epoch 215/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5293 - mae: 1.0162 - mse: 1.5293 - val_loss: 1.6533 - val_mae: 1.0692 - val_mse: 1.6533\n",
      "Epoch 216/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5317 - mae: 1.0198 - mse: 1.5317 - val_loss: 1.6137 - val_mae: 1.0245 - val_mse: 1.6137\n",
      "Epoch 217/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5275 - mae: 1.0157 - mse: 1.5275 - val_loss: 1.6208 - val_mae: 1.0453 - val_mse: 1.6208\n",
      "Epoch 218/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5332 - mae: 1.0180 - mse: 1.5332 - val_loss: 1.6136 - val_mae: 1.0259 - val_mse: 1.6136\n",
      "Epoch 219/450\n",
      "2174/2174 [==============================] - 0s 78us/sample - loss: 1.5257 - mae: 1.0143 - mse: 1.5257 - val_loss: 1.6287 - val_mae: 1.0533 - val_mse: 1.6287\n",
      "Epoch 220/450\n",
      "2174/2174 [==============================] - 0s 68us/sample - loss: 1.5287 - mae: 1.0158 - mse: 1.5287 - val_loss: 1.6421 - val_mae: 1.0629 - val_mse: 1.6421\n",
      "Epoch 221/450\n",
      "2174/2174 [==============================] - 0s 65us/sample - loss: 1.5226 - mae: 1.0155 - mse: 1.5226 - val_loss: 1.6162 - val_mae: 1.0166 - val_mse: 1.6162\n",
      "Epoch 222/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5346 - mae: 1.0173 - mse: 1.5346 - val_loss: 1.6206 - val_mae: 1.0451 - val_mse: 1.6206\n",
      "Epoch 223/450\n",
      "2174/2174 [==============================] - 0s 70us/sample - loss: 1.5295 - mae: 1.0151 - mse: 1.5295 - val_loss: 1.6594 - val_mae: 1.0723 - val_mse: 1.6594\n",
      "Epoch 224/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5391 - mae: 1.0209 - mse: 1.5391 - val_loss: 1.6152 - val_mae: 1.0361 - val_mse: 1.6152\n",
      "Epoch 225/450\n",
      "2174/2174 [==============================] - 0s 83us/sample - loss: 1.5235 - mae: 1.0127 - mse: 1.5235 - val_loss: 1.6333 - val_mae: 1.0569 - val_mse: 1.6333\n",
      "Epoch 226/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5388 - mae: 1.0204 - mse: 1.5388 - val_loss: 1.6207 - val_mae: 1.0451 - val_mse: 1.6207\n",
      "Epoch 227/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5283 - mae: 1.0180 - mse: 1.5283 - val_loss: 1.6136 - val_mae: 1.0254 - val_mse: 1.6136\n",
      "Epoch 228/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5334 - mae: 1.0181 - mse: 1.5334 - val_loss: 1.6180 - val_mae: 1.0415 - val_mse: 1.6180\n",
      "Epoch 229/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5278 - mae: 1.0172 - mse: 1.5278 - val_loss: 1.6138 - val_mae: 1.0307 - val_mse: 1.6138\n",
      "Epoch 230/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5265 - mae: 1.0122 - mse: 1.5265 - val_loss: 1.6494 - val_mae: 1.0672 - val_mse: 1.6494\n",
      "Epoch 231/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5296 - mae: 1.0208 - mse: 1.5296 - val_loss: 1.6256 - val_mae: 1.0045 - val_mse: 1.6256\n",
      "Epoch 232/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5322 - mae: 1.0124 - mse: 1.5322 - val_loss: 1.6273 - val_mae: 1.0520 - val_mse: 1.6273\n",
      "Epoch 233/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5340 - mae: 1.0171 - mse: 1.5340 - val_loss: 1.6168 - val_mae: 1.0395 - val_mse: 1.6168\n",
      "Epoch 234/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5276 - mae: 1.0179 - mse: 1.5276 - val_loss: 1.6146 - val_mae: 1.0342 - val_mse: 1.6146\n",
      "Epoch 235/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5285 - mae: 1.0161 - mse: 1.5285 - val_loss: 1.6148 - val_mae: 1.0199 - val_mse: 1.6148\n",
      "Epoch 236/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5243 - mae: 1.0113 - mse: 1.5243 - val_loss: 1.6354 - val_mae: 1.0584 - val_mse: 1.6354\n",
      "Epoch 237/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5272 - mae: 1.0182 - mse: 1.5272 - val_loss: 1.6135 - val_mae: 1.0268 - val_mse: 1.6135\n",
      "Epoch 238/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5329 - mae: 1.0176 - mse: 1.5329 - val_loss: 1.6188 - val_mae: 1.0122 - val_mse: 1.6188\n",
      "Epoch 239/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5311 - mae: 1.0131 - mse: 1.5311 - val_loss: 1.6267 - val_mae: 1.0515 - val_mse: 1.6267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5274 - mae: 1.0145 - mse: 1.5274 - val_loss: 1.6151 - val_mae: 1.0358 - val_mse: 1.6151\n",
      "Epoch 241/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5283 - mae: 1.0160 - mse: 1.5283 - val_loss: 1.6207 - val_mae: 1.0452 - val_mse: 1.6207\n",
      "Epoch 242/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5307 - mae: 1.0190 - mse: 1.5307 - val_loss: 1.6174 - val_mae: 1.0144 - val_mse: 1.6174\n",
      "Epoch 243/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5316 - mae: 1.0117 - mse: 1.5316 - val_loss: 1.6438 - val_mae: 1.0639 - val_mse: 1.6438\n",
      "Epoch 244/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5340 - mae: 1.0186 - mse: 1.5340 - val_loss: 1.6374 - val_mae: 1.0599 - val_mse: 1.6374\n",
      "Epoch 245/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5261 - mae: 1.0159 - mse: 1.5261 - val_loss: 1.6202 - val_mae: 1.0446 - val_mse: 1.6202\n",
      "Epoch 246/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5305 - mae: 1.0197 - mse: 1.5305 - val_loss: 1.6139 - val_mae: 1.0235 - val_mse: 1.6139\n",
      "Epoch 247/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5323 - mae: 1.0172 - mse: 1.5323 - val_loss: 1.6139 - val_mae: 1.0316 - val_mse: 1.6139\n",
      "Epoch 248/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5320 - mae: 1.0176 - mse: 1.5320 - val_loss: 1.6137 - val_mae: 1.0245 - val_mse: 1.6137\n",
      "Epoch 249/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5329 - mae: 1.0152 - mse: 1.5329 - val_loss: 1.6199 - val_mae: 1.0442 - val_mse: 1.6199\n",
      "Epoch 250/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5223 - mae: 1.0148 - mse: 1.5223 - val_loss: 1.6516 - val_mae: 1.0684 - val_mse: 1.6516\n",
      "Epoch 251/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5357 - mae: 1.0220 - mse: 1.5357 - val_loss: 1.6244 - val_mae: 1.0057 - val_mse: 1.6244\n",
      "Epoch 252/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5297 - mae: 1.0129 - mse: 1.5297 - val_loss: 1.6137 - val_mae: 1.0302 - val_mse: 1.6137\n",
      "Epoch 253/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5285 - mae: 1.0143 - mse: 1.5285 - val_loss: 1.6223 - val_mae: 1.0471 - val_mse: 1.6223\n",
      "Epoch 254/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5356 - mae: 1.0182 - mse: 1.5356 - val_loss: 1.6242 - val_mae: 1.0491 - val_mse: 1.6242\n",
      "Epoch 255/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5258 - mae: 1.0145 - mse: 1.5258 - val_loss: 1.6148 - val_mae: 1.0199 - val_mse: 1.6148\n",
      "Epoch 256/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5292 - mae: 1.0159 - mse: 1.5292 - val_loss: 1.6143 - val_mae: 1.0334 - val_mse: 1.6143\n",
      "Epoch 257/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5301 - mae: 1.0185 - mse: 1.5301 - val_loss: 1.6140 - val_mae: 1.0229 - val_mse: 1.6140\n",
      "Epoch 258/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5285 - mae: 1.0108 - mse: 1.5285 - val_loss: 1.6359 - val_mae: 1.0588 - val_mse: 1.6359\n",
      "Epoch 259/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5302 - mae: 1.0186 - mse: 1.5302 - val_loss: 1.6139 - val_mae: 1.0235 - val_mse: 1.6139\n",
      "Epoch 260/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5315 - mae: 1.0180 - mse: 1.5315 - val_loss: 1.6179 - val_mae: 1.0413 - val_mse: 1.6179\n",
      "Epoch 261/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5290 - mae: 1.0151 - mse: 1.5290 - val_loss: 1.6151 - val_mae: 1.0358 - val_mse: 1.6151\n",
      "Epoch 262/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5366 - mae: 1.0185 - mse: 1.5366 - val_loss: 1.6136 - val_mae: 1.0264 - val_mse: 1.6136\n",
      "Epoch 263/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5255 - mae: 1.0128 - mse: 1.5255 - val_loss: 1.6984 - val_mae: 1.0885 - val_mse: 1.6984\n",
      "Epoch 264/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5375 - mae: 1.0232 - mse: 1.5375 - val_loss: 1.6145 - val_mae: 1.0209 - val_mse: 1.6145\n",
      "Epoch 265/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5317 - mae: 1.0164 - mse: 1.5317 - val_loss: 1.6326 - val_mae: 1.0564 - val_mse: 1.6326\n",
      "Epoch 266/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5288 - mae: 1.0154 - mse: 1.5288 - val_loss: 1.6198 - val_mae: 1.0441 - val_mse: 1.6198\n",
      "Epoch 267/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5292 - mae: 1.0184 - mse: 1.5292 - val_loss: 1.6227 - val_mae: 1.0475 - val_mse: 1.6227\n",
      "Epoch 268/450\n",
      "2174/2174 [==============================] - ETA: 0s - loss: 1.4821 - mae: 1.0039 - mse: 1.482 - 0s 45us/sample - loss: 1.5276 - mae: 1.0173 - mse: 1.5276 - val_loss: 1.6136 - val_mae: 1.0297 - val_mse: 1.6136\n",
      "Epoch 269/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5273 - mae: 1.0153 - mse: 1.5273 - val_loss: 1.6228 - val_mae: 1.0477 - val_mse: 1.6228\n",
      "Epoch 270/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5268 - mae: 1.0163 - mse: 1.5268 - val_loss: 1.6139 - val_mae: 1.0317 - val_mse: 1.6139\n",
      "Epoch 271/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5318 - mae: 1.0159 - mse: 1.5318 - val_loss: 1.6365 - val_mae: 1.0592 - val_mse: 1.6365\n",
      "Epoch 272/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5288 - mae: 1.0153 - mse: 1.5288 - val_loss: 1.6207 - val_mae: 1.0452 - val_mse: 1.6207\n",
      "Epoch 273/450\n",
      "2174/2174 [==============================] - 0s 66us/sample - loss: 1.5300 - mae: 1.0157 - mse: 1.5300 - val_loss: 1.6325 - val_mae: 1.0563 - val_mse: 1.6325\n",
      "Epoch 274/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5318 - mae: 1.0181 - mse: 1.5318 - val_loss: 1.6203 - val_mae: 1.0447 - val_mse: 1.6203\n",
      "Epoch 275/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5255 - mae: 1.0144 - mse: 1.5255 - val_loss: 1.6136 - val_mae: 1.0292 - val_mse: 1.6136\n",
      "Epoch 276/450\n",
      "2174/2174 [==============================] - 0s 71us/sample - loss: 1.5311 - mae: 1.0168 - mse: 1.5311 - val_loss: 1.6332 - val_mae: 1.0568 - val_mse: 1.6332\n",
      "Epoch 277/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5332 - mae: 1.0170 - mse: 1.5332 - val_loss: 1.6147 - val_mae: 1.0346 - val_mse: 1.6147\n",
      "Epoch 278/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5254 - mae: 1.0159 - mse: 1.5254 - val_loss: 1.6136 - val_mae: 1.0295 - val_mse: 1.6136\n",
      "Epoch 279/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5287 - mae: 1.0160 - mse: 1.5287 - val_loss: 1.6174 - val_mae: 1.0405 - val_mse: 1.6174\n",
      "Epoch 280/450\n",
      "2174/2174 [==============================] - 0s 94us/sample - loss: 1.5297 - mae: 1.0166 - mse: 1.5297 - val_loss: 1.6262 - val_mae: 1.0511 - val_mse: 1.6262\n",
      "Epoch 281/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5317 - mae: 1.0173 - mse: 1.5317 - val_loss: 1.6186 - val_mae: 1.0423 - val_mse: 1.6186\n",
      "Epoch 282/450\n",
      "2174/2174 [==============================] - 0s 92us/sample - loss: 1.5323 - mae: 1.0191 - mse: 1.5323 - val_loss: 1.6141 - val_mae: 1.0227 - val_mse: 1.6141\n",
      "Epoch 283/450\n",
      "2174/2174 [==============================] - 0s 106us/sample - loss: 1.5259 - mae: 1.0152 - mse: 1.5259 - val_loss: 1.6136 - val_mae: 1.0291 - val_mse: 1.6136\n",
      "Epoch 284/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5249 - mae: 1.0105 - mse: 1.5249 - val_loss: 1.6165 - val_mae: 1.0388 - val_mse: 1.6165\n",
      "Epoch 285/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5319 - mae: 1.0167 - mse: 1.5319 - val_loss: 1.6140 - val_mae: 1.0318 - val_mse: 1.6140\n",
      "Epoch 286/450\n",
      "2174/2174 [==============================] - 0s 70us/sample - loss: 1.5285 - mae: 1.0138 - mse: 1.5285 - val_loss: 1.6372 - val_mae: 1.0597 - val_mse: 1.6372\n",
      "Epoch 287/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5320 - mae: 1.0173 - mse: 1.5320 - val_loss: 1.6161 - val_mae: 1.0380 - val_mse: 1.6161\n",
      "Epoch 288/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5264 - mae: 1.0163 - mse: 1.5264 - val_loss: 1.6137 - val_mae: 1.0298 - val_mse: 1.6137\n",
      "Epoch 289/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5305 - mae: 1.0148 - mse: 1.5305 - val_loss: 1.6136 - val_mae: 1.0262 - val_mse: 1.6136\n",
      "Epoch 290/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5220 - mae: 1.0144 - mse: 1.5220 - val_loss: 1.6340 - val_mae: 0.9975 - val_mse: 1.6340\n",
      "Epoch 291/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5391 - mae: 1.0177 - mse: 1.5391 - val_loss: 1.6144 - val_mae: 1.0335 - val_mse: 1.6144\n",
      "Epoch 292/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5282 - mae: 1.0145 - mse: 1.5282 - val_loss: 1.6211 - val_mae: 1.0457 - val_mse: 1.6211\n",
      "Epoch 293/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5297 - mae: 1.0174 - mse: 1.5297 - val_loss: 1.6197 - val_mae: 1.0439 - val_mse: 1.6197\n",
      "Epoch 294/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5311 - mae: 1.0170 - mse: 1.5311 - val_loss: 1.6148 - val_mae: 1.0350 - val_mse: 1.6148\n",
      "Epoch 295/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5252 - mae: 1.0125 - mse: 1.5252 - val_loss: 1.6253 - val_mae: 1.0502 - val_mse: 1.6253\n",
      "Epoch 296/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5272 - mae: 1.0171 - mse: 1.5272 - val_loss: 1.6149 - val_mae: 1.0197 - val_mse: 1.6149\n",
      "Epoch 297/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5267 - mae: 1.0142 - mse: 1.5267 - val_loss: 1.6174 - val_mae: 1.0144 - val_mse: 1.6174\n",
      "Epoch 298/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5304 - mae: 1.0122 - mse: 1.5304 - val_loss: 1.6237 - val_mae: 1.0486 - val_mse: 1.6237\n",
      "Epoch 299/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5302 - mae: 1.0197 - mse: 1.5302 - val_loss: 1.6205 - val_mae: 1.0450 - val_mse: 1.6205\n",
      "Epoch 300/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5288 - mae: 1.0143 - mse: 1.5288 - val_loss: 1.6163 - val_mae: 1.0385 - val_mse: 1.6163\n",
      "Epoch 301/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5282 - mae: 1.0145 - mse: 1.5282 - val_loss: 1.6333 - val_mae: 1.0570 - val_mse: 1.6333\n",
      "Epoch 302/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5309 - mae: 1.0180 - mse: 1.5309 - val_loss: 1.6165 - val_mae: 1.0162 - val_mse: 1.6165\n",
      "Epoch 303/450\n",
      "2174/2174 [==============================] - 0s 65us/sample - loss: 1.5341 - mae: 1.0152 - mse: 1.5341 - val_loss: 1.6256 - val_mae: 1.0505 - val_mse: 1.6256\n",
      "Epoch 304/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5312 - mae: 1.0173 - mse: 1.5312 - val_loss: 1.6267 - val_mae: 1.0515 - val_mse: 1.6267\n",
      "Epoch 305/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5316 - mae: 1.0189 - mse: 1.5316 - val_loss: 1.6210 - val_mae: 1.0456 - val_mse: 1.6210\n",
      "Epoch 306/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5322 - mae: 1.0195 - mse: 1.5322 - val_loss: 1.6143 - val_mae: 1.0332 - val_mse: 1.6143\n",
      "Epoch 307/450\n",
      "2174/2174 [==============================] - 0s 66us/sample - loss: 1.5273 - mae: 1.0162 - mse: 1.5273 - val_loss: 1.6223 - val_mae: 1.0471 - val_mse: 1.6223\n",
      "Epoch 308/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5288 - mae: 1.0170 - mse: 1.5288 - val_loss: 1.6135 - val_mae: 1.0270 - val_mse: 1.6135\n",
      "Epoch 309/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5351 - mae: 1.0167 - mse: 1.5351 - val_loss: 1.6266 - val_mae: 1.0514 - val_mse: 1.6266\n",
      "Epoch 310/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5297 - mae: 1.0150 - mse: 1.5297 - val_loss: 1.6164 - val_mae: 1.0162 - val_mse: 1.6164\n",
      "Epoch 311/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5296 - mae: 1.0144 - mse: 1.5296 - val_loss: 1.6161 - val_mae: 1.0380 - val_mse: 1.6161\n",
      "Epoch 312/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5335 - mae: 1.0174 - mse: 1.5335 - val_loss: 1.6295 - val_mae: 1.0540 - val_mse: 1.6295\n",
      "Epoch 313/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5304 - mae: 1.0181 - mse: 1.5304 - val_loss: 1.6136 - val_mae: 1.0256 - val_mse: 1.6136\n",
      "Epoch 314/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5317 - mae: 1.0157 - mse: 1.5317 - val_loss: 1.6284 - val_mae: 1.0530 - val_mse: 1.6284\n",
      "Epoch 315/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5282 - mae: 1.0156 - mse: 1.5282 - val_loss: 1.6196 - val_mae: 1.0111 - val_mse: 1.6196\n",
      "Epoch 316/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5287 - mae: 1.0126 - mse: 1.5287 - val_loss: 1.6499 - val_mae: 1.0674 - val_mse: 1.6499\n",
      "Epoch 317/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5292 - mae: 1.0171 - mse: 1.5292 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 318/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5270 - mae: 1.0149 - mse: 1.5270 - val_loss: 1.6139 - val_mae: 1.0314 - val_mse: 1.6139\n",
      "Epoch 319/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5304 - mae: 1.0163 - mse: 1.5304 - val_loss: 1.6137 - val_mae: 1.0249 - val_mse: 1.6137\n",
      "Epoch 320/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5303 - mae: 1.0148 - mse: 1.5303 - val_loss: 1.6412 - val_mae: 1.0623 - val_mse: 1.6412\n",
      "Epoch 321/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5311 - mae: 1.0181 - mse: 1.5311 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 322/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5299 - mae: 1.0168 - mse: 1.5299 - val_loss: 1.6288 - val_mae: 1.0534 - val_mse: 1.6288\n",
      "Epoch 323/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5276 - mae: 1.0173 - mse: 1.5276 - val_loss: 1.6165 - val_mae: 1.0388 - val_mse: 1.6165\n",
      "Epoch 324/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5344 - mae: 1.0193 - mse: 1.5344 - val_loss: 1.6136 - val_mae: 1.0289 - val_mse: 1.6136\n",
      "Epoch 325/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5317 - mae: 1.0190 - mse: 1.5317 - val_loss: 1.6200 - val_mae: 1.0106 - val_mse: 1.6200\n",
      "Epoch 326/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5299 - mae: 1.0171 - mse: 1.5299 - val_loss: 1.6143 - val_mae: 1.0218 - val_mse: 1.6143\n",
      "Epoch 327/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5302 - mae: 1.0172 - mse: 1.5302 - val_loss: 1.6179 - val_mae: 1.0414 - val_mse: 1.6179\n",
      "Epoch 328/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5295 - mae: 1.0160 - mse: 1.5295 - val_loss: 1.6149 - val_mae: 1.0352 - val_mse: 1.6149\n",
      "Epoch 329/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5250 - mae: 1.0145 - mse: 1.5250 - val_loss: 1.6333 - val_mae: 1.0569 - val_mse: 1.6333\n",
      "Epoch 330/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5282 - mae: 1.0146 - mse: 1.5282 - val_loss: 1.6658 - val_mae: 1.0754 - val_mse: 1.6658\n",
      "Epoch 331/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5286 - mae: 1.0173 - mse: 1.5286 - val_loss: 1.6165 - val_mae: 1.0160 - val_mse: 1.6165\n",
      "Epoch 332/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5295 - mae: 1.0151 - mse: 1.5295 - val_loss: 1.6297 - val_mae: 1.0541 - val_mse: 1.6297\n",
      "Epoch 333/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5309 - mae: 1.0149 - mse: 1.5309 - val_loss: 1.6715 - val_mae: 1.0779 - val_mse: 1.6715\n",
      "Epoch 334/450\n",
      "2174/2174 [==============================] - 0s 71us/sample - loss: 1.5286 - mae: 1.0194 - mse: 1.5286 - val_loss: 1.6157 - val_mae: 1.0178 - val_mse: 1.6157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5261 - mae: 1.0143 - mse: 1.5261 - val_loss: 1.6140 - val_mae: 1.0320 - val_mse: 1.6140\n",
      "Epoch 336/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5325 - mae: 1.0172 - mse: 1.5325 - val_loss: 1.6169 - val_mae: 1.0396 - val_mse: 1.6169\n",
      "Epoch 337/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5330 - mae: 1.0183 - mse: 1.5330 - val_loss: 1.6164 - val_mae: 1.0162 - val_mse: 1.6164\n",
      "Epoch 338/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5290 - mae: 1.0153 - mse: 1.5290 - val_loss: 1.6175 - val_mae: 1.0407 - val_mse: 1.6175\n",
      "Epoch 339/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5343 - mae: 1.0206 - mse: 1.5343 - val_loss: 1.6170 - val_mae: 1.0151 - val_mse: 1.6170\n",
      "Epoch 340/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5305 - mae: 1.0143 - mse: 1.5305 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 341/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5259 - mae: 1.0147 - mse: 1.5259 - val_loss: 1.6291 - val_mae: 1.0536 - val_mse: 1.6291\n",
      "Epoch 342/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5268 - mae: 1.0165 - mse: 1.5268 - val_loss: 1.6206 - val_mae: 1.0451 - val_mse: 1.6206\n",
      "Epoch 343/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5268 - mae: 1.0147 - mse: 1.5268 - val_loss: 1.6352 - val_mae: 1.0583 - val_mse: 1.6352\n",
      "Epoch 344/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5329 - mae: 1.0202 - mse: 1.5329 - val_loss: 1.6139 - val_mae: 1.0317 - val_mse: 1.6139\n",
      "Epoch 345/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5288 - mae: 1.0160 - mse: 1.5288 - val_loss: 1.6162 - val_mae: 1.0382 - val_mse: 1.6162\n",
      "Epoch 346/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5236 - mae: 1.0119 - mse: 1.5236 - val_loss: 1.6283 - val_mae: 1.0529 - val_mse: 1.6283\n",
      "Epoch 347/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5285 - mae: 1.0154 - mse: 1.5285 - val_loss: 1.6342 - val_mae: 1.0576 - val_mse: 1.6342\n",
      "Epoch 348/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5342 - mae: 1.0211 - mse: 1.5342 - val_loss: 1.6138 - val_mae: 1.0311 - val_mse: 1.6138\n",
      "Epoch 349/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5297 - mae: 1.0153 - mse: 1.5297 - val_loss: 1.6283 - val_mae: 1.0529 - val_mse: 1.6283\n",
      "Epoch 350/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5285 - mae: 1.0175 - mse: 1.5285 - val_loss: 1.6137 - val_mae: 1.0299 - val_mse: 1.6137\n",
      "Epoch 351/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5285 - mae: 1.0162 - mse: 1.5285 - val_loss: 1.6139 - val_mae: 1.0314 - val_mse: 1.6139\n",
      "Epoch 352/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5297 - mae: 1.0173 - mse: 1.5297 - val_loss: 1.6141 - val_mae: 1.0225 - val_mse: 1.6141\n",
      "Epoch 353/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5307 - mae: 1.0140 - mse: 1.5307 - val_loss: 1.6273 - val_mae: 1.0520 - val_mse: 1.6273\n",
      "Epoch 354/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5330 - mae: 1.0203 - mse: 1.5330 - val_loss: 1.6154 - val_mae: 1.0185 - val_mse: 1.6154\n",
      "Epoch 355/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5347 - mae: 1.0164 - mse: 1.5347 - val_loss: 1.6307 - val_mae: 1.0549 - val_mse: 1.6307\n",
      "Epoch 356/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5366 - mae: 1.0218 - mse: 1.5366 - val_loss: 1.6157 - val_mae: 1.0178 - val_mse: 1.6157\n",
      "Epoch 357/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5269 - mae: 1.0150 - mse: 1.5269 - val_loss: 1.6242 - val_mae: 1.0491 - val_mse: 1.6242\n",
      "Epoch 358/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5294 - mae: 1.0129 - mse: 1.5294 - val_loss: 1.6283 - val_mae: 1.0530 - val_mse: 1.6283\n",
      "Epoch 359/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5292 - mae: 1.0181 - mse: 1.5292 - val_loss: 1.6166 - val_mae: 1.0390 - val_mse: 1.6166\n",
      "Epoch 360/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5293 - mae: 1.0151 - mse: 1.5293 - val_loss: 1.6265 - val_mae: 1.0513 - val_mse: 1.6265\n",
      "Epoch 361/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5335 - mae: 1.0201 - mse: 1.5335 - val_loss: 1.6144 - val_mae: 1.0214 - val_mse: 1.6144\n",
      "Epoch 362/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5331 - mae: 1.0170 - mse: 1.5331 - val_loss: 1.6255 - val_mae: 1.0504 - val_mse: 1.6255\n",
      "Epoch 363/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5309 - mae: 1.0188 - mse: 1.5309 - val_loss: 1.6136 - val_mae: 1.0284 - val_mse: 1.6136\n",
      "Epoch 364/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5306 - mae: 1.0154 - mse: 1.5306 - val_loss: 1.6491 - val_mae: 1.0670 - val_mse: 1.6491\n",
      "Epoch 365/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5300 - mae: 1.0165 - mse: 1.5300 - val_loss: 1.6149 - val_mae: 1.0351 - val_mse: 1.6149\n",
      "Epoch 366/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5324 - mae: 1.0146 - mse: 1.5324 - val_loss: 1.6321 - val_mae: 1.0560 - val_mse: 1.6321\n",
      "Epoch 367/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5284 - mae: 1.0197 - mse: 1.5284 - val_loss: 1.6163 - val_mae: 1.0165 - val_mse: 1.6163\n",
      "Epoch 368/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5361 - mae: 1.0174 - mse: 1.5361 - val_loss: 1.6136 - val_mae: 1.0295 - val_mse: 1.6136\n",
      "Epoch 369/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5281 - mae: 1.0150 - mse: 1.5281 - val_loss: 1.6135 - val_mae: 1.0269 - val_mse: 1.6135\n",
      "Epoch 370/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5328 - mae: 1.0165 - mse: 1.5328 - val_loss: 1.6147 - val_mae: 1.0202 - val_mse: 1.6147\n",
      "Epoch 371/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5293 - mae: 1.0185 - mse: 1.5293 - val_loss: 1.6205 - val_mae: 1.0100 - val_mse: 1.6205\n",
      "Epoch 372/450\n",
      "2174/2174 [==============================] - 0s 65us/sample - loss: 1.5309 - mae: 1.0164 - mse: 1.5309 - val_loss: 1.6138 - val_mae: 1.0242 - val_mse: 1.6138\n",
      "Epoch 373/450\n",
      "2174/2174 [==============================] - 0s 67us/sample - loss: 1.5271 - mae: 1.0140 - mse: 1.5271 - val_loss: 1.6330 - val_mae: 1.0567 - val_mse: 1.6330\n",
      "Epoch 374/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5354 - mae: 1.0194 - mse: 1.5354 - val_loss: 1.6320 - val_mae: 1.0559 - val_mse: 1.6320\n",
      "Epoch 375/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5273 - mae: 1.0169 - mse: 1.5273 - val_loss: 1.6146 - val_mae: 1.0343 - val_mse: 1.6146\n",
      "Epoch 376/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5327 - mae: 1.0190 - mse: 1.5327 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 377/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5242 - mae: 1.0145 - mse: 1.5242 - val_loss: 1.6164 - val_mae: 1.0387 - val_mse: 1.6164\n",
      "Epoch 378/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5210 - mae: 1.0134 - mse: 1.5210 - val_loss: 1.6210 - val_mae: 1.0094 - val_mse: 1.6210\n",
      "Epoch 379/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5324 - mae: 1.0176 - mse: 1.5324 - val_loss: 1.6176 - val_mae: 1.0408 - val_mse: 1.6176\n",
      "Epoch 380/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5355 - mae: 1.0178 - mse: 1.5355 - val_loss: 1.6263 - val_mae: 1.0511 - val_mse: 1.6263\n",
      "Epoch 381/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5270 - mae: 1.0166 - mse: 1.5270 - val_loss: 1.6139 - val_mae: 1.0317 - val_mse: 1.6139\n",
      "Epoch 382/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5291 - mae: 1.0157 - mse: 1.5291 - val_loss: 1.6156 - val_mae: 1.0180 - val_mse: 1.6156\n",
      "Epoch 383/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5268 - mae: 1.0137 - mse: 1.5268 - val_loss: 1.6136 - val_mae: 1.0283 - val_mse: 1.6136\n",
      "Epoch 384/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5293 - mae: 1.0160 - mse: 1.5293 - val_loss: 1.6172 - val_mae: 1.0147 - val_mse: 1.6172\n",
      "Epoch 385/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5341 - mae: 1.0162 - mse: 1.5341 - val_loss: 1.6143 - val_mae: 1.0333 - val_mse: 1.6143\n",
      "Epoch 386/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5292 - mae: 1.0183 - mse: 1.5292 - val_loss: 1.6164 - val_mae: 1.0162 - val_mse: 1.6164\n",
      "Epoch 387/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5295 - mae: 1.0152 - mse: 1.5295 - val_loss: 1.6136 - val_mae: 1.0284 - val_mse: 1.6136\n",
      "Epoch 388/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5303 - mae: 1.0181 - mse: 1.5303 - val_loss: 1.6143 - val_mae: 1.0331 - val_mse: 1.6143\n",
      "Epoch 389/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5290 - mae: 1.0159 - mse: 1.5290 - val_loss: 1.6160 - val_mae: 1.0379 - val_mse: 1.6160\n",
      "Epoch 390/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5300 - mae: 1.0155 - mse: 1.5300 - val_loss: 1.6171 - val_mae: 1.0400 - val_mse: 1.6171\n",
      "Epoch 391/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5294 - mae: 1.0167 - mse: 1.5294 - val_loss: 1.6136 - val_mae: 1.0285 - val_mse: 1.6136\n",
      "Epoch 392/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5305 - mae: 1.0179 - mse: 1.5305 - val_loss: 1.6189 - val_mae: 1.0428 - val_mse: 1.6189\n",
      "Epoch 393/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5324 - mae: 1.0163 - mse: 1.5324 - val_loss: 1.6138 - val_mae: 1.0311 - val_mse: 1.6138\n",
      "Epoch 394/450\n",
      "2174/2174 [==============================] - 0s 69us/sample - loss: 1.5254 - mae: 1.0147 - mse: 1.5254 - val_loss: 1.6220 - val_mae: 1.0468 - val_mse: 1.6220\n",
      "Epoch 395/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5345 - mae: 1.0178 - mse: 1.5345 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 396/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5286 - mae: 1.0151 - mse: 1.5286 - val_loss: 1.6503 - val_mae: 1.0676 - val_mse: 1.6503\n",
      "Epoch 397/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5300 - mae: 1.0175 - mse: 1.5300 - val_loss: 1.6136 - val_mae: 1.0292 - val_mse: 1.6136\n",
      "Epoch 398/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5328 - mae: 1.0177 - mse: 1.5328 - val_loss: 1.6197 - val_mae: 1.0440 - val_mse: 1.6197\n",
      "Epoch 399/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5268 - mae: 1.0166 - mse: 1.5268 - val_loss: 1.6139 - val_mae: 1.0313 - val_mse: 1.6139\n",
      "Epoch 400/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5351 - mae: 1.0204 - mse: 1.5351 - val_loss: 1.6144 - val_mae: 1.0215 - val_mse: 1.6144\n",
      "Epoch 401/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5254 - mae: 1.0147 - mse: 1.5254 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 402/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5279 - mae: 1.0152 - mse: 1.5279 - val_loss: 1.6162 - val_mae: 1.0167 - val_mse: 1.6162\n",
      "Epoch 403/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5271 - mae: 1.0130 - mse: 1.5271 - val_loss: 1.6199 - val_mae: 1.0441 - val_mse: 1.6199\n",
      "Epoch 404/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5319 - mae: 1.0162 - mse: 1.5319 - val_loss: 1.6399 - val_mae: 1.0615 - val_mse: 1.6399\n",
      "Epoch 405/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5302 - mae: 1.0181 - mse: 1.5302 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 406/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5348 - mae: 1.0186 - mse: 1.5348 - val_loss: 1.6339 - val_mae: 1.0574 - val_mse: 1.6339\n",
      "Epoch 407/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5338 - mae: 1.0167 - mse: 1.5338 - val_loss: 1.6198 - val_mae: 1.0440 - val_mse: 1.6198\n",
      "Epoch 408/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5291 - mae: 1.0162 - mse: 1.5291 - val_loss: 1.6592 - val_mae: 1.0722 - val_mse: 1.6592\n",
      "Epoch 409/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5313 - mae: 1.0190 - mse: 1.5313 - val_loss: 1.6148 - val_mae: 1.0350 - val_mse: 1.6148\n",
      "Epoch 410/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5267 - mae: 1.0135 - mse: 1.5267 - val_loss: 1.6281 - val_mae: 1.0527 - val_mse: 1.6281\n",
      "Epoch 411/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5240 - mae: 1.0128 - mse: 1.5240 - val_loss: 1.6201 - val_mae: 1.0444 - val_mse: 1.6201\n",
      "Epoch 412/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5252 - mae: 1.0169 - mse: 1.5252 - val_loss: 1.6223 - val_mae: 1.0078 - val_mse: 1.6223\n",
      "Epoch 413/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5365 - mae: 1.0173 - mse: 1.5365 - val_loss: 1.6188 - val_mae: 1.0427 - val_mse: 1.6188\n",
      "Epoch 414/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5328 - mae: 1.0167 - mse: 1.5328 - val_loss: 1.6187 - val_mae: 1.0425 - val_mse: 1.6187\n",
      "Epoch 415/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5303 - mae: 1.0186 - mse: 1.5303 - val_loss: 1.6136 - val_mae: 1.0262 - val_mse: 1.6136\n",
      "Epoch 416/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5305 - mae: 1.0165 - mse: 1.5305 - val_loss: 1.6141 - val_mae: 1.0227 - val_mse: 1.6141\n",
      "Epoch 417/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5272 - mae: 1.0156 - mse: 1.5272 - val_loss: 1.6140 - val_mae: 1.0321 - val_mse: 1.6140\n",
      "Epoch 418/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5333 - mae: 1.0174 - mse: 1.5333 - val_loss: 1.6146 - val_mae: 1.0344 - val_mse: 1.6146\n",
      "Epoch 419/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5268 - mae: 1.0146 - mse: 1.5268 - val_loss: 1.6149 - val_mae: 1.0352 - val_mse: 1.6149\n",
      "Epoch 420/450\n",
      "2174/2174 [==============================] - 0s 68us/sample - loss: 1.5233 - mae: 1.0155 - mse: 1.5233 - val_loss: 1.6135 - val_mae: 1.0271 - val_mse: 1.6135\n",
      "Epoch 421/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5250 - mae: 1.0131 - mse: 1.5250 - val_loss: 1.6135 - val_mae: 1.0272 - val_mse: 1.6135\n",
      "Epoch 422/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5278 - mae: 1.0157 - mse: 1.5278 - val_loss: 1.6160 - val_mae: 1.0171 - val_mse: 1.6160\n",
      "Epoch 423/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5285 - mae: 1.0135 - mse: 1.5285 - val_loss: 1.6146 - val_mae: 1.0343 - val_mse: 1.6146\n",
      "Epoch 424/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5284 - mae: 1.0148 - mse: 1.5284 - val_loss: 1.6298 - val_mae: 1.0542 - val_mse: 1.6298\n",
      "Epoch 425/450\n",
      "2174/2174 [==============================] - 0s 86us/sample - loss: 1.5314 - mae: 1.0170 - mse: 1.5314 - val_loss: 1.6137 - val_mae: 1.0298 - val_mse: 1.6137\n",
      "Epoch 426/450\n",
      "2174/2174 [==============================] - 0s 87us/sample - loss: 1.5367 - mae: 1.0170 - mse: 1.5367 - val_loss: 1.6140 - val_mae: 1.0318 - val_mse: 1.6140\n",
      "Epoch 427/450\n",
      "2174/2174 [==============================] - 0s 93us/sample - loss: 1.5298 - mae: 1.0173 - mse: 1.5298 - val_loss: 1.6136 - val_mae: 1.0254 - val_mse: 1.6136\n",
      "Epoch 428/450\n",
      "2174/2174 [==============================] - 0s 77us/sample - loss: 1.5326 - mae: 1.0182 - mse: 1.5326 - val_loss: 1.6137 - val_mae: 1.0248 - val_mse: 1.6137\n",
      "Epoch 429/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5292 - mae: 1.0145 - mse: 1.5292 - val_loss: 1.6299 - val_mae: 1.0543 - val_mse: 1.6299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5275 - mae: 1.0149 - mse: 1.5275 - val_loss: 1.6415 - val_mae: 1.0625 - val_mse: 1.6415\n",
      "Epoch 431/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5282 - mae: 1.0191 - mse: 1.5282 - val_loss: 1.6178 - val_mae: 1.0137 - val_mse: 1.6178\n",
      "Epoch 432/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5245 - mae: 1.0126 - mse: 1.5245 - val_loss: 1.6383 - val_mae: 1.0604 - val_mse: 1.6383\n",
      "Epoch 433/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5349 - mae: 1.0180 - mse: 1.5349 - val_loss: 1.6180 - val_mae: 1.0414 - val_mse: 1.6180\n",
      "Epoch 434/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5279 - mae: 1.0171 - mse: 1.5279 - val_loss: 1.6141 - val_mae: 1.0323 - val_mse: 1.6141\n",
      "Epoch 435/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5317 - mae: 1.0135 - mse: 1.5317 - val_loss: 1.6316 - val_mae: 1.0556 - val_mse: 1.6316\n",
      "Epoch 436/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5234 - mae: 1.0166 - mse: 1.5234 - val_loss: 1.6163 - val_mae: 1.0165 - val_mse: 1.6163\n",
      "Epoch 437/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5272 - mae: 1.0151 - mse: 1.5272 - val_loss: 1.6150 - val_mae: 1.0354 - val_mse: 1.6150\n",
      "Epoch 438/450\n",
      "2174/2174 [==============================] - 0s 65us/sample - loss: 1.5284 - mae: 1.0160 - mse: 1.5284 - val_loss: 1.6147 - val_mae: 1.0203 - val_mse: 1.6147\n",
      "Epoch 439/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5289 - mae: 1.0148 - mse: 1.5289 - val_loss: 1.6461 - val_mae: 1.0653 - val_mse: 1.6461\n",
      "Epoch 440/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5283 - mae: 1.0182 - mse: 1.5283 - val_loss: 1.6138 - val_mae: 1.0307 - val_mse: 1.6138\n",
      "Epoch 441/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5343 - mae: 1.0150 - mse: 1.5343 - val_loss: 1.6347 - val_mae: 1.0579 - val_mse: 1.6347\n",
      "Epoch 442/450\n",
      "2174/2174 [==============================] - 0s 79us/sample - loss: 1.5287 - mae: 1.0172 - mse: 1.5287 - val_loss: 1.6428 - val_mae: 1.0633 - val_mse: 1.6428\n",
      "Epoch 443/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5301 - mae: 1.0178 - mse: 1.5301 - val_loss: 1.6136 - val_mae: 1.0289 - val_mse: 1.6136\n",
      "Epoch 444/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5274 - mae: 1.0164 - mse: 1.5274 - val_loss: 1.6179 - val_mae: 1.0412 - val_mse: 1.6179\n",
      "Epoch 445/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5263 - mae: 1.0159 - mse: 1.5263 - val_loss: 1.6138 - val_mae: 1.0308 - val_mse: 1.6138\n",
      "Epoch 446/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5300 - mae: 1.0135 - mse: 1.5300 - val_loss: 1.6270 - val_mae: 1.0518 - val_mse: 1.6270\n",
      "Epoch 447/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5285 - mae: 1.0148 - mse: 1.5285 - val_loss: 1.6137 - val_mae: 1.0299 - val_mse: 1.6137\n",
      "Epoch 448/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5294 - mae: 1.0186 - mse: 1.5294 - val_loss: 1.6148 - val_mae: 1.0349 - val_mse: 1.6148\n",
      "Epoch 449/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5319 - mae: 1.0168 - mse: 1.5319 - val_loss: 1.6155 - val_mae: 1.0368 - val_mse: 1.6155\n",
      "Epoch 450/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5252 - mae: 1.0150 - mse: 1.5252 - val_loss: 1.6229 - val_mae: 1.0477 - val_mse: 1.6229\n"
     ]
    }
   ],
   "source": [
    "trained_weight = ks.get_weights()[0]\n",
    "trained_bias = ks.get_weights()[1]\n",
    "\n",
    "EPOCHS = 450\n",
    "history = ks.fit(X_train,\n",
    "                 y_train,\n",
    "                 epochs = EPOCHS,\n",
    "                 batch_size = 128,\n",
    "                 validation_split = 0.2,\n",
    "                 verbose = 1)\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist['mse']\n",
    "epochs = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    plt.plot(feature.tolist(), label.tolist(), c='r')\n",
    "    # Render the scatter plot and the red line.\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.97, mse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature = X['all'].copy()\n",
    "#label = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_the_model(trained_weight, trained_bias, feature, label)\n",
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              loss         mae          mse    val_loss     val_mae  \\\n",
      "count   450.000000  450.000000   450.000000  450.000000  450.000000   \n",
      "mean      8.425681    1.046465     8.425682    1.624766    1.037834   \n",
      "std     146.227425    0.634833   146.227451    0.028591    0.018260   \n",
      "min       1.520986    1.003658     1.520986    1.596156    0.962612   \n",
      "25%       1.527757    1.014935     1.527757    1.614285    1.026873   \n",
      "50%       1.529939    1.016399     1.529939    1.617700    1.036768   \n",
      "75%       1.532608    1.017893     1.532608    1.627898    1.051100   \n",
      "max    3103.484555   14.483106  3103.485107    2.101643    1.104442   \n",
      "\n",
      "          val_mse  \n",
      "count  450.000000  \n",
      "mean     1.624766  \n",
      "std      0.028591  \n",
      "min      1.596155  \n",
      "25%      1.614285  \n",
      "50%      1.617700  \n",
      "75%      1.627898  \n",
      "max      2.101643  \n"
     ]
    }
   ],
   "source": [
    "print(hist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(a,b):\n",
    "    #print('pred :',a,'actual :',b)\n",
    "    if a == b:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.027766386623852446\n",
      "dt 0.11047963636925107\n",
      "rf 0.49302376729047986\n",
      "vr 0.27120292763893195\n",
      "ks 1.0273545\n"
     ]
    }
   ],
   "source": [
    "print('lr',lr.score(X_train, y_train))\n",
    "print('dt',dt.score(X_train, y_train))\n",
    "print('rf',rf.score(X_train, y_train))\n",
    "print('vr',vr.score(X_train, y_train))\n",
    "ks_test = ks.evaluate(X_train, y_train,verbose=0)\n",
    "print('ks',ks_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_pred_test(result,num):\n",
    "    score = check(result,y_test.loc[num])\n",
    "    return score\n",
    "\n",
    "def model_pred_test(model):\n",
    "    pred = []\n",
    "    numbers = X_test.index\n",
    "    random_nums = random.choices(numbers, k=50)\n",
    "    for i in random_nums:\n",
    "        p = X_test.loc[i].tolist()\n",
    "        result = model.predict([p]).flatten().round()\n",
    "        prediction = cycle_pred_test(result,i)\n",
    "        pred.append(prediction)\n",
    "    dz = pd.DataFrame(pred)\n",
    "    #df = pd.DataFrame(prob)\n",
    "    c = str(float(dz.sum().values / 50))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(index=range(10),columns=['vr','dt','rf','ks'])\n",
    "test_results = test_results.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    v_s = model_pred_test(vr)\n",
    "    test_results.at[i,'vr'] = v_s\n",
    "    d_s = model_pred_test(dt)\n",
    "    test_results.at[i,'dt'] = d_s\n",
    "    r_s = model_pred_test(rf)\n",
    "    test_results.at[i,'rf'] = r_s\n",
    "    k_s = model_pred_test(ks)\n",
    "    test_results.at[i,'ks'] = k_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.iloc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    e = e[0]\n",
    "    if e < 1:\n",
    "        e = 0\n",
    "    elif e < 2:\n",
    "        e = 1\n",
    "    return e\n",
    "\n",
    "def model_pred_test(model):\n",
    "    b = []\n",
    "    prob = []\n",
    "    random_nums = np.random.randint(low=1, high=58, size=(20))\n",
    "    for i in random_nums:\n",
    "        prob.append(cycle_prob_test(i,model))\n",
    "    df = pd.DataFrame(prob)\n",
    "    df = df.values\n",
    "    print('scores :\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_score_regressor.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import cpl_main as cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forge FC Cavalry FC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todd/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "#model_pred_test(cpl_classifier_model)\n",
    "\n",
    "# home side\n",
    "q1 = schedule.iloc[0]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[0]['away']\n",
    "print(q1,q2)\n",
    "\n",
    "compare = cpl.get_team_comparison(results_brief,q1,q2)\n",
    "\n",
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)\n",
    "\n",
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "game_info\n",
    "\n",
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)\n",
    "\n",
    "home_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "away_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.33\n"
     ]
    }
   ],
   "source": [
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db\n",
    "\n",
    "q1_roster = get_overall_roster(home_roster)\n",
    "#print(q1_roster)\n",
    "\n",
    "q2_roster = get_overall_roster(away_roster)\n",
    "#print(q2_roster)\n",
    "\n",
    "def roster_regressor_pred(model,array):\n",
    "    prediction = model.predict([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    return df\n",
    "\n",
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)\n",
    "print(home_win, away_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n",
      "0.35\n",
      "\n",
      " Forge FC \n",
      "win probability:  0.34\n",
      "\n",
      " Cavalry FC \n",
      "win probability:  0.35\n",
      "\n",
      "Draw probability:  0.31\n"
     ]
    }
   ],
   "source": [
    "classifier = 'models/cpl_roster_classifier.sav'\n",
    "cpl_classifier_model = pickle.load(open(classifier, 'rb'))\n",
    "\n",
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(home_win_new)\n",
    "print(away_win_new)\n",
    "\n",
    "print('\\n',q1,'\\nwin probability: ', round(home_win_new,2))\n",
    "\n",
    "print('\\n',q2,'\\nwin probability: ', round(away_win_new,2))\n",
    "\n",
    "print('\\nDraw probability: ', round(draw_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/cpl_score_regressor.sav'\n",
    "cpl_score_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_score_prediction(model,q1_roster,q2_roster,home_win_new,away_win_new):\n",
    "\n",
    "    def roster_pred(model,array):\n",
    "        prediction = model.predict([array]).flatten()\n",
    "        return prediction\n",
    "\n",
    "    def final_score_fix(home_score,away_score,home_win_new,away_win_new):\n",
    "        if home_win_new > away_win_new and home_score < away_score: # fix the score prediction - if the probability of home win > away win and score doesn't reflect it\n",
    "            old_home = home_score\n",
    "            home_score = away_score # change the predicted score to reflect that\n",
    "            away_score = old_home\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score > away_score: # else the probability of home win < away win\n",
    "            old_away = away_score\n",
    "            away_score = home_score # change the predicted score to reflect that\n",
    "            home_score = away_score\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new > away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        else:\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "\n",
    "    def score(num): #improve this later for greater predictions\n",
    "        new_score = int(round(num,0)) # convert the float value to int and round it\n",
    "        return new_score\n",
    "\n",
    "    q1_pred = roster_pred(model,q1_roster)\n",
    "    q1_s = score(q1_pred[0])\n",
    "    q2_pred = roster_pred(model,q2_roster)\n",
    "    q2_s = score(q2_pred[0])\n",
    "    home_score, away_score, home_win_new, away_win_new = final_score_fix(q1_s, q2_s,home_win_new,away_win_new)\n",
    "    return home_score,away_score, home_win_new, away_win_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home 2 away 2\n"
     ]
    }
   ],
   "source": [
    "home_score, away_score, home_win_new, away_win_new = get_final_score_prediction(cpl_score_model,q1_roster,q2_roster,home_win_new,away_win_new)\n",
    "print('home',home_score,'away', away_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = q1_roster.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = l.values.tolist()\n",
    "l = l[0]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "average = statistics.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4421428516507149"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-136c17fc3cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "l.extend(average)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
