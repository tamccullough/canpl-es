{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>team</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I1</td>\n",
       "      <td>Forge FC</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I1</td>\n",
       "      <td>York9 FC</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I2</td>\n",
       "      <td>Pacific FC</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I2</td>\n",
       "      <td>HFX Wanderers FC</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I3</td>\n",
       "      <td>Pacific FC</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>XVIII389</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>XVIII390</td>\n",
       "      <td>NY Red Bulls</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>XVIII390</td>\n",
       "      <td>Orlando City</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>XVIII391</td>\n",
       "      <td>NYCFC</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>XVIII391</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1790 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          game              team    p1    p2    p3    p4    p5    p6    p7  \\\n",
       "0           I1          Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77   \n",
       "1           I1          York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65   \n",
       "2           I2        Pacific FC  0.54  0.78  0.69  0.50  0.39  0.72  0.69   \n",
       "3           I2  HFX Wanderers FC  0.54  0.92  0.77  0.67  0.62  0.57  0.52   \n",
       "4           I3        Pacific FC  0.54  0.78  0.69  0.50  0.39  0.72  0.69   \n",
       "...        ...               ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1785  XVIII389           Atlanta  0.73  0.82  0.74  0.70  0.53  0.94  0.85   \n",
       "1786  XVIII390      NY Red Bulls  0.72  0.74  0.63  0.60  0.58  0.79  0.70   \n",
       "1787  XVIII390      Orlando City  0.58  0.71  0.43  0.36  0.26  0.66  0.40   \n",
       "1788  XVIII391             NYCFC  0.77  0.66  0.56  0.46  0.42  0.97  0.60   \n",
       "1789  XVIII391      Philadelphia  0.85  0.82  0.72  0.46  0.39  0.80  0.72   \n",
       "\n",
       "        p8    p9  p9.1   p10  p11  p12  p13  r  s  \n",
       "0     0.52  0.50  0.48  0.42    0    0    0  2  1  \n",
       "1     0.64  0.46  0.70  0.47    0    0    0  2  1  \n",
       "2     0.57  0.51  0.79  0.27    0    0    0  3  1  \n",
       "3     0.47  0.37  0.77  0.28    0    0    0  1  0  \n",
       "4     0.57  0.51  0.79  0.27    0    0    0  1  1  \n",
       "...    ...   ...   ...   ...  ...  ...  ... .. ..  \n",
       "1785  0.62  0.40  0.84  0.03    0    0    0  1  1  \n",
       "1786  0.69  0.61  0.06  0.03    0    0    0  3  1  \n",
       "1787  0.33  0.33  0.62  0.47    0    0    0  1  0  \n",
       "1788  0.54  0.49  0.67  0.33    0    0    0  3  3  \n",
       "1789  0.66  0.65  0.44  0.00    0    0    0  1  1  \n",
       "\n",
       "[1790 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790, 18)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5370, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pump_it_up(results)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16110, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pump_it_up(df)\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "y = db.pop('s')\n",
    "db.pop('r')\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
       "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
       "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
       "\n",
       "   p13   all  \n",
       "0  0.0  0.59  \n",
       "1  0.0  0.64  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "#colours = sns.set_palette('pastel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model\n",
      "\n",
      "RMSE:  1.2706527111336665\n",
      "\n",
      "Score 1.29\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model\n",
    "def linearRegression():\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "lr = linearRegression()\n",
    "\n",
    "print('Linear Regression Model')\n",
    "\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n",
    "print('\\nScore',round(lr.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regression Model\n",
      "\n",
      "RMSE:  1.2683436222214746\n",
      "\n",
      "Score 1.65\n"
     ]
    }
   ],
   "source": [
    "#DecisionTreeRegressor\n",
    "def decisionTree():\n",
    "    model = DecisionTreeRegressor(criterion='mse', splitter='random', max_depth=8, max_features='log2')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "dt = decisionTree()\n",
    "\n",
    "print('Decision Tree Regression Model')\n",
    "\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test, dt.predict(X_test))))\n",
    "print('\\nScore',round(dt.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression Model\n",
      "\n",
      "RMSE:  1.2733561723611195\n",
      "\n",
      "Score 0.87\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Regression\n",
    "def forestRegression():\n",
    "    model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "    \n",
    "rf = forestRegression()\n",
    "\n",
    "print('Random Forest Regression Model')\n",
    "\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,rf.predict(X_test))))\n",
    "print('\\nScore',round(rf.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "vr = VotingRegressor(estimators=[('lr', lr), ('dt', dt), ('rf', rf)])\n",
    "vr = vr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Regressor Model\n",
      "\n",
      "RMSE:  1.263311275380111\n",
      "\n",
      "Score 2.43\n"
     ]
    }
   ],
   "source": [
    "print('Voting Regressor Model')\n",
    "\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,vr.predict(X_test))))\n",
    "print('\\nScore',round(vr.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasSequential():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                optimizer = tf.keras.optimizers.RMSprop(0.1),\n",
    "                metrics = ['mae', 'mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1024      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,249\n",
      "Trainable params: 5,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ks = kerasSequential()\n",
    "ks.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weight = ks.get_weights()[0]\n",
    "trained_bias = ks.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9021 samples, validate on 2256 samples\n",
      "Epoch 1/450\n",
      "9021/9021 [==============================] - 2s 174us/sample - loss: 1329.5937 - mae: 5.3999 - mse: 1329.5936 - val_loss: 1.5562 - val_mae: 1.0193 - val_mse: 1.5562\n",
      "Epoch 2/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.9748 - mae: 1.0994 - mse: 1.9748 - val_loss: 1.9628 - val_mae: 1.0556 - val_mse: 1.9628\n",
      "Epoch 3/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.6291 - mae: 1.0267 - mse: 1.6291 - val_loss: 1.5061 - val_mae: 0.9974 - val_mse: 1.5061\n",
      "Epoch 4/450\n",
      "9021/9021 [==============================] - 0s 49us/sample - loss: 1.5802 - mae: 1.0242 - mse: 1.5802 - val_loss: 1.4931 - val_mae: 1.0069 - val_mse: 1.4931\n",
      "Epoch 5/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5800 - mae: 1.0252 - mse: 1.5800 - val_loss: 1.5105 - val_mae: 0.9962 - val_mse: 1.5105\n",
      "Epoch 6/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5779 - mae: 1.0254 - mse: 1.5779 - val_loss: 1.4955 - val_mae: 1.0024 - val_mse: 1.4955\n",
      "Epoch 7/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5770 - mae: 1.0246 - mse: 1.5770 - val_loss: 1.5083 - val_mae: 0.9969 - val_mse: 1.5083\n",
      "Epoch 8/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5783 - mae: 1.0253 - mse: 1.5783 - val_loss: 1.4950 - val_mae: 1.0027 - val_mse: 1.4950\n",
      "Epoch 9/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5816 - mae: 1.0264 - mse: 1.5816 - val_loss: 1.5002 - val_mae: 0.9998 - val_mse: 1.5002\n",
      "Epoch 10/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5792 - mae: 1.0260 - mse: 1.5792 - val_loss: 1.4989 - val_mae: 1.0118 - val_mse: 1.4989\n",
      "Epoch 11/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5770 - mae: 1.0256 - mse: 1.5770 - val_loss: 1.4972 - val_mae: 1.0109 - val_mse: 1.4972\n",
      "Epoch 12/450\n",
      "9021/9021 [==============================] - 1s 79us/sample - loss: 1.5784 - mae: 1.0244 - mse: 1.5784 - val_loss: 1.4930 - val_mae: 1.0057 - val_mse: 1.4930\n",
      "Epoch 13/450\n",
      "9021/9021 [==============================] - 1s 96us/sample - loss: 1.5766 - mae: 1.0246 - mse: 1.5766 - val_loss: 1.4930 - val_mae: 1.0059 - val_mse: 1.4930\n",
      "Epoch 14/450\n",
      "9021/9021 [==============================] - 1s 99us/sample - loss: 1.5766 - mae: 1.0255 - mse: 1.5766 - val_loss: 1.4954 - val_mae: 1.0097 - val_mse: 1.4954\n",
      "Epoch 15/450\n",
      "9021/9021 [==============================] - 1s 73us/sample - loss: 1.5791 - mae: 1.0254 - mse: 1.5791 - val_loss: 1.5043 - val_mae: 0.9981 - val_mse: 1.5043\n",
      "Epoch 16/450\n",
      "9021/9021 [==============================] - 1s 57us/sample - loss: 1.5807 - mae: 1.0261 - mse: 1.5807 - val_loss: 1.5130 - val_mae: 0.9955 - val_mse: 1.5130\n",
      "Epoch 17/450\n",
      "9021/9021 [==============================] - 0s 53us/sample - loss: 1.5762 - mae: 1.0237 - mse: 1.5762 - val_loss: 1.4976 - val_mae: 1.0010 - val_mse: 1.4976\n",
      "Epoch 18/450\n",
      "9021/9021 [==============================] - 1s 74us/sample - loss: 1.5804 - mae: 1.0269 - mse: 1.5804 - val_loss: 1.4961 - val_mae: 1.0102 - val_mse: 1.4961\n",
      "Epoch 19/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5756 - mae: 1.0246 - mse: 1.5756 - val_loss: 1.4937 - val_mae: 1.0041 - val_mse: 1.4937\n",
      "Epoch 20/450\n",
      "9021/9021 [==============================] - 1s 81us/sample - loss: 1.5780 - mae: 1.0260 - mse: 1.5780 - val_loss: 1.4964 - val_mae: 1.0104 - val_mse: 1.4964\n",
      "Epoch 21/450\n",
      "9021/9021 [==============================] - 1s 69us/sample - loss: 1.5799 - mae: 1.0265 - mse: 1.5799 - val_loss: 1.5236 - val_mae: 1.0191 - val_mse: 1.5236\n",
      "Epoch 22/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5789 - mae: 1.0263 - mse: 1.5789 - val_loss: 1.4951 - val_mae: 1.0027 - val_mse: 1.4951\n",
      "Epoch 23/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5783 - mae: 1.0256 - mse: 1.5783 - val_loss: 1.4986 - val_mae: 1.0005 - val_mse: 1.4986\n",
      "Epoch 24/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5794 - mae: 1.0265 - mse: 1.5794 - val_loss: 1.4955 - val_mae: 1.0098 - val_mse: 1.4955\n",
      "Epoch 25/450\n",
      "9021/9021 [==============================] - 1s 93us/sample - loss: 1.5806 - mae: 1.0260 - mse: 1.5806 - val_loss: 1.4930 - val_mae: 1.0060 - val_mse: 1.4930\n",
      "Epoch 26/450\n",
      "9021/9021 [==============================] - 1s 57us/sample - loss: 1.5781 - mae: 1.0259 - mse: 1.5781 - val_loss: 1.4975 - val_mae: 1.0011 - val_mse: 1.4975\n",
      "Epoch 27/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5763 - mae: 1.0248 - mse: 1.5763 - val_loss: 1.4930 - val_mae: 1.0058 - val_mse: 1.4930\n",
      "Epoch 28/450\n",
      "9021/9021 [==============================] - 1s 58us/sample - loss: 1.5765 - mae: 1.0245 - mse: 1.5765 - val_loss: 1.4961 - val_mae: 1.0102 - val_mse: 1.4961\n",
      "Epoch 29/450\n",
      "9021/9021 [==============================] - 1s 74us/sample - loss: 1.5786 - mae: 1.0267 - mse: 1.5786 - val_loss: 1.4984 - val_mae: 1.0115 - val_mse: 1.4984\n",
      "Epoch 30/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5794 - mae: 1.0255 - mse: 1.5794 - val_loss: 1.5116 - val_mae: 1.0162 - val_mse: 1.5116\n",
      "Epoch 31/450\n",
      "9021/9021 [==============================] - 1s 61us/sample - loss: 1.5794 - mae: 1.0265 - mse: 1.5794 - val_loss: 1.4961 - val_mae: 1.0102 - val_mse: 1.4961\n",
      "Epoch 32/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5781 - mae: 1.0247 - mse: 1.5781 - val_loss: 1.5114 - val_mae: 0.9960 - val_mse: 1.5114\n",
      "Epoch 33/450\n",
      "9021/9021 [==============================] - 0s 52us/sample - loss: 1.5796 - mae: 1.0264 - mse: 1.5796 - val_loss: 1.5029 - val_mae: 1.0135 - val_mse: 1.5029\n",
      "Epoch 34/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5798 - mae: 1.0252 - mse: 1.5798 - val_loss: 1.5098 - val_mae: 1.0157 - val_mse: 1.5098\n",
      "Epoch 35/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5792 - mae: 1.0267 - mse: 1.5792 - val_loss: 1.5136 - val_mae: 0.9954 - val_mse: 1.5136\n",
      "Epoch 36/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5794 - mae: 1.0260 - mse: 1.5794 - val_loss: 1.5242 - val_mae: 0.9929 - val_mse: 1.5242\n",
      "Epoch 37/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5788 - mae: 1.0244 - mse: 1.5788 - val_loss: 1.5312 - val_mae: 0.9915 - val_mse: 1.5312\n",
      "Epoch 38/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5765 - mae: 1.0237 - mse: 1.5765 - val_loss: 1.4947 - val_mae: 1.0091 - val_mse: 1.4947\n",
      "Epoch 39/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5768 - mae: 1.0244 - mse: 1.5768 - val_loss: 1.4930 - val_mae: 1.0062 - val_mse: 1.4930\n",
      "Epoch 40/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5765 - mae: 1.0251 - mse: 1.5765 - val_loss: 1.5197 - val_mae: 0.9939 - val_mse: 1.5197\n",
      "Epoch 41/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5792 - mae: 1.0257 - mse: 1.5792 - val_loss: 1.4956 - val_mae: 1.0023 - val_mse: 1.4956\n",
      "Epoch 42/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5803 - mae: 1.0266 - mse: 1.5803 - val_loss: 1.4931 - val_mae: 1.0054 - val_mse: 1.4931\n",
      "Epoch 43/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5778 - mae: 1.0260 - mse: 1.5778 - val_loss: 1.4933 - val_mae: 1.0074 - val_mse: 1.4933\n",
      "Epoch 44/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5755 - mae: 1.0242 - mse: 1.5755 - val_loss: 1.5254 - val_mae: 0.9927 - val_mse: 1.5254\n",
      "Epoch 45/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5781 - mae: 1.0249 - mse: 1.5781 - val_loss: 1.4969 - val_mae: 1.0107 - val_mse: 1.4969\n",
      "Epoch 46/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5801 - mae: 1.0266 - mse: 1.5801 - val_loss: 1.5411 - val_mae: 0.9897 - val_mse: 1.5411\n",
      "Epoch 47/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5779 - mae: 1.0246 - mse: 1.5779 - val_loss: 1.5107 - val_mae: 1.0160 - val_mse: 1.5107\n",
      "Epoch 48/450\n",
      "9021/9021 [==============================] - 0s 52us/sample - loss: 1.5815 - mae: 1.0260 - mse: 1.5815 - val_loss: 1.4983 - val_mae: 1.0007 - val_mse: 1.4983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/450\n",
      "9021/9021 [==============================] - 0s 53us/sample - loss: 1.5789 - mae: 1.0262 - mse: 1.5789 - val_loss: 1.4956 - val_mae: 1.0023 - val_mse: 1.4956\n",
      "Epoch 50/450\n",
      "9021/9021 [==============================] - 0s 49us/sample - loss: 1.5782 - mae: 1.0259 - mse: 1.5782 - val_loss: 1.5181 - val_mae: 1.0179 - val_mse: 1.5181\n",
      "Epoch 51/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5777 - mae: 1.0251 - mse: 1.5777 - val_loss: 1.4947 - val_mae: 1.0091 - val_mse: 1.4947\n",
      "Epoch 52/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5784 - mae: 1.0257 - mse: 1.5784 - val_loss: 1.4932 - val_mae: 1.0070 - val_mse: 1.4932\n",
      "Epoch 53/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5791 - mae: 1.0244 - mse: 1.5791 - val_loss: 1.4932 - val_mae: 1.0051 - val_mse: 1.4932\n",
      "Epoch 54/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5788 - mae: 1.0261 - mse: 1.5788 - val_loss: 1.5262 - val_mae: 0.9925 - val_mse: 1.5262\n",
      "Epoch 55/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5816 - mae: 1.0248 - mse: 1.5816 - val_loss: 1.5168 - val_mae: 1.0176 - val_mse: 1.5168\n",
      "Epoch 56/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5794 - mae: 1.0264 - mse: 1.5794 - val_loss: 1.4969 - val_mae: 1.0014 - val_mse: 1.4969\n",
      "Epoch 57/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5808 - mae: 1.0262 - mse: 1.5808 - val_loss: 1.5093 - val_mae: 0.9966 - val_mse: 1.5093\n",
      "Epoch 58/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5781 - mae: 1.0248 - mse: 1.5781 - val_loss: 1.5099 - val_mae: 0.9964 - val_mse: 1.5099\n",
      "Epoch 59/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5794 - mae: 1.0265 - mse: 1.5794 - val_loss: 1.4992 - val_mae: 1.0119 - val_mse: 1.4992\n",
      "Epoch 60/450\n",
      "9021/9021 [==============================] - 1s 58us/sample - loss: 1.5802 - mae: 1.0266 - mse: 1.5802 - val_loss: 1.4954 - val_mae: 1.0097 - val_mse: 1.4954\n",
      "Epoch 61/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5765 - mae: 1.0256 - mse: 1.5765 - val_loss: 1.5041 - val_mae: 0.9982 - val_mse: 1.5041\n",
      "Epoch 62/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5798 - mae: 1.0262 - mse: 1.5798 - val_loss: 1.4993 - val_mae: 1.0120 - val_mse: 1.4993\n",
      "Epoch 63/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5778 - mae: 1.0260 - mse: 1.5778 - val_loss: 1.5038 - val_mae: 1.0138 - val_mse: 1.5038\n",
      "Epoch 64/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5807 - mae: 1.0270 - mse: 1.5807 - val_loss: 1.4932 - val_mae: 1.0070 - val_mse: 1.4932\n",
      "Epoch 65/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5802 - mae: 1.0263 - mse: 1.5802 - val_loss: 1.5131 - val_mae: 0.9955 - val_mse: 1.5131\n",
      "Epoch 66/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5793 - mae: 1.0254 - mse: 1.5793 - val_loss: 1.4988 - val_mae: 1.0117 - val_mse: 1.4988\n",
      "Epoch 67/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5795 - mae: 1.0268 - mse: 1.5795 - val_loss: 1.5019 - val_mae: 0.9990 - val_mse: 1.5019\n",
      "Epoch 68/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5792 - mae: 1.0262 - mse: 1.5792 - val_loss: 1.5037 - val_mae: 1.0138 - val_mse: 1.5037\n",
      "Epoch 69/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5780 - mae: 1.0256 - mse: 1.5780 - val_loss: 1.5126 - val_mae: 0.9956 - val_mse: 1.5126\n",
      "Epoch 70/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5798 - mae: 1.0255 - mse: 1.5798 - val_loss: 1.4930 - val_mae: 1.0064 - val_mse: 1.4930\n",
      "Epoch 71/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5777 - mae: 1.0253 - mse: 1.5777 - val_loss: 1.5048 - val_mae: 1.0141 - val_mse: 1.5048\n",
      "Epoch 72/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5785 - mae: 1.0264 - mse: 1.5785 - val_loss: 1.5285 - val_mae: 1.0201 - val_mse: 1.5285\n",
      "Epoch 73/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5762 - mae: 1.0246 - mse: 1.5762 - val_loss: 1.5213 - val_mae: 1.0186 - val_mse: 1.5213\n",
      "Epoch 74/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5780 - mae: 1.0253 - mse: 1.5780 - val_loss: 1.4930 - val_mae: 1.0058 - val_mse: 1.4930\n",
      "Epoch 75/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5783 - mae: 1.0245 - mse: 1.5783 - val_loss: 1.4971 - val_mae: 1.0013 - val_mse: 1.4971\n",
      "Epoch 76/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5759 - mae: 1.0248 - mse: 1.5759 - val_loss: 1.5002 - val_mae: 0.9997 - val_mse: 1.5002\n",
      "Epoch 77/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5780 - mae: 1.0244 - mse: 1.5780 - val_loss: 1.4930 - val_mae: 1.0064 - val_mse: 1.4930\n",
      "Epoch 78/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5780 - mae: 1.0258 - mse: 1.5780 - val_loss: 1.4977 - val_mae: 1.0010 - val_mse: 1.4977\n",
      "Epoch 79/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5783 - mae: 1.0244 - mse: 1.5783 - val_loss: 1.5037 - val_mae: 1.0138 - val_mse: 1.5037\n",
      "Epoch 80/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5781 - mae: 1.0255 - mse: 1.5781 - val_loss: 1.4985 - val_mae: 1.0116 - val_mse: 1.4985\n",
      "Epoch 81/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5788 - mae: 1.0256 - mse: 1.5788 - val_loss: 1.4994 - val_mae: 1.0001 - val_mse: 1.4994\n",
      "Epoch 82/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5774 - mae: 1.0252 - mse: 1.5774 - val_loss: 1.5290 - val_mae: 0.9919 - val_mse: 1.5290\n",
      "Epoch 83/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5808 - mae: 1.0267 - mse: 1.5808 - val_loss: 1.5047 - val_mae: 0.9980 - val_mse: 1.5047\n",
      "Epoch 84/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5787 - mae: 1.0253 - mse: 1.5787 - val_loss: 1.4958 - val_mae: 1.0021 - val_mse: 1.4958\n",
      "Epoch 85/450\n",
      "9021/9021 [==============================] - 1s 81us/sample - loss: 1.5790 - mae: 1.0257 - mse: 1.5790 - val_loss: 1.4982 - val_mae: 1.0007 - val_mse: 1.4982\n",
      "Epoch 86/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5760 - mae: 1.0245 - mse: 1.5760 - val_loss: 1.5191 - val_mae: 0.9940 - val_mse: 1.5191\n",
      "Epoch 87/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5781 - mae: 1.0242 - mse: 1.5781 - val_loss: 1.5337 - val_mae: 1.0211 - val_mse: 1.5337\n",
      "Epoch 88/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5781 - mae: 1.0257 - mse: 1.5781 - val_loss: 1.4944 - val_mae: 1.0033 - val_mse: 1.4944\n",
      "Epoch 89/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5780 - mae: 1.0246 - mse: 1.5780 - val_loss: 1.5259 - val_mae: 1.0196 - val_mse: 1.5259\n",
      "Epoch 90/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5785 - mae: 1.0253 - mse: 1.5785 - val_loss: 1.4944 - val_mae: 1.0088 - val_mse: 1.4944\n",
      "Epoch 91/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5794 - mae: 1.0271 - mse: 1.5794 - val_loss: 1.5030 - val_mae: 1.0135 - val_mse: 1.5030\n",
      "Epoch 92/450\n",
      "9021/9021 [==============================] - 0s 52us/sample - loss: 1.5779 - mae: 1.0261 - mse: 1.5779 - val_loss: 1.4998 - val_mae: 1.0122 - val_mse: 1.4998\n",
      "Epoch 93/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5799 - mae: 1.0256 - mse: 1.5799 - val_loss: 1.5023 - val_mae: 0.9989 - val_mse: 1.5023\n",
      "Epoch 94/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5801 - mae: 1.0258 - mse: 1.5801 - val_loss: 1.4946 - val_mae: 1.0090 - val_mse: 1.4946\n",
      "Epoch 95/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5793 - mae: 1.0264 - mse: 1.5793 - val_loss: 1.4965 - val_mae: 1.0017 - val_mse: 1.4965\n",
      "Epoch 96/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5802 - mae: 1.0268 - mse: 1.5802 - val_loss: 1.5183 - val_mae: 0.9942 - val_mse: 1.5183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5772 - mae: 1.0237 - mse: 1.5772 - val_loss: 1.5056 - val_mae: 1.0144 - val_mse: 1.5056\n",
      "Epoch 98/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5801 - mae: 1.0261 - mse: 1.5801 - val_loss: 1.4934 - val_mae: 1.0075 - val_mse: 1.4934\n",
      "Epoch 99/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5777 - mae: 1.0251 - mse: 1.5777 - val_loss: 1.4983 - val_mae: 1.0115 - val_mse: 1.4983\n",
      "Epoch 100/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5801 - mae: 1.0255 - mse: 1.5801 - val_loss: 1.5557 - val_mae: 1.0247 - val_mse: 1.5557\n",
      "Epoch 101/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5769 - mae: 1.0253 - mse: 1.5769 - val_loss: 1.5261 - val_mae: 0.9925 - val_mse: 1.5261\n",
      "Epoch 102/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5783 - mae: 1.0255 - mse: 1.5783 - val_loss: 1.4954 - val_mae: 1.0097 - val_mse: 1.4954\n",
      "Epoch 103/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5770 - mae: 1.0237 - mse: 1.5770 - val_loss: 1.4988 - val_mae: 1.0117 - val_mse: 1.4988\n",
      "Epoch 104/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5760 - mae: 1.0249 - mse: 1.5760 - val_loss: 1.5126 - val_mae: 1.0165 - val_mse: 1.5126\n",
      "Epoch 105/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5792 - mae: 1.0267 - mse: 1.5792 - val_loss: 1.4942 - val_mae: 1.0035 - val_mse: 1.4942\n",
      "Epoch 106/450\n",
      "9021/9021 [==============================] - 1s 63us/sample - loss: 1.5773 - mae: 1.0258 - mse: 1.5773 - val_loss: 1.4940 - val_mae: 1.0037 - val_mse: 1.4940\n",
      "Epoch 107/450\n",
      "9021/9021 [==============================] - 0s 54us/sample - loss: 1.5753 - mae: 1.0249 - mse: 1.5753 - val_loss: 1.5404 - val_mae: 0.9899 - val_mse: 1.5404\n",
      "Epoch 108/450\n",
      "9021/9021 [==============================] - 1s 58us/sample - loss: 1.5788 - mae: 1.0241 - mse: 1.5788 - val_loss: 1.4956 - val_mae: 1.0098 - val_mse: 1.4956\n",
      "Epoch 109/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5792 - mae: 1.0266 - mse: 1.5792 - val_loss: 1.4953 - val_mae: 1.0096 - val_mse: 1.4953\n",
      "Epoch 110/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5793 - mae: 1.0258 - mse: 1.5793 - val_loss: 1.4931 - val_mae: 1.0053 - val_mse: 1.4931\n",
      "Epoch 111/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5771 - mae: 1.0261 - mse: 1.5771 - val_loss: 1.4993 - val_mae: 1.0120 - val_mse: 1.4993\n",
      "Epoch 112/450\n",
      "9021/9021 [==============================] - 0s 53us/sample - loss: 1.5772 - mae: 1.0252 - mse: 1.5772 - val_loss: 1.4931 - val_mae: 1.0066 - val_mse: 1.4931\n",
      "Epoch 113/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5786 - mae: 1.0270 - mse: 1.5786 - val_loss: 1.5031 - val_mae: 1.0135 - val_mse: 1.5031\n",
      "Epoch 114/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5800 - mae: 1.0256 - mse: 1.5800 - val_loss: 1.4932 - val_mae: 1.0072 - val_mse: 1.4932\n",
      "Epoch 115/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5785 - mae: 1.0245 - mse: 1.5785 - val_loss: 1.4988 - val_mae: 1.0004 - val_mse: 1.4988\n",
      "Epoch 116/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5801 - mae: 1.0261 - mse: 1.5801 - val_loss: 1.4966 - val_mae: 1.0106 - val_mse: 1.4966\n",
      "Epoch 117/450\n",
      "9021/9021 [==============================] - 0s 54us/sample - loss: 1.5795 - mae: 1.0269 - mse: 1.5795 - val_loss: 1.4958 - val_mae: 1.0021 - val_mse: 1.4958\n",
      "Epoch 118/450\n",
      "9021/9021 [==============================] - 0s 53us/sample - loss: 1.5792 - mae: 1.0267 - mse: 1.5792 - val_loss: 1.4942 - val_mae: 1.0035 - val_mse: 1.4942\n",
      "Epoch 119/450\n",
      "9021/9021 [==============================] - 1s 58us/sample - loss: 1.5774 - mae: 1.0245 - mse: 1.5774 - val_loss: 1.5015 - val_mae: 0.9992 - val_mse: 1.5015\n",
      "Epoch 120/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5796 - mae: 1.0264 - mse: 1.5796 - val_loss: 1.4983 - val_mae: 1.0115 - val_mse: 1.4983\n",
      "Epoch 121/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5756 - mae: 1.0244 - mse: 1.5756 - val_loss: 1.4943 - val_mae: 1.0088 - val_mse: 1.4943\n",
      "Epoch 122/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5777 - mae: 1.0251 - mse: 1.5777 - val_loss: 1.5438 - val_mae: 0.9893 - val_mse: 1.5438\n",
      "Epoch 123/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5769 - mae: 1.0253 - mse: 1.5769 - val_loss: 1.4935 - val_mae: 1.0045 - val_mse: 1.4935\n",
      "Epoch 124/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5807 - mae: 1.0279 - mse: 1.5807 - val_loss: 1.5282 - val_mae: 0.9921 - val_mse: 1.5282\n",
      "Epoch 125/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5789 - mae: 1.0250 - mse: 1.5789 - val_loss: 1.4935 - val_mae: 1.0077 - val_mse: 1.4935\n",
      "Epoch 126/450\n",
      "9021/9021 [==============================] - 1s 64us/sample - loss: 1.5793 - mae: 1.0270 - mse: 1.5793 - val_loss: 1.4958 - val_mae: 1.0100 - val_mse: 1.4958\n",
      "Epoch 127/450\n",
      "9021/9021 [==============================] - 0s 53us/sample - loss: 1.5786 - mae: 1.0271 - mse: 1.5786 - val_loss: 1.4931 - val_mae: 1.0066 - val_mse: 1.4931\n",
      "Epoch 128/450\n",
      "9021/9021 [==============================] - 0s 54us/sample - loss: 1.5787 - mae: 1.0254 - mse: 1.5787 - val_loss: 1.4930 - val_mae: 1.0063 - val_mse: 1.4930\n",
      "Epoch 129/450\n",
      "9021/9021 [==============================] - 0s 53us/sample - loss: 1.5795 - mae: 1.0261 - mse: 1.5795 - val_loss: 1.4930 - val_mae: 1.0061 - val_mse: 1.4930\n",
      "Epoch 130/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5793 - mae: 1.0264 - mse: 1.5793 - val_loss: 1.5074 - val_mae: 1.0150 - val_mse: 1.5074\n",
      "Epoch 131/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5795 - mae: 1.0265 - mse: 1.5795 - val_loss: 1.4943 - val_mae: 1.0034 - val_mse: 1.4943\n",
      "Epoch 132/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5779 - mae: 1.0261 - mse: 1.5779 - val_loss: 1.5115 - val_mae: 0.9959 - val_mse: 1.5115\n",
      "Epoch 133/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5797 - mae: 1.0258 - mse: 1.5797 - val_loss: 1.4935 - val_mae: 1.0078 - val_mse: 1.4935\n",
      "Epoch 134/450\n",
      "9021/9021 [==============================] - 1s 61us/sample - loss: 1.5787 - mae: 1.0247 - mse: 1.5787 - val_loss: 1.4935 - val_mae: 1.0045 - val_mse: 1.4935\n",
      "Epoch 135/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5778 - mae: 1.0260 - mse: 1.5778 - val_loss: 1.4946 - val_mae: 1.0091 - val_mse: 1.4946\n",
      "Epoch 136/450\n",
      "9021/9021 [==============================] - 1s 62us/sample - loss: 1.5801 - mae: 1.0263 - mse: 1.5801 - val_loss: 1.5114 - val_mae: 0.9960 - val_mse: 1.5114\n",
      "Epoch 137/450\n",
      "9021/9021 [==============================] - 1s 57us/sample - loss: 1.5792 - mae: 1.0259 - mse: 1.5792 - val_loss: 1.4930 - val_mae: 1.0058 - val_mse: 1.4930\n",
      "Epoch 138/450\n",
      "9021/9021 [==============================] - 1s 63us/sample - loss: 1.5772 - mae: 1.0252 - mse: 1.5772 - val_loss: 1.5039 - val_mae: 1.0139 - val_mse: 1.5039\n",
      "Epoch 139/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5770 - mae: 1.0252 - mse: 1.5770 - val_loss: 1.4931 - val_mae: 1.0066 - val_mse: 1.4931\n",
      "Epoch 140/450\n",
      "9021/9021 [==============================] - 1s 63us/sample - loss: 1.5779 - mae: 1.0253 - mse: 1.5779 - val_loss: 1.4944 - val_mae: 1.0088 - val_mse: 1.4944\n",
      "Epoch 141/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5795 - mae: 1.0257 - mse: 1.5795 - val_loss: 1.4934 - val_mae: 1.0076 - val_mse: 1.4934\n",
      "Epoch 142/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5787 - mae: 1.0249 - mse: 1.5787 - val_loss: 1.4975 - val_mae: 1.0111 - val_mse: 1.4975\n",
      "Epoch 143/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5793 - mae: 1.0261 - mse: 1.5793 - val_loss: 1.5219 - val_mae: 1.0187 - val_mse: 1.5219\n",
      "Epoch 144/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5780 - mae: 1.0255 - mse: 1.5780 - val_loss: 1.5086 - val_mae: 1.0154 - val_mse: 1.5086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5801 - mae: 1.0262 - mse: 1.5801 - val_loss: 1.4949 - val_mae: 1.0028 - val_mse: 1.4949\n",
      "Epoch 146/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5780 - mae: 1.0249 - mse: 1.5780 - val_loss: 1.5145 - val_mae: 0.9951 - val_mse: 1.5145\n",
      "Epoch 147/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5765 - mae: 1.0240 - mse: 1.5765 - val_loss: 1.4943 - val_mae: 1.0034 - val_mse: 1.4943\n",
      "Epoch 148/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5795 - mae: 1.0258 - mse: 1.5795 - val_loss: 1.4940 - val_mae: 1.0038 - val_mse: 1.4940\n",
      "Epoch 149/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5779 - mae: 1.0250 - mse: 1.5779 - val_loss: 1.5149 - val_mae: 0.9951 - val_mse: 1.5149\n",
      "Epoch 150/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5786 - mae: 1.0249 - mse: 1.5786 - val_loss: 1.4946 - val_mae: 1.0031 - val_mse: 1.4946\n",
      "Epoch 151/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5780 - mae: 1.0254 - mse: 1.5780 - val_loss: 1.5062 - val_mae: 1.0146 - val_mse: 1.5062\n",
      "Epoch 152/450\n",
      "9021/9021 [==============================] - 1s 58us/sample - loss: 1.5753 - mae: 1.0247 - mse: 1.5753 - val_loss: 1.4939 - val_mae: 1.0038 - val_mse: 1.4939\n",
      "Epoch 153/450\n",
      "9021/9021 [==============================] - 1s 69us/sample - loss: 1.5805 - mae: 1.0254 - mse: 1.5805 - val_loss: 1.4945 - val_mae: 1.0089 - val_mse: 1.4945\n",
      "Epoch 154/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5770 - mae: 1.0254 - mse: 1.5770 - val_loss: 1.4956 - val_mae: 1.0099 - val_mse: 1.4956\n",
      "Epoch 155/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5791 - mae: 1.0261 - mse: 1.5791 - val_loss: 1.4932 - val_mae: 1.0049 - val_mse: 1.4932\n",
      "Epoch 156/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5778 - mae: 1.0259 - mse: 1.5778 - val_loss: 1.4989 - val_mae: 1.0003 - val_mse: 1.4989\n",
      "Epoch 157/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5815 - mae: 1.0263 - mse: 1.5815 - val_loss: 1.5081 - val_mae: 1.0152 - val_mse: 1.5081\n",
      "Epoch 158/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5736 - mae: 1.0231 - mse: 1.5736 - val_loss: 1.5120 - val_mae: 0.9958 - val_mse: 1.5120\n",
      "Epoch 159/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5795 - mae: 1.0260 - mse: 1.5795 - val_loss: 1.5103 - val_mae: 1.0159 - val_mse: 1.5103\n",
      "Epoch 160/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5790 - mae: 1.0262 - mse: 1.5790 - val_loss: 1.4940 - val_mae: 1.0085 - val_mse: 1.4940\n",
      "Epoch 161/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5775 - mae: 1.0256 - mse: 1.5775 - val_loss: 1.4945 - val_mae: 1.0089 - val_mse: 1.4945\n",
      "Epoch 162/450\n",
      "9021/9021 [==============================] - 1s 74us/sample - loss: 1.5801 - mae: 1.0258 - mse: 1.5801 - val_loss: 1.4964 - val_mae: 1.0104 - val_mse: 1.4964\n",
      "Epoch 163/450\n",
      "9021/9021 [==============================] - 1s 66us/sample - loss: 1.5803 - mae: 1.0250 - mse: 1.5803 - val_loss: 1.4972 - val_mae: 1.0012 - val_mse: 1.4972\n",
      "Epoch 164/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5777 - mae: 1.0259 - mse: 1.5777 - val_loss: 1.5374 - val_mae: 0.9904 - val_mse: 1.5374\n",
      "Epoch 165/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5797 - mae: 1.0261 - mse: 1.5797 - val_loss: 1.4957 - val_mae: 1.0099 - val_mse: 1.4957\n",
      "Epoch 166/450\n",
      "9021/9021 [==============================] - 0s 52us/sample - loss: 1.5777 - mae: 1.0255 - mse: 1.5777 - val_loss: 1.4951 - val_mae: 1.0027 - val_mse: 1.4951\n",
      "Epoch 167/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5777 - mae: 1.0250 - mse: 1.5777 - val_loss: 1.4987 - val_mae: 1.0117 - val_mse: 1.4987\n",
      "Epoch 168/450\n",
      "9021/9021 [==============================] - 0s 54us/sample - loss: 1.5783 - mae: 1.0256 - mse: 1.5783 - val_loss: 1.5082 - val_mae: 1.0153 - val_mse: 1.5082\n",
      "Epoch 169/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5800 - mae: 1.0264 - mse: 1.5800 - val_loss: 1.4931 - val_mae: 1.0053 - val_mse: 1.4931\n",
      "Epoch 170/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5800 - mae: 1.0266 - mse: 1.5800 - val_loss: 1.4930 - val_mae: 1.0064 - val_mse: 1.4930\n",
      "Epoch 171/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5807 - mae: 1.0271 - mse: 1.5807 - val_loss: 1.5002 - val_mae: 0.9998 - val_mse: 1.5002\n",
      "Epoch 172/450\n",
      "9021/9021 [==============================] - 0s 49us/sample - loss: 1.5797 - mae: 1.0263 - mse: 1.5797 - val_loss: 1.5050 - val_mae: 0.9979 - val_mse: 1.5050\n",
      "Epoch 173/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5791 - mae: 1.0263 - mse: 1.5791 - val_loss: 1.5066 - val_mae: 0.9974 - val_mse: 1.5066\n",
      "Epoch 174/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5795 - mae: 1.0269 - mse: 1.5795 - val_loss: 1.4976 - val_mae: 1.0010 - val_mse: 1.4976\n",
      "Epoch 175/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5791 - mae: 1.0262 - mse: 1.5791 - val_loss: 1.4984 - val_mae: 1.0115 - val_mse: 1.4984\n",
      "Epoch 176/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5780 - mae: 1.0257 - mse: 1.5780 - val_loss: 1.5405 - val_mae: 1.0223 - val_mse: 1.5405\n",
      "Epoch 177/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5793 - mae: 1.0266 - mse: 1.5793 - val_loss: 1.4986 - val_mae: 1.0005 - val_mse: 1.4986\n",
      "Epoch 178/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5752 - mae: 1.0247 - mse: 1.5752 - val_loss: 1.5068 - val_mae: 0.9973 - val_mse: 1.5068\n",
      "Epoch 179/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5784 - mae: 1.0251 - mse: 1.5784 - val_loss: 1.4992 - val_mae: 1.0002 - val_mse: 1.4992\n",
      "Epoch 180/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5787 - mae: 1.0261 - mse: 1.5787 - val_loss: 1.5270 - val_mae: 1.0198 - val_mse: 1.5270\n",
      "Epoch 181/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5812 - mae: 1.0272 - mse: 1.5812 - val_loss: 1.4935 - val_mae: 1.0045 - val_mse: 1.4935\n",
      "Epoch 182/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5802 - mae: 1.0255 - mse: 1.5802 - val_loss: 1.4947 - val_mae: 1.0030 - val_mse: 1.4947\n",
      "Epoch 183/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5779 - mae: 1.0251 - mse: 1.5779 - val_loss: 1.4948 - val_mae: 1.0092 - val_mse: 1.4948\n",
      "Epoch 184/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5781 - mae: 1.0252 - mse: 1.5781 - val_loss: 1.5110 - val_mae: 1.0160 - val_mse: 1.5110\n",
      "Epoch 185/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5796 - mae: 1.0259 - mse: 1.5796 - val_loss: 1.5096 - val_mae: 0.9965 - val_mse: 1.5096\n",
      "Epoch 186/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5790 - mae: 1.0268 - mse: 1.5790 - val_loss: 1.5093 - val_mae: 0.9966 - val_mse: 1.5093\n",
      "Epoch 187/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5787 - mae: 1.0258 - mse: 1.5787 - val_loss: 1.4950 - val_mae: 1.0027 - val_mse: 1.4950\n",
      "Epoch 188/450\n",
      "9021/9021 [==============================] - 0s 49us/sample - loss: 1.5781 - mae: 1.0248 - mse: 1.5781 - val_loss: 1.4988 - val_mae: 1.0004 - val_mse: 1.4988\n",
      "Epoch 189/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5780 - mae: 1.0242 - mse: 1.5780 - val_loss: 1.5165 - val_mae: 1.0175 - val_mse: 1.5165\n",
      "Epoch 190/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5770 - mae: 1.0263 - mse: 1.5770 - val_loss: 1.4999 - val_mae: 1.0123 - val_mse: 1.4999\n",
      "Epoch 191/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5786 - mae: 1.0261 - mse: 1.5786 - val_loss: 1.5000 - val_mae: 0.9998 - val_mse: 1.5000\n",
      "Epoch 192/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5789 - mae: 1.0256 - mse: 1.5789 - val_loss: 1.5044 - val_mae: 1.0140 - val_mse: 1.5044\n",
      "Epoch 193/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5789 - mae: 1.0261 - mse: 1.5789 - val_loss: 1.5266 - val_mae: 1.0197 - val_mse: 1.5266\n",
      "Epoch 194/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5784 - mae: 1.0255 - mse: 1.5784 - val_loss: 1.4970 - val_mae: 1.0014 - val_mse: 1.4970\n",
      "Epoch 195/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5783 - mae: 1.0259 - mse: 1.5783 - val_loss: 1.5179 - val_mae: 0.9943 - val_mse: 1.5179\n",
      "Epoch 196/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5778 - mae: 1.0261 - mse: 1.5778 - val_loss: 1.4986 - val_mae: 1.0005 - val_mse: 1.4986\n",
      "Epoch 197/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5776 - mae: 1.0256 - mse: 1.5776 - val_loss: 1.4930 - val_mae: 1.0059 - val_mse: 1.4930\n",
      "Epoch 198/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5810 - mae: 1.0270 - mse: 1.5810 - val_loss: 1.4993 - val_mae: 1.0001 - val_mse: 1.4993\n",
      "Epoch 199/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5785 - mae: 1.0254 - mse: 1.5785 - val_loss: 1.4938 - val_mae: 1.0040 - val_mse: 1.4938\n",
      "Epoch 200/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5822 - mae: 1.0265 - mse: 1.5822 - val_loss: 1.4942 - val_mae: 1.0086 - val_mse: 1.4942\n",
      "Epoch 201/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5775 - mae: 1.0257 - mse: 1.5775 - val_loss: 1.4953 - val_mae: 1.0096 - val_mse: 1.4953\n",
      "Epoch 202/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5799 - mae: 1.0268 - mse: 1.5799 - val_loss: 1.5179 - val_mae: 0.9943 - val_mse: 1.5179\n",
      "Epoch 203/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5810 - mae: 1.0262 - mse: 1.5810 - val_loss: 1.4951 - val_mae: 1.0094 - val_mse: 1.4951\n",
      "Epoch 204/450\n",
      "9021/9021 [==============================] - 1s 73us/sample - loss: 1.5803 - mae: 1.0263 - mse: 1.5803 - val_loss: 1.5037 - val_mae: 0.9984 - val_mse: 1.5037\n",
      "Epoch 205/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5779 - mae: 1.0245 - mse: 1.5779 - val_loss: 1.4998 - val_mae: 1.0122 - val_mse: 1.4998\n",
      "Epoch 206/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5796 - mae: 1.0272 - mse: 1.5796 - val_loss: 1.4938 - val_mae: 1.0082 - val_mse: 1.4938\n",
      "Epoch 207/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5777 - mae: 1.0257 - mse: 1.5777 - val_loss: 1.5224 - val_mae: 0.9933 - val_mse: 1.5224\n",
      "Epoch 208/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5796 - mae: 1.0269 - mse: 1.5796 - val_loss: 1.5404 - val_mae: 0.9899 - val_mse: 1.5404\n",
      "Epoch 209/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5799 - mae: 1.0258 - mse: 1.5799 - val_loss: 1.5092 - val_mae: 1.0156 - val_mse: 1.5092\n",
      "Epoch 210/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5806 - mae: 1.0258 - mse: 1.5805 - val_loss: 1.5242 - val_mae: 0.9929 - val_mse: 1.5242\n",
      "Epoch 211/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5775 - mae: 1.0253 - mse: 1.5775 - val_loss: 1.4979 - val_mae: 1.0009 - val_mse: 1.4979\n",
      "Epoch 212/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5790 - mae: 1.0249 - mse: 1.5790 - val_loss: 1.4936 - val_mae: 1.0042 - val_mse: 1.4936\n",
      "Epoch 213/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5794 - mae: 1.0260 - mse: 1.5794 - val_loss: 1.5125 - val_mae: 0.9957 - val_mse: 1.5125\n",
      "Epoch 214/450\n",
      "9021/9021 [==============================] - 1s 64us/sample - loss: 1.5787 - mae: 1.0257 - mse: 1.5787 - val_loss: 1.4937 - val_mae: 1.0080 - val_mse: 1.4937\n",
      "Epoch 215/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5780 - mae: 1.0246 - mse: 1.5780 - val_loss: 1.5104 - val_mae: 1.0159 - val_mse: 1.5104\n",
      "Epoch 216/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5756 - mae: 1.0239 - mse: 1.5756 - val_loss: 1.4965 - val_mae: 1.0017 - val_mse: 1.4965\n",
      "Epoch 217/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5800 - mae: 1.0261 - mse: 1.5800 - val_loss: 1.5099 - val_mae: 0.9964 - val_mse: 1.5099\n",
      "Epoch 218/450\n",
      "9021/9021 [==============================] - 0s 52us/sample - loss: 1.5791 - mae: 1.0267 - mse: 1.5791 - val_loss: 1.4931 - val_mae: 1.0067 - val_mse: 1.4931\n",
      "Epoch 219/450\n",
      "9021/9021 [==============================] - 1s 57us/sample - loss: 1.5763 - mae: 1.0235 - mse: 1.5763 - val_loss: 1.5405 - val_mae: 1.0223 - val_mse: 1.5405\n",
      "Epoch 220/450\n",
      "9021/9021 [==============================] - 1s 66us/sample - loss: 1.5796 - mae: 1.0257 - mse: 1.5796 - val_loss: 1.4996 - val_mae: 1.0121 - val_mse: 1.4996\n",
      "Epoch 221/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5807 - mae: 1.0267 - mse: 1.5807 - val_loss: 1.4932 - val_mae: 1.0049 - val_mse: 1.4932\n",
      "Epoch 222/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5783 - mae: 1.0264 - mse: 1.5783 - val_loss: 1.4951 - val_mae: 1.0026 - val_mse: 1.4951\n",
      "Epoch 223/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5775 - mae: 1.0256 - mse: 1.5775 - val_loss: 1.5260 - val_mae: 1.0196 - val_mse: 1.5260\n",
      "Epoch 224/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5770 - mae: 1.0261 - mse: 1.5770 - val_loss: 1.5252 - val_mae: 0.9927 - val_mse: 1.5252\n",
      "Epoch 225/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5761 - mae: 1.0244 - mse: 1.5761 - val_loss: 1.5234 - val_mae: 1.0191 - val_mse: 1.5234\n",
      "Epoch 226/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5793 - mae: 1.0263 - mse: 1.5793 - val_loss: 1.5293 - val_mae: 0.9919 - val_mse: 1.5293\n",
      "Epoch 227/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5788 - mae: 1.0258 - mse: 1.5788 - val_loss: 1.4964 - val_mae: 1.0104 - val_mse: 1.4964\n",
      "Epoch 228/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5790 - mae: 1.0247 - mse: 1.5790 - val_loss: 1.5011 - val_mae: 1.0128 - val_mse: 1.5011\n",
      "Epoch 229/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5793 - mae: 1.0258 - mse: 1.5793 - val_loss: 1.4992 - val_mae: 1.0002 - val_mse: 1.4992\n",
      "Epoch 230/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5788 - mae: 1.0258 - mse: 1.5788 - val_loss: 1.5019 - val_mae: 0.9990 - val_mse: 1.5019\n",
      "Epoch 231/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5772 - mae: 1.0235 - mse: 1.5772 - val_loss: 1.4981 - val_mae: 1.0114 - val_mse: 1.4981\n",
      "Epoch 232/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5775 - mae: 1.0248 - mse: 1.5775 - val_loss: 1.4931 - val_mae: 1.0056 - val_mse: 1.4931\n",
      "Epoch 233/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5749 - mae: 1.0242 - mse: 1.5749 - val_loss: 1.4958 - val_mae: 1.0100 - val_mse: 1.4958\n",
      "Epoch 234/450\n",
      "9021/9021 [==============================] - 0s 34us/sample - loss: 1.5784 - mae: 1.0248 - mse: 1.5784 - val_loss: 1.5241 - val_mae: 0.9929 - val_mse: 1.5241\n",
      "Epoch 235/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5782 - mae: 1.0254 - mse: 1.5782 - val_loss: 1.5027 - val_mae: 0.9987 - val_mse: 1.5027\n",
      "Epoch 236/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5769 - mae: 1.0237 - mse: 1.5769 - val_loss: 1.4987 - val_mae: 1.0117 - val_mse: 1.4987\n",
      "Epoch 237/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5769 - mae: 1.0256 - mse: 1.5769 - val_loss: 1.4989 - val_mae: 1.0004 - val_mse: 1.4989\n",
      "Epoch 238/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5776 - mae: 1.0260 - mse: 1.5776 - val_loss: 1.5036 - val_mae: 0.9984 - val_mse: 1.5036\n",
      "Epoch 239/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5802 - mae: 1.0265 - mse: 1.5802 - val_loss: 1.5035 - val_mae: 0.9984 - val_mse: 1.5035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5799 - mae: 1.0251 - mse: 1.5799 - val_loss: 1.4999 - val_mae: 0.9999 - val_mse: 1.4999\n",
      "Epoch 241/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5806 - mae: 1.0261 - mse: 1.5806 - val_loss: 1.4943 - val_mae: 1.0034 - val_mse: 1.4943\n",
      "Epoch 242/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5773 - mae: 1.0257 - mse: 1.5773 - val_loss: 1.4947 - val_mae: 1.0030 - val_mse: 1.4947\n",
      "Epoch 243/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5793 - mae: 1.0254 - mse: 1.5793 - val_loss: 1.4953 - val_mae: 1.0096 - val_mse: 1.4953\n",
      "Epoch 244/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5765 - mae: 1.0239 - mse: 1.5765 - val_loss: 1.4993 - val_mae: 1.0120 - val_mse: 1.4993\n",
      "Epoch 245/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5794 - mae: 1.0266 - mse: 1.5794 - val_loss: 1.5215 - val_mae: 0.9935 - val_mse: 1.5215\n",
      "Epoch 246/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5792 - mae: 1.0248 - mse: 1.5792 - val_loss: 1.5039 - val_mae: 0.9983 - val_mse: 1.5039\n",
      "Epoch 247/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5788 - mae: 1.0259 - mse: 1.5788 - val_loss: 1.4931 - val_mae: 1.0069 - val_mse: 1.4931\n",
      "Epoch 248/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5789 - mae: 1.0251 - mse: 1.5789 - val_loss: 1.4997 - val_mae: 1.0122 - val_mse: 1.4997\n",
      "Epoch 249/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5787 - mae: 1.0261 - mse: 1.5787 - val_loss: 1.4937 - val_mae: 1.0042 - val_mse: 1.4937\n",
      "Epoch 250/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5800 - mae: 1.0252 - mse: 1.5800 - val_loss: 1.4931 - val_mae: 1.0066 - val_mse: 1.4931\n",
      "Epoch 251/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5784 - mae: 1.0250 - mse: 1.5784 - val_loss: 1.4930 - val_mae: 1.0060 - val_mse: 1.4930\n",
      "Epoch 252/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5784 - mae: 1.0265 - mse: 1.5784 - val_loss: 1.4954 - val_mae: 1.0097 - val_mse: 1.4954\n",
      "Epoch 253/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5791 - mae: 1.0264 - mse: 1.5791 - val_loss: 1.4931 - val_mae: 1.0066 - val_mse: 1.4931\n",
      "Epoch 254/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5799 - mae: 1.0266 - mse: 1.5799 - val_loss: 1.4953 - val_mae: 1.0025 - val_mse: 1.4953\n",
      "Epoch 255/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5801 - mae: 1.0272 - mse: 1.5801 - val_loss: 1.4999 - val_mae: 0.9999 - val_mse: 1.4999\n",
      "Epoch 256/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5781 - mae: 1.0255 - mse: 1.5781 - val_loss: 1.5114 - val_mae: 1.0162 - val_mse: 1.5114\n",
      "Epoch 257/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5812 - mae: 1.0266 - mse: 1.5812 - val_loss: 1.5010 - val_mae: 0.9994 - val_mse: 1.5010\n",
      "Epoch 258/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5791 - mae: 1.0257 - mse: 1.5791 - val_loss: 1.5015 - val_mae: 0.9992 - val_mse: 1.5015\n",
      "Epoch 259/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5787 - mae: 1.0268 - mse: 1.5787 - val_loss: 1.5001 - val_mae: 0.9998 - val_mse: 1.5001\n",
      "Epoch 260/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5789 - mae: 1.0254 - mse: 1.5789 - val_loss: 1.5134 - val_mae: 0.9954 - val_mse: 1.5134\n",
      "Epoch 261/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5781 - mae: 1.0253 - mse: 1.5781 - val_loss: 1.4946 - val_mae: 1.0090 - val_mse: 1.4946\n",
      "Epoch 262/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5765 - mae: 1.0257 - mse: 1.5765 - val_loss: 1.5097 - val_mae: 0.9965 - val_mse: 1.5097\n",
      "Epoch 263/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5801 - mae: 1.0263 - mse: 1.5801 - val_loss: 1.5164 - val_mae: 1.0174 - val_mse: 1.5164\n",
      "Epoch 264/450\n",
      "9021/9021 [==============================] - 0s 34us/sample - loss: 1.5796 - mae: 1.0259 - mse: 1.5796 - val_loss: 1.5023 - val_mae: 0.9989 - val_mse: 1.5023\n",
      "Epoch 265/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5785 - mae: 1.0264 - mse: 1.5785 - val_loss: 1.4985 - val_mae: 1.0006 - val_mse: 1.4985\n",
      "Epoch 266/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5790 - mae: 1.0254 - mse: 1.5790 - val_loss: 1.4940 - val_mae: 1.0084 - val_mse: 1.4940\n",
      "Epoch 267/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5791 - mae: 1.0272 - mse: 1.5791 - val_loss: 1.4935 - val_mae: 1.0044 - val_mse: 1.4935\n",
      "Epoch 268/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5775 - mae: 1.0255 - mse: 1.5775 - val_loss: 1.4933 - val_mae: 1.0049 - val_mse: 1.4933\n",
      "Epoch 269/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5791 - mae: 1.0262 - mse: 1.5791 - val_loss: 1.4951 - val_mae: 1.0027 - val_mse: 1.4951\n",
      "Epoch 270/450\n",
      "9021/9021 [==============================] - 0s 54us/sample - loss: 1.5789 - mae: 1.0262 - mse: 1.5789 - val_loss: 1.4961 - val_mae: 1.0020 - val_mse: 1.4961\n",
      "Epoch 271/450\n",
      "9021/9021 [==============================] - ETA: 0s - loss: 1.5789 - mae: 1.0264 - mse: 1.578 - 0s 40us/sample - loss: 1.5777 - mae: 1.0253 - mse: 1.5777 - val_loss: 1.5249 - val_mae: 1.0194 - val_mse: 1.5249\n",
      "Epoch 272/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5789 - mae: 1.0267 - mse: 1.5789 - val_loss: 1.5025 - val_mae: 0.9988 - val_mse: 1.5025\n",
      "Epoch 273/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5794 - mae: 1.0259 - mse: 1.5794 - val_loss: 1.4931 - val_mae: 1.0052 - val_mse: 1.4931\n",
      "Epoch 274/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5785 - mae: 1.0263 - mse: 1.5785 - val_loss: 1.5034 - val_mae: 0.9985 - val_mse: 1.5034\n",
      "Epoch 275/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5792 - mae: 1.0245 - mse: 1.5792 - val_loss: 1.5178 - val_mae: 1.0178 - val_mse: 1.5178\n",
      "Epoch 276/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5804 - mae: 1.0270 - mse: 1.5804 - val_loss: 1.4959 - val_mae: 1.0101 - val_mse: 1.4959\n",
      "Epoch 277/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5758 - mae: 1.0249 - mse: 1.5758 - val_loss: 1.5274 - val_mae: 0.9923 - val_mse: 1.5274\n",
      "Epoch 278/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5784 - mae: 1.0261 - mse: 1.5784 - val_loss: 1.4934 - val_mae: 1.0046 - val_mse: 1.4934\n",
      "Epoch 279/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5783 - mae: 1.0261 - mse: 1.5783 - val_loss: 1.5493 - val_mae: 0.9884 - val_mse: 1.5493\n",
      "Epoch 280/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5769 - mae: 1.0242 - mse: 1.5769 - val_loss: 1.5309 - val_mae: 0.9916 - val_mse: 1.5309\n",
      "Epoch 281/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5783 - mae: 1.0244 - mse: 1.5783 - val_loss: 1.5246 - val_mae: 1.0193 - val_mse: 1.5246\n",
      "Epoch 282/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5786 - mae: 1.0246 - mse: 1.5786 - val_loss: 1.5129 - val_mae: 0.9956 - val_mse: 1.5129\n",
      "Epoch 283/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5801 - mae: 1.0256 - mse: 1.5801 - val_loss: 1.4951 - val_mae: 1.0095 - val_mse: 1.4951\n",
      "Epoch 284/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5805 - mae: 1.0272 - mse: 1.5805 - val_loss: 1.4972 - val_mae: 1.0013 - val_mse: 1.4972\n",
      "Epoch 285/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5757 - mae: 1.0246 - mse: 1.5757 - val_loss: 1.5332 - val_mae: 0.9911 - val_mse: 1.5332\n",
      "Epoch 286/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5787 - mae: 1.0257 - mse: 1.5787 - val_loss: 1.5096 - val_mae: 0.9965 - val_mse: 1.5096\n",
      "Epoch 287/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5794 - mae: 1.0256 - mse: 1.5794 - val_loss: 1.4993 - val_mae: 1.0120 - val_mse: 1.4993\n",
      "Epoch 288/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5769 - mae: 1.0252 - mse: 1.5769 - val_loss: 1.5003 - val_mae: 1.0124 - val_mse: 1.5003\n",
      "Epoch 289/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5829 - mae: 1.0269 - mse: 1.5829 - val_loss: 1.4982 - val_mae: 1.0114 - val_mse: 1.4982\n",
      "Epoch 290/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5780 - mae: 1.0265 - mse: 1.5780 - val_loss: 1.5580 - val_mae: 0.9871 - val_mse: 1.5580\n",
      "Epoch 291/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5774 - mae: 1.0244 - mse: 1.5774 - val_loss: 1.5427 - val_mae: 0.9895 - val_mse: 1.5427\n",
      "Epoch 292/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5785 - mae: 1.0253 - mse: 1.5785 - val_loss: 1.4930 - val_mae: 1.0061 - val_mse: 1.4930\n",
      "Epoch 293/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5784 - mae: 1.0257 - mse: 1.5784 - val_loss: 1.5336 - val_mae: 1.0211 - val_mse: 1.5336\n",
      "Epoch 294/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5803 - mae: 1.0259 - mse: 1.5803 - val_loss: 1.4973 - val_mae: 1.0109 - val_mse: 1.4973\n",
      "Epoch 295/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5786 - mae: 1.0260 - mse: 1.5786 - val_loss: 1.5042 - val_mae: 1.0139 - val_mse: 1.5042\n",
      "Epoch 296/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5763 - mae: 1.0237 - mse: 1.5763 - val_loss: 1.4938 - val_mae: 1.0081 - val_mse: 1.4938\n",
      "Epoch 297/450\n",
      "9021/9021 [==============================] - 1s 75us/sample - loss: 1.5790 - mae: 1.0254 - mse: 1.5790 - val_loss: 1.4979 - val_mae: 1.0113 - val_mse: 1.4979\n",
      "Epoch 298/450\n",
      "9021/9021 [==============================] - 1s 70us/sample - loss: 1.5790 - mae: 1.0263 - mse: 1.5790 - val_loss: 1.4987 - val_mae: 1.0117 - val_mse: 1.4987\n",
      "Epoch 299/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5797 - mae: 1.0261 - mse: 1.5797 - val_loss: 1.4934 - val_mae: 1.0076 - val_mse: 1.4934\n",
      "Epoch 300/450\n",
      "9021/9021 [==============================] - 1s 57us/sample - loss: 1.5792 - mae: 1.0257 - mse: 1.5792 - val_loss: 1.5088 - val_mae: 0.9967 - val_mse: 1.5088\n",
      "Epoch 301/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5785 - mae: 1.0258 - mse: 1.5785 - val_loss: 1.5238 - val_mae: 0.9930 - val_mse: 1.5238\n",
      "Epoch 302/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5803 - mae: 1.0270 - mse: 1.5803 - val_loss: 1.5046 - val_mae: 0.9981 - val_mse: 1.5046\n",
      "Epoch 303/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5776 - mae: 1.0251 - mse: 1.5776 - val_loss: 1.5000 - val_mae: 0.9998 - val_mse: 1.5000\n",
      "Epoch 304/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5824 - mae: 1.0269 - mse: 1.5824 - val_loss: 1.4954 - val_mae: 1.0024 - val_mse: 1.4954\n",
      "Epoch 305/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5798 - mae: 1.0264 - mse: 1.5798 - val_loss: 1.5038 - val_mae: 1.0138 - val_mse: 1.5038\n",
      "Epoch 306/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5773 - mae: 1.0261 - mse: 1.5773 - val_loss: 1.4948 - val_mae: 1.0092 - val_mse: 1.4948\n",
      "Epoch 307/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5763 - mae: 1.0253 - mse: 1.5763 - val_loss: 1.5062 - val_mae: 0.9975 - val_mse: 1.5062\n",
      "Epoch 308/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5792 - mae: 1.0262 - mse: 1.5792 - val_loss: 1.4945 - val_mae: 1.0032 - val_mse: 1.4945\n",
      "Epoch 309/450\n",
      "9021/9021 [==============================] - 1s 69us/sample - loss: 1.5779 - mae: 1.0262 - mse: 1.5779 - val_loss: 1.5093 - val_mae: 0.9966 - val_mse: 1.5093\n",
      "Epoch 310/450\n",
      "9021/9021 [==============================] - 1s 61us/sample - loss: 1.5796 - mae: 1.0267 - mse: 1.5796 - val_loss: 1.4947 - val_mae: 1.0031 - val_mse: 1.4947\n",
      "Epoch 311/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5768 - mae: 1.0222 - mse: 1.5768 - val_loss: 1.5716 - val_mae: 1.0269 - val_mse: 1.5716\n",
      "Epoch 312/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5806 - mae: 1.0272 - mse: 1.5806 - val_loss: 1.5056 - val_mae: 1.0144 - val_mse: 1.5056\n",
      "Epoch 313/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5773 - mae: 1.0244 - mse: 1.5773 - val_loss: 1.4972 - val_mae: 1.0109 - val_mse: 1.4972\n",
      "Epoch 314/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5784 - mae: 1.0250 - mse: 1.5784 - val_loss: 1.4936 - val_mae: 1.0079 - val_mse: 1.4936\n",
      "Epoch 315/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5793 - mae: 1.0268 - mse: 1.5793 - val_loss: 1.5077 - val_mae: 0.9971 - val_mse: 1.5077\n",
      "Epoch 316/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5779 - mae: 1.0250 - mse: 1.5779 - val_loss: 1.4969 - val_mae: 1.0014 - val_mse: 1.4969\n",
      "Epoch 317/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5796 - mae: 1.0272 - mse: 1.5796 - val_loss: 1.5019 - val_mae: 1.0131 - val_mse: 1.5019\n",
      "Epoch 318/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5779 - mae: 1.0250 - mse: 1.5779 - val_loss: 1.5012 - val_mae: 0.9993 - val_mse: 1.5012\n",
      "Epoch 319/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5782 - mae: 1.0260 - mse: 1.5782 - val_loss: 1.5628 - val_mae: 0.9864 - val_mse: 1.5628\n",
      "Epoch 320/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5807 - mae: 1.0264 - mse: 1.5807 - val_loss: 1.5556 - val_mae: 0.9874 - val_mse: 1.5556\n",
      "Epoch 321/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5821 - mae: 1.0263 - mse: 1.5821 - val_loss: 1.5147 - val_mae: 1.0170 - val_mse: 1.5147\n",
      "Epoch 322/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5796 - mae: 1.0265 - mse: 1.5796 - val_loss: 1.5674 - val_mae: 0.9858 - val_mse: 1.5674\n",
      "Epoch 323/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5831 - mae: 1.0257 - mse: 1.5831 - val_loss: 1.4932 - val_mae: 1.0051 - val_mse: 1.4932\n",
      "Epoch 324/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5774 - mae: 1.0243 - mse: 1.5774 - val_loss: 1.4947 - val_mae: 1.0030 - val_mse: 1.4947\n",
      "Epoch 325/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5806 - mae: 1.0270 - mse: 1.5806 - val_loss: 1.5216 - val_mae: 0.9935 - val_mse: 1.5216\n",
      "Epoch 326/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5804 - mae: 1.0261 - mse: 1.5804 - val_loss: 1.4963 - val_mae: 1.0018 - val_mse: 1.4963\n",
      "Epoch 327/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5782 - mae: 1.0269 - mse: 1.5782 - val_loss: 1.5105 - val_mae: 0.9962 - val_mse: 1.5105\n",
      "Epoch 328/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5798 - mae: 1.0262 - mse: 1.5798 - val_loss: 1.5016 - val_mae: 1.0130 - val_mse: 1.5016\n",
      "Epoch 329/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5780 - mae: 1.0248 - mse: 1.5780 - val_loss: 1.4947 - val_mae: 1.0030 - val_mse: 1.4947\n",
      "Epoch 330/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5794 - mae: 1.0255 - mse: 1.5794 - val_loss: 1.5516 - val_mae: 1.0241 - val_mse: 1.5516\n",
      "Epoch 331/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5814 - mae: 1.0271 - mse: 1.5814 - val_loss: 1.5277 - val_mae: 0.9922 - val_mse: 1.5277\n",
      "Epoch 332/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5786 - mae: 1.0250 - mse: 1.5786 - val_loss: 1.4940 - val_mae: 1.0084 - val_mse: 1.4940\n",
      "Epoch 333/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5772 - mae: 1.0257 - mse: 1.5772 - val_loss: 1.4947 - val_mae: 1.0091 - val_mse: 1.4947\n",
      "Epoch 334/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5789 - mae: 1.0258 - mse: 1.5789 - val_loss: 1.4931 - val_mae: 1.0067 - val_mse: 1.4931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5803 - mae: 1.0258 - mse: 1.5803 - val_loss: 1.4960 - val_mae: 1.0101 - val_mse: 1.4960\n",
      "Epoch 336/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5778 - mae: 1.0261 - mse: 1.5778 - val_loss: 1.5124 - val_mae: 1.0164 - val_mse: 1.5124\n",
      "Epoch 337/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5780 - mae: 1.0263 - mse: 1.5780 - val_loss: 1.5204 - val_mae: 0.9938 - val_mse: 1.5204\n",
      "Epoch 338/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5783 - mae: 1.0250 - mse: 1.5783 - val_loss: 1.4979 - val_mae: 1.0113 - val_mse: 1.4979\n",
      "Epoch 339/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5804 - mae: 1.0262 - mse: 1.5804 - val_loss: 1.5024 - val_mae: 1.0133 - val_mse: 1.5024\n",
      "Epoch 340/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5768 - mae: 1.0256 - mse: 1.5768 - val_loss: 1.4992 - val_mae: 1.0119 - val_mse: 1.4992\n",
      "Epoch 341/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5807 - mae: 1.0258 - mse: 1.5807 - val_loss: 1.5015 - val_mae: 1.0129 - val_mse: 1.5015\n",
      "Epoch 342/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5767 - mae: 1.0242 - mse: 1.5767 - val_loss: 1.5087 - val_mae: 1.0154 - val_mse: 1.5087\n",
      "Epoch 343/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5779 - mae: 1.0250 - mse: 1.5779 - val_loss: 1.4981 - val_mae: 1.0114 - val_mse: 1.4981\n",
      "Epoch 344/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5789 - mae: 1.0269 - mse: 1.5789 - val_loss: 1.5343 - val_mae: 0.9909 - val_mse: 1.5343\n",
      "Epoch 345/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5783 - mae: 1.0261 - mse: 1.5783 - val_loss: 1.5062 - val_mae: 1.0146 - val_mse: 1.5062\n",
      "Epoch 346/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5770 - mae: 1.0245 - mse: 1.5770 - val_loss: 1.5012 - val_mae: 0.9993 - val_mse: 1.5012\n",
      "Epoch 347/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5790 - mae: 1.0254 - mse: 1.5790 - val_loss: 1.4944 - val_mae: 1.0033 - val_mse: 1.4944\n",
      "Epoch 348/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5780 - mae: 1.0260 - mse: 1.5780 - val_loss: 1.4973 - val_mae: 1.0012 - val_mse: 1.4973\n",
      "Epoch 349/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5768 - mae: 1.0252 - mse: 1.5768 - val_loss: 1.5143 - val_mae: 0.9952 - val_mse: 1.5143\n",
      "Epoch 350/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5800 - mae: 1.0257 - mse: 1.5800 - val_loss: 1.5071 - val_mae: 0.9972 - val_mse: 1.5071\n",
      "Epoch 351/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5776 - mae: 1.0255 - mse: 1.5776 - val_loss: 1.5038 - val_mae: 1.0138 - val_mse: 1.5038\n",
      "Epoch 352/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5794 - mae: 1.0249 - mse: 1.5794 - val_loss: 1.5083 - val_mae: 0.9969 - val_mse: 1.5083\n",
      "Epoch 353/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5782 - mae: 1.0258 - mse: 1.5782 - val_loss: 1.4968 - val_mae: 1.0106 - val_mse: 1.4968\n",
      "Epoch 354/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5783 - mae: 1.0252 - mse: 1.5783 - val_loss: 1.4978 - val_mae: 1.0009 - val_mse: 1.4978\n",
      "Epoch 355/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5795 - mae: 1.0260 - mse: 1.5795 - val_loss: 1.5054 - val_mae: 0.9978 - val_mse: 1.5054\n",
      "Epoch 356/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5805 - mae: 1.0261 - mse: 1.5805 - val_loss: 1.4951 - val_mae: 1.0094 - val_mse: 1.4951\n",
      "Epoch 357/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5774 - mae: 1.0260 - mse: 1.5774 - val_loss: 1.5118 - val_mae: 0.9959 - val_mse: 1.5118\n",
      "Epoch 358/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5811 - mae: 1.0256 - mse: 1.5811 - val_loss: 1.4974 - val_mae: 1.0011 - val_mse: 1.4974\n",
      "Epoch 359/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5779 - mae: 1.0250 - mse: 1.5779 - val_loss: 1.5286 - val_mae: 1.0201 - val_mse: 1.5286\n",
      "Epoch 360/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5809 - mae: 1.0272 - mse: 1.5809 - val_loss: 1.5174 - val_mae: 0.9944 - val_mse: 1.5174\n",
      "Epoch 361/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5799 - mae: 1.0265 - mse: 1.5799 - val_loss: 1.5018 - val_mae: 1.0131 - val_mse: 1.5018\n",
      "Epoch 362/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5793 - mae: 1.0258 - mse: 1.5793 - val_loss: 1.4994 - val_mae: 1.0120 - val_mse: 1.4994\n",
      "Epoch 363/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5779 - mae: 1.0260 - mse: 1.5779 - val_loss: 1.4955 - val_mae: 1.0097 - val_mse: 1.4955\n",
      "Epoch 364/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5779 - mae: 1.0251 - mse: 1.5779 - val_loss: 1.5223 - val_mae: 1.0188 - val_mse: 1.5223\n",
      "Epoch 365/450\n",
      "9021/9021 [==============================] - 0s 34us/sample - loss: 1.5801 - mae: 1.0261 - mse: 1.5801 - val_loss: 1.5147 - val_mae: 1.0170 - val_mse: 1.5147\n",
      "Epoch 366/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5810 - mae: 1.0270 - mse: 1.5810 - val_loss: 1.5209 - val_mae: 0.9936 - val_mse: 1.5209\n",
      "Epoch 367/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5802 - mae: 1.0266 - mse: 1.5802 - val_loss: 1.5341 - val_mae: 0.9910 - val_mse: 1.5341\n",
      "Epoch 368/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5765 - mae: 1.0232 - mse: 1.5765 - val_loss: 1.4938 - val_mae: 1.0082 - val_mse: 1.4938\n",
      "Epoch 369/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5792 - mae: 1.0246 - mse: 1.5792 - val_loss: 1.5195 - val_mae: 0.9940 - val_mse: 1.5195\n",
      "Epoch 370/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5785 - mae: 1.0258 - mse: 1.5785 - val_loss: 1.4930 - val_mae: 1.0061 - val_mse: 1.4930\n",
      "Epoch 371/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5809 - mae: 1.0263 - mse: 1.5809 - val_loss: 1.5213 - val_mae: 0.9935 - val_mse: 1.5213\n",
      "Epoch 372/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5763 - mae: 1.0235 - mse: 1.5763 - val_loss: 1.4941 - val_mae: 1.0036 - val_mse: 1.4941\n",
      "Epoch 373/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5779 - mae: 1.0246 - mse: 1.5779 - val_loss: 1.5020 - val_mae: 0.9990 - val_mse: 1.5020\n",
      "Epoch 374/450\n",
      "9021/9021 [==============================] - 1s 64us/sample - loss: 1.5804 - mae: 1.0273 - mse: 1.5804 - val_loss: 1.4934 - val_mae: 1.0045 - val_mse: 1.4934\n",
      "Epoch 375/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5776 - mae: 1.0255 - mse: 1.5776 - val_loss: 1.5335 - val_mae: 1.0211 - val_mse: 1.5335\n",
      "Epoch 376/450\n",
      "9021/9021 [==============================] - 0s 51us/sample - loss: 1.5811 - mae: 1.0275 - mse: 1.5811 - val_loss: 1.4948 - val_mae: 1.0092 - val_mse: 1.4948\n",
      "Epoch 377/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5794 - mae: 1.0254 - mse: 1.5794 - val_loss: 1.4969 - val_mae: 1.0014 - val_mse: 1.4969\n",
      "Epoch 378/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5773 - mae: 1.0254 - mse: 1.5773 - val_loss: 1.4971 - val_mae: 1.0013 - val_mse: 1.4971\n",
      "Epoch 379/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5776 - mae: 1.0246 - mse: 1.5776 - val_loss: 1.5178 - val_mae: 1.0178 - val_mse: 1.5178\n",
      "Epoch 380/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5792 - mae: 1.0262 - mse: 1.5792 - val_loss: 1.5020 - val_mae: 1.0131 - val_mse: 1.5020\n",
      "Epoch 381/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5757 - mae: 1.0247 - mse: 1.5757 - val_loss: 1.5078 - val_mae: 1.0151 - val_mse: 1.5078\n",
      "Epoch 382/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9021/9021 [==============================] - 0s 32us/sample - loss: 1.5784 - mae: 1.0264 - mse: 1.5784 - val_loss: 1.5603 - val_mae: 1.0254 - val_mse: 1.5603\n",
      "Epoch 383/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5801 - mae: 1.0274 - mse: 1.5801 - val_loss: 1.5397 - val_mae: 0.9900 - val_mse: 1.5397\n",
      "Epoch 384/450\n",
      "9021/9021 [==============================] - 0s 34us/sample - loss: 1.5809 - mae: 1.0258 - mse: 1.5809 - val_loss: 1.5207 - val_mae: 0.9937 - val_mse: 1.5207\n",
      "Epoch 385/450\n",
      "9021/9021 [==============================] - 0s 54us/sample - loss: 1.5777 - mae: 1.0256 - mse: 1.5777 - val_loss: 1.4932 - val_mae: 1.0072 - val_mse: 1.4932\n",
      "Epoch 386/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5779 - mae: 1.0269 - mse: 1.5779 - val_loss: 1.5012 - val_mae: 0.9993 - val_mse: 1.5012\n",
      "Epoch 387/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5807 - mae: 1.0259 - mse: 1.5807 - val_loss: 1.4994 - val_mae: 1.0001 - val_mse: 1.4994\n",
      "Epoch 388/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5795 - mae: 1.0259 - mse: 1.5795 - val_loss: 1.5067 - val_mae: 0.9973 - val_mse: 1.5067\n",
      "Epoch 389/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5799 - mae: 1.0256 - mse: 1.5799 - val_loss: 1.4932 - val_mae: 1.0072 - val_mse: 1.4932\n",
      "Epoch 390/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5790 - mae: 1.0252 - mse: 1.5790 - val_loss: 1.4951 - val_mae: 1.0027 - val_mse: 1.4951\n",
      "Epoch 391/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5795 - mae: 1.0256 - mse: 1.5795 - val_loss: 1.5166 - val_mae: 0.9946 - val_mse: 1.5166\n",
      "Epoch 392/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5786 - mae: 1.0263 - mse: 1.5786 - val_loss: 1.5647 - val_mae: 0.9861 - val_mse: 1.5647\n",
      "Epoch 393/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5800 - mae: 1.0251 - mse: 1.5800 - val_loss: 1.4978 - val_mae: 1.0112 - val_mse: 1.4978\n",
      "Epoch 394/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5766 - mae: 1.0252 - mse: 1.5766 - val_loss: 1.5183 - val_mae: 1.0179 - val_mse: 1.5183\n",
      "Epoch 395/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5804 - mae: 1.0259 - mse: 1.5804 - val_loss: 1.4966 - val_mae: 1.0016 - val_mse: 1.4966\n",
      "Epoch 396/450\n",
      "9021/9021 [==============================] - 0s 55us/sample - loss: 1.5777 - mae: 1.0248 - mse: 1.5777 - val_loss: 1.5002 - val_mae: 1.0124 - val_mse: 1.5002\n",
      "Epoch 397/450\n",
      "9021/9021 [==============================] - 1s 62us/sample - loss: 1.5784 - mae: 1.0258 - mse: 1.5784 - val_loss: 1.5130 - val_mae: 1.0166 - val_mse: 1.5130\n",
      "Epoch 398/450\n",
      "9021/9021 [==============================] - 1s 58us/sample - loss: 1.5764 - mae: 1.0252 - mse: 1.5764 - val_loss: 1.4993 - val_mae: 1.0119 - val_mse: 1.4993\n",
      "Epoch 399/450\n",
      "9021/9021 [==============================] - 1s 60us/sample - loss: 1.5790 - mae: 1.0271 - mse: 1.5790 - val_loss: 1.4943 - val_mae: 1.0034 - val_mse: 1.4943\n",
      "Epoch 400/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5814 - mae: 1.0263 - mse: 1.5814 - val_loss: 1.4963 - val_mae: 1.0104 - val_mse: 1.4963\n",
      "Epoch 401/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5780 - mae: 1.0254 - mse: 1.5780 - val_loss: 1.4950 - val_mae: 1.0094 - val_mse: 1.4950\n",
      "Epoch 402/450\n",
      "9021/9021 [==============================] - 0s 44us/sample - loss: 1.5785 - mae: 1.0259 - mse: 1.5785 - val_loss: 1.5649 - val_mae: 0.9861 - val_mse: 1.5649\n",
      "Epoch 403/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5793 - mae: 1.0249 - mse: 1.5793 - val_loss: 1.4993 - val_mae: 1.0002 - val_mse: 1.4993\n",
      "Epoch 404/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5795 - mae: 1.0256 - mse: 1.5795 - val_loss: 1.4949 - val_mae: 1.0093 - val_mse: 1.4949\n",
      "Epoch 405/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5783 - mae: 1.0258 - mse: 1.5783 - val_loss: 1.4955 - val_mae: 1.0023 - val_mse: 1.4955\n",
      "Epoch 406/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5779 - mae: 1.0253 - mse: 1.5779 - val_loss: 1.4954 - val_mae: 1.0025 - val_mse: 1.4954\n",
      "Epoch 407/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5809 - mae: 1.0255 - mse: 1.5809 - val_loss: 1.5045 - val_mae: 1.0140 - val_mse: 1.5045\n",
      "Epoch 408/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5792 - mae: 1.0266 - mse: 1.5792 - val_loss: 1.4960 - val_mae: 1.0020 - val_mse: 1.4960\n",
      "Epoch 409/450\n",
      "9021/9021 [==============================] - 0s 29us/sample - loss: 1.5795 - mae: 1.0258 - mse: 1.5795 - val_loss: 1.4930 - val_mae: 1.0064 - val_mse: 1.4930\n",
      "Epoch 410/450\n",
      "9021/9021 [==============================] - 0s 32us/sample - loss: 1.5778 - mae: 1.0256 - mse: 1.5778 - val_loss: 1.4955 - val_mae: 1.0024 - val_mse: 1.4955\n",
      "Epoch 411/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5815 - mae: 1.0266 - mse: 1.5815 - val_loss: 1.5134 - val_mae: 0.9954 - val_mse: 1.5134\n",
      "Epoch 412/450\n",
      "9021/9021 [==============================] - 0s 37us/sample - loss: 1.5771 - mae: 1.0246 - mse: 1.5771 - val_loss: 1.5415 - val_mae: 1.0225 - val_mse: 1.5415\n",
      "Epoch 413/450\n",
      "9021/9021 [==============================] - 0s 50us/sample - loss: 1.5779 - mae: 1.0250 - mse: 1.5779 - val_loss: 1.4940 - val_mae: 1.0084 - val_mse: 1.4940\n",
      "Epoch 414/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5788 - mae: 1.0263 - mse: 1.5788 - val_loss: 1.4997 - val_mae: 1.0000 - val_mse: 1.4997\n",
      "Epoch 415/450\n",
      "9021/9021 [==============================] - 0s 32us/sample - loss: 1.5795 - mae: 1.0254 - mse: 1.5795 - val_loss: 1.5606 - val_mae: 1.0254 - val_mse: 1.5606\n",
      "Epoch 416/450\n",
      "9021/9021 [==============================] - 0s 32us/sample - loss: 1.5788 - mae: 1.0271 - mse: 1.5788 - val_loss: 1.5486 - val_mae: 0.9885 - val_mse: 1.5486\n",
      "Epoch 417/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5804 - mae: 1.0255 - mse: 1.5804 - val_loss: 1.5012 - val_mae: 0.9993 - val_mse: 1.5012\n",
      "Epoch 418/450\n",
      "9021/9021 [==============================] - 0s 34us/sample - loss: 1.5806 - mae: 1.0261 - mse: 1.5806 - val_loss: 1.4958 - val_mae: 1.0100 - val_mse: 1.4958\n",
      "Epoch 419/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5794 - mae: 1.0254 - mse: 1.5794 - val_loss: 1.5031 - val_mae: 0.9986 - val_mse: 1.5031\n",
      "Epoch 420/450\n",
      "9021/9021 [==============================] - 1s 59us/sample - loss: 1.5794 - mae: 1.0252 - mse: 1.5794 - val_loss: 1.4931 - val_mae: 1.0069 - val_mse: 1.4931\n",
      "Epoch 421/450\n",
      "9021/9021 [==============================] - 0s 48us/sample - loss: 1.5781 - mae: 1.0264 - mse: 1.5781 - val_loss: 1.5220 - val_mae: 1.0187 - val_mse: 1.5220\n",
      "Epoch 422/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5779 - mae: 1.0262 - mse: 1.5779 - val_loss: 1.5216 - val_mae: 0.9935 - val_mse: 1.5216\n",
      "Epoch 423/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5775 - mae: 1.0257 - mse: 1.5775 - val_loss: 1.5080 - val_mae: 0.9970 - val_mse: 1.5080\n",
      "Epoch 424/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5810 - mae: 1.0269 - mse: 1.5810 - val_loss: 1.4947 - val_mae: 1.0030 - val_mse: 1.4947\n",
      "Epoch 425/450\n",
      "9021/9021 [==============================] - 1s 57us/sample - loss: 1.5779 - mae: 1.0257 - mse: 1.5779 - val_loss: 1.4933 - val_mae: 1.0048 - val_mse: 1.4933\n",
      "Epoch 426/450\n",
      "9021/9021 [==============================] - 0s 47us/sample - loss: 1.5787 - mae: 1.0258 - mse: 1.5787 - val_loss: 1.5187 - val_mae: 0.9941 - val_mse: 1.5187\n",
      "Epoch 427/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5811 - mae: 1.0259 - mse: 1.5811 - val_loss: 1.5346 - val_mae: 1.0213 - val_mse: 1.5346\n",
      "Epoch 428/450\n",
      "9021/9021 [==============================] - 0s 45us/sample - loss: 1.5806 - mae: 1.0274 - mse: 1.5806 - val_loss: 1.4970 - val_mae: 1.0014 - val_mse: 1.4970\n",
      "Epoch 429/450\n",
      "9021/9021 [==============================] - 0s 30us/sample - loss: 1.5768 - mae: 1.0250 - mse: 1.5768 - val_loss: 1.4960 - val_mae: 1.0020 - val_mse: 1.4960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5764 - mae: 1.0258 - mse: 1.5764 - val_loss: 1.5186 - val_mae: 0.9942 - val_mse: 1.5186\n",
      "Epoch 431/450\n",
      "9021/9021 [==============================] - 0s 31us/sample - loss: 1.5804 - mae: 1.0263 - mse: 1.5804 - val_loss: 1.4997 - val_mae: 1.0000 - val_mse: 1.4997\n",
      "Epoch 432/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5785 - mae: 1.0246 - mse: 1.5785 - val_loss: 1.4992 - val_mae: 1.0119 - val_mse: 1.4992\n",
      "Epoch 433/450\n",
      "9021/9021 [==============================] - 0s 42us/sample - loss: 1.5768 - mae: 1.0253 - mse: 1.5768 - val_loss: 1.5163 - val_mae: 0.9947 - val_mse: 1.5163\n",
      "Epoch 434/450\n",
      "9021/9021 [==============================] - 0s 38us/sample - loss: 1.5796 - mae: 1.0267 - mse: 1.5796 - val_loss: 1.5219 - val_mae: 1.0187 - val_mse: 1.5219\n",
      "Epoch 435/450\n",
      "9021/9021 [==============================] - 0s 35us/sample - loss: 1.5778 - mae: 1.0249 - mse: 1.5778 - val_loss: 1.4948 - val_mae: 1.0093 - val_mse: 1.4948\n",
      "Epoch 436/450\n",
      "9021/9021 [==============================] - 0s 32us/sample - loss: 1.5757 - mae: 1.0250 - mse: 1.5757 - val_loss: 1.5096 - val_mae: 0.9965 - val_mse: 1.5096\n",
      "Epoch 437/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5796 - mae: 1.0266 - mse: 1.5796 - val_loss: 1.5230 - val_mae: 1.0190 - val_mse: 1.5230\n",
      "Epoch 438/450\n",
      "9021/9021 [==============================] - 0s 30us/sample - loss: 1.5787 - mae: 1.0263 - mse: 1.5787 - val_loss: 1.5162 - val_mae: 1.0174 - val_mse: 1.5162\n",
      "Epoch 439/450\n",
      "9021/9021 [==============================] - 0s 36us/sample - loss: 1.5802 - mae: 1.0266 - mse: 1.5802 - val_loss: 1.5445 - val_mae: 0.9892 - val_mse: 1.5445\n",
      "Epoch 440/450\n",
      "9021/9021 [==============================] - 0s 29us/sample - loss: 1.5804 - mae: 1.0267 - mse: 1.5804 - val_loss: 1.4955 - val_mae: 1.0098 - val_mse: 1.4955\n",
      "Epoch 441/450\n",
      "9021/9021 [==============================] - 0s 40us/sample - loss: 1.5783 - mae: 1.0255 - mse: 1.5783 - val_loss: 1.4940 - val_mae: 1.0038 - val_mse: 1.4940\n",
      "Epoch 442/450\n",
      "9021/9021 [==============================] - 0s 46us/sample - loss: 1.5793 - mae: 1.0259 - mse: 1.5793 - val_loss: 1.4934 - val_mae: 1.0075 - val_mse: 1.4934\n",
      "Epoch 443/450\n",
      "9021/9021 [==============================] - 1s 63us/sample - loss: 1.5809 - mae: 1.0272 - mse: 1.5809 - val_loss: 1.5153 - val_mae: 0.9950 - val_mse: 1.5153\n",
      "Epoch 444/450\n",
      "9021/9021 [==============================] - 1s 71us/sample - loss: 1.5776 - mae: 1.0248 - mse: 1.5776 - val_loss: 1.4950 - val_mae: 1.0027 - val_mse: 1.4950\n",
      "Epoch 445/450\n",
      "9021/9021 [==============================] - 0s 39us/sample - loss: 1.5780 - mae: 1.0254 - mse: 1.5780 - val_loss: 1.4930 - val_mae: 1.0062 - val_mse: 1.4930\n",
      "Epoch 446/450\n",
      "9021/9021 [==============================] - 0s 33us/sample - loss: 1.5778 - mae: 1.0260 - mse: 1.5778 - val_loss: 1.4971 - val_mae: 1.0108 - val_mse: 1.4971\n",
      "Epoch 447/450\n",
      "9021/9021 [==============================] - 0s 41us/sample - loss: 1.5768 - mae: 1.0243 - mse: 1.5768 - val_loss: 1.4930 - val_mae: 1.0062 - val_mse: 1.4930\n",
      "Epoch 448/450\n",
      "9021/9021 [==============================] - 0s 49us/sample - loss: 1.5795 - mae: 1.0258 - mse: 1.5795 - val_loss: 1.4943 - val_mae: 1.0088 - val_mse: 1.4943\n",
      "Epoch 449/450\n",
      "9021/9021 [==============================] - 1s 56us/sample - loss: 1.5770 - mae: 1.0255 - mse: 1.5770 - val_loss: 1.4944 - val_mae: 1.0088 - val_mse: 1.4944\n",
      "Epoch 450/450\n",
      "9021/9021 [==============================] - 0s 43us/sample - loss: 1.5764 - mae: 1.0237 - mse: 1.5764 - val_loss: 1.4961 - val_mae: 1.0019 - val_mse: 1.4961\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 450\n",
    "history = ks.fit(X_train,\n",
    "                 y_train,\n",
    "                 epochs = EPOCHS,\n",
    "                 batch_size = 128,\n",
    "                 validation_split = 0.2,\n",
    "                 verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist['mse']\n",
    "epochs = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    plt.plot(feature.tolist(), label.tolist(), c='r')\n",
    "    # Render the scatter plot and the red line.\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.97, mse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = X['all'].copy()\n",
    "label = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_the_model(trained_weight, trained_bias, feature, label)\n",
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.803843</td>\n",
       "      <td>1.031873</td>\n",
       "      <td>2.803842</td>\n",
       "      <td>1.504275</td>\n",
       "      <td>1.005114</td>\n",
       "      <td>1.504275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>25.982278</td>\n",
       "      <td>0.130777</td>\n",
       "      <td>25.982274</td>\n",
       "      <td>0.014562</td>\n",
       "      <td>0.007881</td>\n",
       "      <td>0.014562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.574093</td>\n",
       "      <td>1.022573</td>\n",
       "      <td>1.574093</td>\n",
       "      <td>1.487447</td>\n",
       "      <td>0.985365</td>\n",
       "      <td>1.487447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.577757</td>\n",
       "      <td>1.025135</td>\n",
       "      <td>1.577757</td>\n",
       "      <td>1.494450</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>1.494450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.578756</td>\n",
       "      <td>1.025715</td>\n",
       "      <td>1.578756</td>\n",
       "      <td>1.498291</td>\n",
       "      <td>1.004923</td>\n",
       "      <td>1.498290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.579734</td>\n",
       "      <td>1.026289</td>\n",
       "      <td>1.579734</td>\n",
       "      <td>1.508579</td>\n",
       "      <td>1.010344</td>\n",
       "      <td>1.508579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>552.746356</td>\n",
       "      <td>3.799840</td>\n",
       "      <td>552.746277</td>\n",
       "      <td>1.588293</td>\n",
       "      <td>1.029055</td>\n",
       "      <td>1.588293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             loss         mae         mse    val_loss     val_mae     val_mse\n",
       "count  450.000000  450.000000  450.000000  450.000000  450.000000  450.000000\n",
       "mean     2.803843    1.031873    2.803842    1.504275    1.005114    1.504275\n",
       "std     25.982278    0.130777   25.982274    0.014562    0.007881    0.014562\n",
       "min      1.574093    1.022573    1.574093    1.487447    0.985365    1.487447\n",
       "25%      1.577757    1.025135    1.577757    1.494450    0.999757    1.494450\n",
       "50%      1.578756    1.025715    1.578756    1.498291    1.004923    1.498290\n",
       "75%      1.579734    1.026289    1.579734    1.508579    1.010344    1.508579\n",
       "max    552.746356    3.799840  552.746277    1.588293    1.029055    1.588293"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(a,b):\n",
    "    #print('pred :',a,'actual :',b)\n",
    "    if a == b:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.01645190829802634\n",
      "dt 0.027579228500445585\n",
      "rf 0.06393244945485976\n",
      "vr 0.044768542160292024\n",
      "ks 1.0110543\n"
     ]
    }
   ],
   "source": [
    "print('lr',lr.score(X_train, y_train))\n",
    "print('dt',dt.score(X_train, y_train))\n",
    "print('rf',rf.score(X_train, y_train))\n",
    "print('vr',vr.score(X_train, y_train))\n",
    "ks_test = ks.evaluate(X_train, y_train,verbose=0)\n",
    "print('ks',ks_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_results(result,num):\n",
    "    score = check(result,y_test.loc[num])\n",
    "    return score\n",
    "\n",
    "def predictionTest(num,model):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    result = model.predict([p]).flatten().round()\n",
    "    prediction = print_pred_results(int(result[0]),num)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "numbers = X_test.index\n",
    "random_nums = random.choices(numbers, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vr score:  0.55 \n",
      "dt score:  0.5 \n",
      "rf score:  0.5 \n",
      "ks score:  0.4\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d = [],[],[],[]\n",
    "for i in random_nums:\n",
    "    a.append(predictionTest(i,vr))\n",
    "    b.append(predictionTest(i,dt))\n",
    "    c.append(predictionTest(i,rf))\n",
    "    d.append(predictionTest(i,ks))\n",
    "print('vr score: ',sum(a) / 20,'\\ndt score: ',sum(b) / 20,'\\nrf score: ',sum(c) / 20,'\\nks score: ',sum(d) / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.iloc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    e = e[0]\n",
    "    if e < 1:\n",
    "        e = 0\n",
    "    elif e < 2:\n",
    "        e = 1\n",
    "    return e\n",
    "\n",
    "def model_pred_test(model):\n",
    "    b = []\n",
    "    prob = []\n",
    "    random_nums = np.random.randint(low=1, high=58, size=(20))\n",
    "    for i in random_nums:\n",
    "        prob.append(cycle_prob_test(i,model))\n",
    "    df = pd.DataFrame(prob)\n",
    "    df = df.values\n",
    "    print('scores :\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_score_regressor.sav'\n",
    "pickle.dump(dt, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import cpl_main as cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/cpl_score_regressor.sav'\n",
    "cpl_score_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_pred_test(cpl_classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forge FC Cavalry FC\n"
     ]
    }
   ],
   "source": [
    "# home side\n",
    "q1 = schedule.iloc[0]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[0]['away']\n",
    "print(q1,q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = cpl.get_team_comparison(results_brief,q1,q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game</th>\n",
       "      <th>home</th>\n",
       "      <th>away</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>II1</td>\n",
       "      <td>Forge FC</td>\n",
       "      <td>Cavalry FC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  game      home        away\n",
       "0  II1  Forge FC  Cavalry FC"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "game_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todd/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "home_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "away_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6    7     8     9    10 11 12 13\n",
       "0  0.71  0.89  0.74  0.71  0.33  0.77  0.52  0.5  0.48  0.33  0.21  0  0  0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_roster = get_overall_roster(home_roster)\n",
    "q1_roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3    4     5     6     7     8     9    10 11 12 13\n",
       "0  0.79  0.91  0.75  0.33  0.0  0.66  0.66  0.62  0.37  0.73  0.43  0  0  0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_roster = get_overall_roster(away_roster)\n",
    "q2_roster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roster_regressor_pred(model,array):\n",
    "    prediction = model.predict([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'models/cpl_roster_classifier.sav'\n",
    "cpl_classifier_model = pickle.load(open(classifier, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_win_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_win_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forge FC \n",
      "win probability:  0.35\n"
     ]
    }
   ],
   "source": [
    "print(q1,'\\nwin probability: ', round(home_win_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cavalry FC \n",
      "win probability:  0.35\n"
     ]
    }
   ],
   "source": [
    "print(q2,'\\nwin probability: ', round(away_win_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draw probability:  0.3\n"
     ]
    }
   ],
   "source": [
    "print('Draw probability: ', round(draw_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_score_prediction(model,q1_roster,q2_roster,home_win_new,away_win_new):\n",
    "    \n",
    "    def final_score_fix(home_score,away_score,home_win_new,away_win_new):\n",
    "        if home_win_new > away_win_new: # fix the score prediction - if the probability of home win > away win\n",
    "            home_score = away_score + 1 # change the predicted score to reflect that\n",
    "            return home_score,away_score \n",
    "        elif home_win_new < away_win_new: # else the probability of home win < away win\n",
    "            away_score = home_score + 1 # change the predicted score to reflect that\n",
    "            return home_score,away_score\n",
    "        else:\n",
    "            return home_score,away_score\n",
    "    \n",
    "    def score(num): #improve this later for greater predictions\n",
    "        new_score = int(round(num,0)) # convert the float value to int and round it\n",
    "        return new_score\n",
    "    \n",
    "    q1_pred = roster_pred(model,q1_roster)\n",
    "    q1_s = score(q1_pred.iloc[0][0])\n",
    "    q2_pred = roster_pred(model,q2_roster)\n",
    "    q2_s = score(q2_pred.iloc[0][0])\n",
    "    home_score, away_score = final_score_fix(q1_s, q2_s,home_win_new,away_win_new)\n",
    "    return home_score, away_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roster_pred(model,array):\n",
    "    prediction = model.predict([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    #print('score :',prediction)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_score, away_score = get_final_score_prediction(cpl_score_model,q1_roster,q2_roster,home_win_new,away_win_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "print(home_score, away_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def final_score_fix(home_score,away_score,home_win_new,away_win_new):\n",
    "    if home_win_new > away_win_new:\n",
    "        print('greater')\n",
    "        home_score = away_score + 1\n",
    "        return home_score,away_score \n",
    "    elif home_win_new < away_win_new:\n",
    "        print('less than')\n",
    "        away_score = home_score + 1\n",
    "        return home_score,away_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "away_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
