{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todd McCullough\n",
    "# 2020\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game      team    p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1  \\\n",
      "0   I1  Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48   \n",
      "1   I1  York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70   \n",
      "\n",
      "    p10  p11  p12  p13  r  s  \n",
      "0  0.42  0.0  0.0  0.0  2  1  \n",
      "1  0.47  0.0  0.0  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')\n",
    "print(results.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pump_it_up(results)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "y = db.pop('r')\n",
    "db.pop('s')\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
      "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
      "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
      "\n",
      "   p13  \n",
      "0  0.0  \n",
      "1  0.0  \n"
     ]
    }
   ],
   "source": [
    "print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "\n",
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Neighbors Classifier\n",
    "# algorithm = ‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’\n",
    "def kNeighbors(x,y):\n",
    "    model = KNeighborsClassifier(algorithm = 'auto',\n",
    "                                 leaf_size=30,\n",
    "                                 metric='minkowski',\n",
    "                                 metric_params=None,\n",
    "                                 n_jobs=1,\n",
    "                                 n_neighbors=5,\n",
    "                                 p=2,\n",
    "                                 weights='uniform')\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "knn = kNeighbors(X_train, y_train)\n",
    "\n",
    "#Random Forest Regression\n",
    "def forestRegression_1(x,y):\n",
    "    model = RandomForestClassifier(n_estimators = 200,\n",
    "                                   min_samples_leaf = 5,\n",
    "                                   min_samples_split = 12,\n",
    "                                   random_state = 0,\n",
    "                                   max_depth = 80)\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "rf1 = forestRegression_1(X_train, y_train)\n",
    "\n",
    "# Bagged Decision Trees for Classification - necessary dependencies\n",
    "def baggedTree(x,y,seed):\n",
    "    model = BaggingClassifier(base_estimator=rf1,\n",
    "                              n_estimators=10,\n",
    "                              random_state=seed)\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "bag = baggedTree(X_train, y_train,seed)\n",
    "\n",
    "# AdaBoost Classification\n",
    "def adaBoost(x,y,seed):\n",
    "    model = AdaBoostClassifier(n_estimators=70,\n",
    "                               random_state=seed,\n",
    "                               algorithm='SAMME')\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "ada = adaBoost(X_train, y_train,seed)\n",
    "\n",
    "# Voting Ensemble for Classification\n",
    "def ensembleClassifier(x,y):\n",
    "    # create the sub models\n",
    "    estimators = []\n",
    "    estimators.append(('rf1', rf1))\n",
    "    estimators.append(('knn', knn))\n",
    "    estimators.append(('bag', bag))\n",
    "\n",
    "    # create the ensemble model\n",
    "    model = VotingClassifier(estimators,voting='soft',weights=[0.83,0.79,0.82])\n",
    "    model.fit(x,y)\n",
    "    return model\n",
    "\n",
    "ens = ensembleClassifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(a,b):\n",
    "    if a == b:\n",
    "        result = '<'\n",
    "    else:\n",
    "        result = '-'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf1 0.7340631036703155\n",
      "ens 0.6661300708306503\n",
      "bag 0.6712813908564069\n",
      "knn 0.5222150676110754\n",
      "ada 0.45235028976175146\n"
     ]
    }
   ],
   "source": [
    "print('rf1',rf1.score(X_train, y_train))\n",
    "print('ens',ens.score(X_train, y_train))\n",
    "print('bag',bag.score(X_train, y_train))\n",
    "print('knn',knn.score(X_train, y_train))\n",
    "print('ada',ada.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_results(model,result,num):\n",
    "    print('model : ', result, check(result,y_test.loc[num]))\n",
    "\n",
    "def predictionTest(num,model1,model2,model3):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    result1 = model1.predict([p]).flatten()\n",
    "    result2 = model2.predict([p]).flatten()\n",
    "    result3 = model3.predict([p]).flatten()\n",
    "    print('\\nActual       : ',y_test.loc[num])\n",
    "    print_pred_results(model1,result1,num)\n",
    "    print_pred_results(model2,result2,num)\n",
    "    print_pred_results(model3,result3,num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numbers = X_test.index\n",
    "random_nums = random.choices(numbers, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    e = model.predict_proba([p]).flatten()\n",
    "    return e.tolist()\n",
    "\n",
    "def cycle_pred_test(num,model):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    if e[0] == y_test.loc[num]:\n",
    "        a = 1\n",
    "    else:\n",
    "        a = 0\n",
    "    return a\n",
    "\n",
    "def model_pred_test(model):\n",
    "    pred = []\n",
    "    prob = []\n",
    "    numbers = X_test.index\n",
    "    random_nums = random.choices(numbers, k=50)\n",
    "    for i in random_nums:\n",
    "        pred.append(cycle_pred_test(i,model)) # check to see if the values are correct and score it\n",
    "        #prob.append(cycle_prob_test(i,model))\n",
    "    dz = pd.DataFrame(pred)\n",
    "    #df = pd.DataFrame(prob)\n",
    "    c = str(float(dz.sum().values / 50))\n",
    "    return c\n",
    "    #print('score :',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(rf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(ens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(index=range(10),columns=['knn','ada','bag','rf1','ens'])\n",
    "test_results = test_results.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 1 complete\n",
      "test 2 complete\n",
      "test 3 complete\n",
      "test 4 complete\n",
      "test 5 complete\n",
      "test 6 complete\n",
      "test 7 complete\n",
      "test 8 complete\n",
      "test 9 complete\n",
      "test 10 complete\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    k_s = model_pred_test(knn)\n",
    "    test_results.at[i,'knn'] = k_s\n",
    "    a_s = model_pred_test(ada)\n",
    "    test_results.at[i,'ada'] = a_s\n",
    "    b_s = model_pred_test(bag)\n",
    "    test_results.at[i,'bag'] = b_s\n",
    "    r_s = model_pred_test(rf1)\n",
    "    test_results.at[i,'rf1'] = r_s\n",
    "    e_s = model_pred_test(ens)\n",
    "    test_results.at[i,'ens'] = e_s\n",
    "    print(f'test {i+1} complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>ada</th>\n",
       "      <th>bag</th>\n",
       "      <th>rf1</th>\n",
       "      <th>ens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.344000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074117</td>\n",
       "      <td>0.100133</td>\n",
       "      <td>0.062004</td>\n",
       "      <td>0.070048</td>\n",
       "      <td>0.077172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             knn        ada        bag        rf1        ens\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.344000   0.404000   0.430000   0.448000   0.440000\n",
       "std     0.074117   0.100133   0.062004   0.070048   0.077172\n",
       "min     0.240000   0.260000   0.300000   0.360000   0.300000\n",
       "25%     0.275000   0.345000   0.405000   0.380000   0.395000\n",
       "50%     0.350000   0.400000   0.450000   0.470000   0.460000\n",
       "75%     0.395000   0.470000   0.475000   0.515000   0.480000\n",
       "max     0.460000   0.580000   0.500000   0.520000   0.540000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_roster_classifier.sav'\n",
    "pickle.dump(ens, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_classifier_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "York9 FC Valour FC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todd/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2     3     4     5    6     7     8    9   10 11 12 13\n",
      "0  0.93  0.92  0.91  0.86  0.67  0.65  0.6  0.46  0.45  0.0  0.0  0  0  0\n",
      "      0     1     2    3    4     5     6     7     8    9    10 11 12 13\n",
      "0  0.55  0.45  0.21  0.0  0.0  0.41  0.35  0.33  0.16  0.3  0.05  0  0  0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'any'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-489f3cf8b46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mhome_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maway_win\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_match_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt1_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt1_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mhome_win_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maway_win_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_final_game_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpl_classifier_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq1_roster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2_roster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhome_win\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maway_win\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\nwin probability: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_win_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/canpl-es/cpl_main.py\u001b[0m in \u001b[0;36mget_final_game_prediction\u001b[0;34m(model, q1_roster, q2_roster, home_win, away_win, draw)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_final_game_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq1_roster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2_roster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhome_win\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maway_win\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m     \u001b[0mq1_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroster_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq1_roster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0mq1_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0mq2_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroster_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq2_roster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/canpl-es/cpl_main.py\u001b[0m in \u001b[0;36mroster_pred\u001b[0;34m(model, array)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mroster_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[0;31m#print('score :',prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \"\"\"\n\u001b[1;32m    800\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m                               \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                               \u001b[0mallow_nd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                               dtype=None)\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             ret = check_X_y(X, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'any'"
     ]
    }
   ],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')\n",
    "\n",
    "# home side\n",
    "q1 = schedule.iloc[3]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[3]['away']\n",
    "print(q1,q2)\n",
    "\n",
    "compare = cpl.get_team_comparison(results_brief,q1,q2)\n",
    "\n",
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)\n",
    "\n",
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "\n",
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)\n",
    "\n",
    "h1_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "h2_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "\n",
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db\n",
    "\n",
    "q1_roster = get_overall_roster(h1_roster)\n",
    "print(q1_roster)\n",
    "\n",
    "q2_roster = get_overall_roster(h2_roster)\n",
    "print(q2_roster)\n",
    "\n",
    "def roster_pred(model,array):\n",
    "    prediction = model.predict_proba([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    #print('score :',prediction)\n",
    "    return df\n",
    "\n",
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)\n",
    "\n",
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(q1,'\\nwin probability: ', round(home_win_new,2))\n",
    "\n",
    "print(q2,'\\nwin probability: ', round(away_win_new,2))\n",
    "\n",
    "print('Draw probability: ', round(draw_new,2))\n",
    "\n",
    "q1_prediction = roster_pred(cpl_classifier_model,q1_roster)\n",
    "q2_prediction = roster_pred(cpl_classifier_model,q2_roster)\n",
    "\n",
    "q_draw = (q1_prediction.iloc[1][0] + q2_prediction.iloc[2][0]) /2\n",
    "\n",
    "q1_p = round(q1_prediction.iloc[2][0],2)\n",
    "q2_p = round(q2_prediction.iloc[2][0],2)\n",
    "\n",
    "if q1_p > q2_p:\n",
    "    print(q1,'predicted to win ',q1_p)\n",
    "elif q2_p > q1_p:\n",
    "    print(q2,'predicted to win ',q2_p)\n",
    "else:\n",
    "    print('A Draw is predicted ',q1_p,' ',q2_p)\n",
    "\n",
    "total_ = q1_p + home_win + q2_p + away_win + q_draw + draw\n",
    "print(round((q1_p + home_win) / total_, 2))\n",
    "\n",
    "print(round((q2_p + away_win) / total_, 2))\n",
    "\n",
    "print(round((q_draw + draw) / total_, 2))\n",
    "\n",
    "def get_final_game_prediction(model,q1_roster,q2_roster,home_win,away_win,draw):\n",
    "    q1_prediction = roster_pred(model,q1_roster)\n",
    "    q1_p = round(q1_prediction.iloc[2][0],2)\n",
    "    q2_prediction = roster_pred(model,q2_roster)\n",
    "    q2_p = round(q2_prediction.iloc[2][0],2)\n",
    "    q_draw = (q1_prediction.iloc[1][0] + q2_prediction.iloc[2][0]) / 2\n",
    "    total_ = q1_p + home_win + q2_p + away_win + q_draw + draw\n",
    "    h_w = round((q1_p + home_win) / total_, 2)\n",
    "    a_w = round((q2_p + away_win) / total_, 2)\n",
    "    g_d = round((q_draw + draw) / total_, 2)\n",
    "    return h_w, a_w, g_d\n",
    "\n",
    "home_win_new, away_win_new, draw_new = get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print('home win prob',home_win_new,'away win prob',away_win_new,'draw prob',draw_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
