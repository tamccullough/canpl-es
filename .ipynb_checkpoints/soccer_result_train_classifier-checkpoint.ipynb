{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todd McCullough\n",
    "# 2020\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game      team    p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1  \\\n",
      "0   I1  Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48   \n",
      "1   I1  York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70   \n",
      "\n",
      "    p10  p11  p12  p13  r  s  \n",
      "0  0.42  0.0  0.0  0.0  2  1  \n",
      "1  0.47  0.0  0.0  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')\n",
    "\n",
    "print(results.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11649, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pump_it_up(results)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11649, 18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "y = db.pop('r')\n",
    "db.pop('s')\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
      "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
      "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
      "\n",
      "   p13   all  \n",
      "0  0.0  0.55  \n",
      "1  0.0  0.59  \n"
     ]
    }
   ],
   "source": [
    "print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "\n",
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Neighbors Classifier\n",
    "# algorithm = ‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’\n",
    "def kNeighbors(x,y):\n",
    "    model = KNeighborsClassifier(algorithm = 'auto',\n",
    "                                 leaf_size=30,\n",
    "                                 metric='minkowski',\n",
    "                                 metric_params=None,\n",
    "                                 n_jobs=1,\n",
    "                                 n_neighbors=5,\n",
    "                                 p=2,\n",
    "                                 weights='uniform')\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "knn = kNeighbors(X_train, y_train)\n",
    "\n",
    "#Random Forest Regression\n",
    "def forestRegression_1(x,y):\n",
    "    model = RandomForestClassifier(n_estimators = 100,\n",
    "                                   min_samples_leaf = 5,\n",
    "                                   min_samples_split = 12,\n",
    "                                   random_state = 0,\n",
    "                                   max_depth = 80)\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "rf1 = forestRegression_1(X_train, y_train)\n",
    "\n",
    "# Bagged Decision Trees for Classification - necessary dependencies\n",
    "def baggedTree(x,y,seed):\n",
    "    model = BaggingClassifier(base_estimator=rf1,\n",
    "                              n_estimators=10,\n",
    "                              random_state=seed)\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "bag = baggedTree(X_train, y_train,seed)\n",
    "\n",
    "# AdaBoost Classification\n",
    "def adaBoost(x,y,seed):\n",
    "    model = AdaBoostClassifier(n_estimators=70,\n",
    "                               random_state=seed,\n",
    "                               algorithm='SAMME')\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "ada = adaBoost(X_train, y_train,seed)\n",
    "\n",
    "# Voting Ensemble for Classification\n",
    "def ensembleClassifier(x,y):\n",
    "    # create the sub models\n",
    "    estimators = []\n",
    "    estimators.append(('rf1', rf1))\n",
    "    estimators.append(('knn', knn))\n",
    "    estimators.append(('bag', bag))\n",
    "\n",
    "    # create the ensemble model\n",
    "    model = VotingClassifier(estimators,voting='soft',weights=[0.83,0.79,0.82])\n",
    "    model.fit(x,y)\n",
    "    return model\n",
    "\n",
    "ens = ensembleClassifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf1 0.7346087809663968\n",
      "ens 0.6672798626441011\n",
      "bag 0.6727986264410105\n",
      "knn 0.5252636742702967\n",
      "ada 0.43512386558744176\n"
     ]
    }
   ],
   "source": [
    "def check(a,b):\n",
    "    if a == b:\n",
    "        result = '<'\n",
    "    else:\n",
    "        result = '-'\n",
    "    return result\n",
    "\n",
    "print('rf1',rf1.score(X_train, y_train))\n",
    "print('ens',ens.score(X_train, y_train))\n",
    "print('bag',bag.score(X_train, y_train))\n",
    "print('knn',knn.score(X_train, y_train))\n",
    "print('ada',ada.score(X_train, y_train))\n",
    "\n",
    "def print_pred_results(model,result,num):\n",
    "    print('model : ', result, check(result,y_test.loc[num]))\n",
    "\n",
    "def predictionTest(num,model1,model2,model3):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    result1 = model1.predict([p]).flatten()\n",
    "    result2 = model2.predict([p]).flatten()\n",
    "    result3 = model3.predict([p]).flatten()\n",
    "    print('\\nActual       : ',y_test.loc[num])\n",
    "    print_pred_results(model1,result1,num)\n",
    "    print_pred_results(model2,result2,num)\n",
    "    print_pred_results(model3,result3,num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numbers = X_test.index\n",
    "random_nums = random.choices(numbers, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    e = model.predict_proba([p]).flatten()\n",
    "    return e.tolist()\n",
    "\n",
    "def cycle_pred_test(num,model):\n",
    "    p = X_test.loc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    if e[0] == y_test.loc[num]:\n",
    "        a = 1\n",
    "    else:\n",
    "        a = 0\n",
    "    return a\n",
    "\n",
    "def model_pred_test(model):\n",
    "    pred = []\n",
    "    prob = []\n",
    "    numbers = X_test.index\n",
    "    random_nums = random.choices(numbers, k=50)\n",
    "    for i in random_nums:\n",
    "        pred.append(cycle_pred_test(i,model)) # check to see if the values are correct and score it\n",
    "        #prob.append(cycle_prob_test(i,model))\n",
    "    dz = pd.DataFrame(pred)\n",
    "    #df = pd.DataFrame(prob)\n",
    "    c = str(float(dz.sum().values / 50))\n",
    "    return c\n",
    "    #print('score :',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.36\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.34\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(ada))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.4\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.38\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(rf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score : 0.42\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_pred_test(ens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(index=range(10),columns=['knn','ada','bag','rf1','ens'])\n",
    "test_results = test_results.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    k_s = model_pred_test(knn)\n",
    "    test_results.at[i,'knn'] = k_s\n",
    "    a_s = model_pred_test(ada)\n",
    "    test_results.at[i,'ada'] = a_s\n",
    "    b_s = model_pred_test(bag)\n",
    "    test_results.at[i,'bag'] = b_s\n",
    "    r_s = model_pred_test(rf1)\n",
    "    test_results.at[i,'rf1'] = r_s\n",
    "    e_s = model_pred_test(ens)\n",
    "    test_results.at[i,'ens'] = e_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>ada</th>\n",
       "      <th>bag</th>\n",
       "      <th>rf1</th>\n",
       "      <th>ens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.386000</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.45600</td>\n",
       "      <td>0.45200</td>\n",
       "      <td>0.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.046236</td>\n",
       "      <td>0.050596</td>\n",
       "      <td>0.04402</td>\n",
       "      <td>0.06941</td>\n",
       "      <td>0.088468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.38000</td>\n",
       "      <td>0.34000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.42500</td>\n",
       "      <td>0.39500</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.46000</td>\n",
       "      <td>0.46000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.47500</td>\n",
       "      <td>0.49500</td>\n",
       "      <td>0.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.52000</td>\n",
       "      <td>0.56000</td>\n",
       "      <td>0.540000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             knn        ada       bag       rf1        ens\n",
       "count  10.000000  10.000000  10.00000  10.00000  10.000000\n",
       "mean    0.386000   0.404000   0.45600   0.45200   0.414000\n",
       "std     0.046236   0.050596   0.04402   0.06941   0.088468\n",
       "min     0.280000   0.320000   0.38000   0.34000   0.240000\n",
       "25%     0.380000   0.380000   0.42500   0.39500   0.365000\n",
       "50%     0.380000   0.400000   0.46000   0.46000   0.430000\n",
       "75%     0.415000   0.455000   0.47500   0.49500   0.455000\n",
       "max     0.440000   0.460000   0.52000   0.56000   0.540000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_roster_classifier.sav'\n",
    "pickle.dump(rf1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpl_classifier_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')\n",
    "\n",
    "# home side\n",
    "q1 = schedule.iloc[3]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[3]['away']\n",
    "print(q1,q2)\n",
    "\n",
    "compare = cpl.get_team_comparison(results_brief,q1,q2)\n",
    "\n",
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)\n",
    "\n",
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "\n",
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)\n",
    "\n",
    "h1_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "h2_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "\n",
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db\n",
    "\n",
    "q1_roster = get_overall_roster(h1_roster)\n",
    "print(q1_roster)\n",
    "\n",
    "q2_roster = get_overall_roster(h2_roster)\n",
    "print(q2_roster)\n",
    "\n",
    "def roster_pred(model,array):\n",
    "    prediction = model.predict_proba([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    #print('score :',prediction)\n",
    "    return df\n",
    "\n",
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)\n",
    "\n",
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(q1,'\\nwin probability: ', round(home_win_new,2))\n",
    "\n",
    "print(q2,'\\nwin probability: ', round(away_win_new,2))\n",
    "\n",
    "print('Draw probability: ', round(draw_new,2))\n",
    "\n",
    "q1_prediction = roster_pred(cpl_classifier_model,q1_roster)\n",
    "q2_prediction = roster_pred(cpl_classifier_model,q2_roster)\n",
    "\n",
    "q_draw = (q1_prediction.iloc[1][0] + q2_prediction.iloc[2][0]) /2\n",
    "\n",
    "q1_p = round(q1_prediction.iloc[2][0],2)\n",
    "q2_p = round(q2_prediction.iloc[2][0],2)\n",
    "\n",
    "if q1_p > q2_p:\n",
    "    print(q1,'predicted to win ',q1_p)\n",
    "elif q2_p > q1_p:\n",
    "    print(q2,'predicted to win ',q2_p)\n",
    "else:\n",
    "    print('A Draw is predicted ',q1_p,' ',q2_p)\n",
    "\n",
    "total_ = q1_p + home_win + q2_p + away_win + q_draw + draw\n",
    "print(round((q1_p + home_win) / total_, 2))\n",
    "\n",
    "print(round((q2_p + away_win) / total_, 2))\n",
    "\n",
    "print(round((q_draw + draw) / total_, 2))\n",
    "\n",
    "def get_final_game_prediction(model,q1_roster,q2_roster,home_win,away_win,draw):\n",
    "    q1_prediction = roster_pred(model,q1_roster)\n",
    "    q1_p = round(q1_prediction.iloc[2][0],2)\n",
    "    q2_prediction = roster_pred(model,q2_roster)\n",
    "    q2_p = round(q2_prediction.iloc[2][0],2)\n",
    "    q_draw = (q1_prediction.iloc[1][0] + q2_prediction.iloc[2][0]) / 2\n",
    "    total_ = q1_p + home_win + q2_p + away_win + q_draw + draw\n",
    "    h_w = round((q1_p + home_win) / total_, 2)\n",
    "    a_w = round((q2_p + away_win) / total_, 2)\n",
    "    g_d = round((q_draw + draw) / total_, 2)\n",
    "    return h_w, a_w, g_d\n",
    "\n",
    "home_win_new, away_win_new, draw_new = get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(home_win_new)\n",
    "print(away_win_new)\n",
    "print(draw_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
