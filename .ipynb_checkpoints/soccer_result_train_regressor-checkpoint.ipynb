{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todd McCullough\n",
    "# 2020\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game      team    p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1  \\\n",
      "0   I1  Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48   \n",
      "1   I1  York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70   \n",
      "\n",
      "    p10  p11  p12  p13  r  s  \n",
      "0  0.42  0.0  0.0  0.0  2  1  \n",
      "1  0.47  0.0  0.0  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')\n",
    "print(results.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pump_it_up(results)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "y = db.pop('s')\n",
    "db.pop('r')\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
      "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
      "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
      "\n",
      "   p13  \n",
      "0  0.0  \n",
      "1  0.0  \n"
     ]
    }
   ],
   "source": [
    "print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model\n",
      "\n",
      "RMSE:  1.22284599419792\n",
      "\n",
      "Score 1.72\n",
      "Decision Tree Regression Model\n",
      "\n",
      "RMSE:  1.2781151642415143\n",
      "\n",
      "Score -7.36\n",
      "Random Forest Regression Model\n",
      "\n",
      "RMSE:  1.2241217978700074\n",
      "\n",
      "Score 1.52\n",
      "Voting Regressor Model\n",
      "\n",
      "RMSE:  1.2185526913563272\n",
      "\n",
      "Score 2.41\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model\n",
    "def linearRegression():\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "lr = linearRegression()\n",
    "\n",
    "print('Linear Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n",
    "print('\\nScore',round(lr.score(X_test, y_test)*100,2))\n",
    "\n",
    "#DecisionTreeRegressor\n",
    "def decisionTree():\n",
    "    model = DecisionTreeRegressor(criterion='mse', splitter='random', max_depth=8, max_features='log2')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "dt = decisionTree()\n",
    "\n",
    "print('Decision Tree Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test, dt.predict(X_test))))\n",
    "print('\\nScore',round(dt.score(X_test, y_test)*100,2))\n",
    "\n",
    "#Random Forest Regression\n",
    "def forestRegression():\n",
    "    model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "rf = forestRegression()\n",
    "\n",
    "print('Random Forest Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,rf.predict(X_test))))\n",
    "print('\\nScore',round(rf.score(X_test, y_test)*100,2))\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "vr = VotingRegressor(estimators=[('lr', lr), ('dt', dt), ('rf', rf)])\n",
    "vr = vr.fit(X_train, y_train)\n",
    "\n",
    "print('Voting Regressor Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,vr.predict(X_test))))\n",
    "print('\\nScore',round(vr.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasSequential():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                optimizer = tf.keras.optimizers.RMSprop(0.1),\n",
    "                metrics = ['mae', 'mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,185\n",
      "Trainable params: 5,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ks = kerasSequential()\n",
    "print(ks.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2174 samples, validate on 544 samples\n",
      "Epoch 1/450\n",
      "2174/2174 [==============================] - 1s 616us/sample - loss: 3398.8263 - mae: 15.7236 - mse: 3398.8262 - val_loss: 1.7575 - val_mae: 0.9845 - val_mse: 1.7575\n",
      "Epoch 2/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 2.3010 - mae: 1.1813 - mse: 2.3010 - val_loss: 1.6120 - val_mae: 1.0121 - val_mse: 1.6120\n",
      "Epoch 3/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 2.1775 - mae: 1.1561 - mse: 2.1775 - val_loss: 2.1127 - val_mae: 1.2227 - val_mse: 2.1127\n",
      "Epoch 4/450\n",
      "2174/2174 [==============================] - 0s 70us/sample - loss: 2.0792 - mae: 1.1420 - mse: 2.0792 - val_loss: 2.0876 - val_mae: 1.0752 - val_mse: 2.0876\n",
      "Epoch 5/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.9975 - mae: 1.1164 - mse: 1.9975 - val_loss: 1.7597 - val_mae: 1.0971 - val_mse: 1.7597\n",
      "Epoch 6/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.6316 - mae: 1.0243 - mse: 1.6316 - val_loss: 4.3062 - val_mae: 1.6383 - val_mse: 4.3062\n",
      "Epoch 7/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.7014 - mae: 1.0453 - mse: 1.7014 - val_loss: 2.3516 - val_mae: 1.1509 - val_mse: 2.3516\n",
      "Epoch 8/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.6692 - mae: 1.0379 - mse: 1.6692 - val_loss: 1.6435 - val_mae: 1.0574 - val_mse: 1.6435\n",
      "Epoch 9/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.6009 - mae: 1.0262 - mse: 1.6009 - val_loss: 1.6344 - val_mae: 0.9974 - val_mse: 1.6344\n",
      "Epoch 10/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5723 - mae: 1.0193 - mse: 1.5723 - val_loss: 1.6790 - val_mae: 0.9779 - val_mse: 1.6790\n",
      "Epoch 11/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.6139 - mae: 1.0238 - mse: 1.6139 - val_loss: 1.6546 - val_mae: 1.0695 - val_mse: 1.6546\n",
      "Epoch 12/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5469 - mae: 1.0172 - mse: 1.5469 - val_loss: 1.6241 - val_mae: 1.0221 - val_mse: 1.6241\n",
      "Epoch 13/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5549 - mae: 1.0199 - mse: 1.5549 - val_loss: 1.6504 - val_mae: 1.0677 - val_mse: 1.6504\n",
      "Epoch 14/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5462 - mae: 1.0148 - mse: 1.5462 - val_loss: 1.6182 - val_mae: 1.0132 - val_mse: 1.6182\n",
      "Epoch 15/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5376 - mae: 1.0147 - mse: 1.5376 - val_loss: 1.6473 - val_mae: 0.9890 - val_mse: 1.6473\n",
      "Epoch 16/450\n",
      "2174/2174 [==============================] - 0s 77us/sample - loss: 1.5424 - mae: 1.0167 - mse: 1.5424 - val_loss: 1.6140 - val_mae: 1.0230 - val_mse: 1.6140\n",
      "Epoch 17/450\n",
      "2174/2174 [==============================] - 0s 79us/sample - loss: 1.5280 - mae: 1.0122 - mse: 1.5280 - val_loss: 1.6500 - val_mae: 0.9875 - val_mse: 1.6500\n",
      "Epoch 18/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5260 - mae: 1.0093 - mse: 1.5260 - val_loss: 1.6324 - val_mae: 1.0562 - val_mse: 1.6324\n",
      "Epoch 19/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5428 - mae: 1.0121 - mse: 1.5428 - val_loss: 1.6249 - val_mae: 1.0498 - val_mse: 1.6249\n",
      "Epoch 20/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5301 - mae: 1.0156 - mse: 1.5301 - val_loss: 1.6145 - val_mae: 1.0212 - val_mse: 1.6145\n",
      "Epoch 21/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5334 - mae: 1.0156 - mse: 1.5334 - val_loss: 1.6149 - val_mae: 1.0199 - val_mse: 1.6149\n",
      "Epoch 22/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5351 - mae: 1.0167 - mse: 1.5351 - val_loss: 1.6182 - val_mae: 1.0418 - val_mse: 1.6182\n",
      "Epoch 23/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5287 - mae: 1.0139 - mse: 1.5287 - val_loss: 1.6339 - val_mae: 1.0573 - val_mse: 1.6339\n",
      "Epoch 24/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5355 - mae: 1.0210 - mse: 1.5355 - val_loss: 1.6137 - val_mae: 1.0252 - val_mse: 1.6137\n",
      "Epoch 25/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5280 - mae: 1.0164 - mse: 1.5280 - val_loss: 1.6139 - val_mae: 1.0317 - val_mse: 1.6139\n",
      "Epoch 26/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5261 - mae: 1.0150 - mse: 1.5261 - val_loss: 1.6490 - val_mae: 1.0669 - val_mse: 1.6490\n",
      "Epoch 27/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5332 - mae: 1.0201 - mse: 1.5332 - val_loss: 1.6158 - val_mae: 1.0374 - val_mse: 1.6158\n",
      "Epoch 28/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5314 - mae: 1.0165 - mse: 1.5314 - val_loss: 1.6209 - val_mae: 1.0455 - val_mse: 1.6209\n",
      "Epoch 29/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5309 - mae: 1.0196 - mse: 1.5309 - val_loss: 1.6151 - val_mae: 1.0192 - val_mse: 1.6151\n",
      "Epoch 30/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5280 - mae: 1.0159 - mse: 1.5280 - val_loss: 1.6177 - val_mae: 1.0410 - val_mse: 1.6177\n",
      "Epoch 31/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5314 - mae: 1.0173 - mse: 1.5314 - val_loss: 1.6155 - val_mae: 1.0367 - val_mse: 1.6155\n",
      "Epoch 32/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5288 - mae: 1.0180 - mse: 1.5288 - val_loss: 1.6196 - val_mae: 1.0111 - val_mse: 1.6196\n",
      "Epoch 33/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5275 - mae: 1.0122 - mse: 1.5275 - val_loss: 1.6244 - val_mae: 1.0493 - val_mse: 1.6244\n",
      "Epoch 34/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5248 - mae: 1.0158 - mse: 1.5248 - val_loss: 1.6215 - val_mae: 1.0462 - val_mse: 1.6215\n",
      "Epoch 35/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5257 - mae: 1.0150 - mse: 1.5257 - val_loss: 1.6165 - val_mae: 1.0389 - val_mse: 1.6165\n",
      "Epoch 36/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5318 - mae: 1.0193 - mse: 1.5318 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 37/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5343 - mae: 1.0185 - mse: 1.5343 - val_loss: 1.6143 - val_mae: 1.0333 - val_mse: 1.6143\n",
      "Epoch 38/450\n",
      "2174/2174 [==============================] - 0s 65us/sample - loss: 1.5215 - mae: 1.0133 - mse: 1.5215 - val_loss: 1.6200 - val_mae: 1.0443 - val_mse: 1.6200\n",
      "Epoch 39/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5262 - mae: 1.0159 - mse: 1.5262 - val_loss: 1.6280 - val_mae: 1.0526 - val_mse: 1.6280\n",
      "Epoch 40/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5248 - mae: 1.0143 - mse: 1.5248 - val_loss: 1.6611 - val_mae: 1.0732 - val_mse: 1.6611\n",
      "Epoch 41/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5292 - mae: 1.0178 - mse: 1.5292 - val_loss: 1.6171 - val_mae: 1.0149 - val_mse: 1.6171\n",
      "Epoch 42/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5291 - mae: 1.0130 - mse: 1.5291 - val_loss: 1.6183 - val_mae: 1.0419 - val_mse: 1.6183\n",
      "Epoch 43/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5283 - mae: 1.0183 - mse: 1.5283 - val_loss: 1.6183 - val_mae: 1.0130 - val_mse: 1.6183\n",
      "Epoch 44/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5288 - mae: 1.0134 - mse: 1.5288 - val_loss: 1.6163 - val_mae: 1.0385 - val_mse: 1.6163\n",
      "Epoch 45/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5310 - mae: 1.0174 - mse: 1.5310 - val_loss: 1.6149 - val_mae: 1.0352 - val_mse: 1.6149\n",
      "Epoch 46/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5286 - mae: 1.0154 - mse: 1.5286 - val_loss: 1.6511 - val_mae: 1.0681 - val_mse: 1.6511\n",
      "Epoch 47/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5337 - mae: 1.0195 - mse: 1.5337 - val_loss: 1.6156 - val_mae: 1.0369 - val_mse: 1.6156\n",
      "Epoch 48/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5304 - mae: 1.0168 - mse: 1.5304 - val_loss: 1.6180 - val_mae: 1.0414 - val_mse: 1.6180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5248 - mae: 1.0132 - mse: 1.5248 - val_loss: 1.6538 - val_mae: 1.0695 - val_mse: 1.6538\n",
      "Epoch 50/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5280 - mae: 1.0187 - mse: 1.5280 - val_loss: 1.6168 - val_mae: 1.0394 - val_mse: 1.6168\n",
      "Epoch 51/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5301 - mae: 1.0167 - mse: 1.5301 - val_loss: 1.6148 - val_mae: 1.0201 - val_mse: 1.6148\n",
      "Epoch 52/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5288 - mae: 1.0147 - mse: 1.5288 - val_loss: 1.6269 - val_mae: 1.0517 - val_mse: 1.6269\n",
      "Epoch 53/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5269 - mae: 1.0165 - mse: 1.5269 - val_loss: 1.6145 - val_mae: 1.0211 - val_mse: 1.6145\n",
      "Epoch 54/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5304 - mae: 1.0145 - mse: 1.5304 - val_loss: 1.6266 - val_mae: 1.0514 - val_mse: 1.6266\n",
      "Epoch 55/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5244 - mae: 1.0120 - mse: 1.5244 - val_loss: 1.6691 - val_mae: 1.0768 - val_mse: 1.6691\n",
      "Epoch 56/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5321 - mae: 1.0182 - mse: 1.5321 - val_loss: 1.6137 - val_mae: 1.0245 - val_mse: 1.6137\n",
      "Epoch 57/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5289 - mae: 1.0165 - mse: 1.5289 - val_loss: 1.6142 - val_mae: 1.0222 - val_mse: 1.6142\n",
      "Epoch 58/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5267 - mae: 1.0141 - mse: 1.5267 - val_loss: 1.6503 - val_mae: 1.0676 - val_mse: 1.6503\n",
      "Epoch 59/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5283 - mae: 1.0179 - mse: 1.5283 - val_loss: 1.6199 - val_mae: 1.0108 - val_mse: 1.6199\n",
      "Epoch 60/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5297 - mae: 1.0160 - mse: 1.5297 - val_loss: 1.6168 - val_mae: 1.0395 - val_mse: 1.6168\n",
      "Epoch 61/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5300 - mae: 1.0148 - mse: 1.5300 - val_loss: 1.6228 - val_mae: 1.0476 - val_mse: 1.6228\n",
      "Epoch 62/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5273 - mae: 1.0187 - mse: 1.5273 - val_loss: 1.6199 - val_mae: 1.0108 - val_mse: 1.6199\n",
      "Epoch 63/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5270 - mae: 1.0133 - mse: 1.5270 - val_loss: 1.6177 - val_mae: 1.0409 - val_mse: 1.6177\n",
      "Epoch 64/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5268 - mae: 1.0160 - mse: 1.5268 - val_loss: 1.6161 - val_mae: 1.0168 - val_mse: 1.6161\n",
      "Epoch 65/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5301 - mae: 1.0148 - mse: 1.5301 - val_loss: 1.6140 - val_mae: 1.0319 - val_mse: 1.6140\n",
      "Epoch 66/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5312 - mae: 1.0170 - mse: 1.5312 - val_loss: 1.6149 - val_mae: 1.0353 - val_mse: 1.6149\n",
      "Epoch 67/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5310 - mae: 1.0172 - mse: 1.5310 - val_loss: 1.6256 - val_mae: 1.0505 - val_mse: 1.6256\n",
      "Epoch 68/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5290 - mae: 1.0173 - mse: 1.5290 - val_loss: 1.6422 - val_mae: 1.0630 - val_mse: 1.6422\n",
      "Epoch 69/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5269 - mae: 1.0194 - mse: 1.5269 - val_loss: 1.6282 - val_mae: 1.0021 - val_mse: 1.6282\n",
      "Epoch 70/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5334 - mae: 1.0170 - mse: 1.5334 - val_loss: 1.6136 - val_mae: 1.0261 - val_mse: 1.6136\n",
      "Epoch 71/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5336 - mae: 1.0171 - mse: 1.5336 - val_loss: 1.6135 - val_mae: 1.0280 - val_mse: 1.6135\n",
      "Epoch 72/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5317 - mae: 1.0164 - mse: 1.5317 - val_loss: 1.6239 - val_mae: 1.0488 - val_mse: 1.6239\n",
      "Epoch 73/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5282 - mae: 1.0168 - mse: 1.5282 - val_loss: 1.6159 - val_mae: 1.0375 - val_mse: 1.6159\n",
      "Epoch 74/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5304 - mae: 1.0118 - mse: 1.5304 - val_loss: 1.6400 - val_mae: 1.0615 - val_mse: 1.6400\n",
      "Epoch 75/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5345 - mae: 1.0187 - mse: 1.5345 - val_loss: 1.6183 - val_mae: 1.0419 - val_mse: 1.6183\n",
      "Epoch 76/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5294 - mae: 1.0177 - mse: 1.5294 - val_loss: 1.6192 - val_mae: 1.0432 - val_mse: 1.6192\n",
      "Epoch 77/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5307 - mae: 1.0184 - mse: 1.5307 - val_loss: 1.6137 - val_mae: 1.0301 - val_mse: 1.6137\n",
      "Epoch 78/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5226 - mae: 1.0118 - mse: 1.5226 - val_loss: 1.6142 - val_mae: 1.0220 - val_mse: 1.6142\n",
      "Epoch 79/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5298 - mae: 1.0137 - mse: 1.5298 - val_loss: 1.6264 - val_mae: 1.0513 - val_mse: 1.6264\n",
      "Epoch 80/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5267 - mae: 1.0168 - mse: 1.5267 - val_loss: 1.6154 - val_mae: 1.0365 - val_mse: 1.6154\n",
      "Epoch 81/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5296 - mae: 1.0154 - mse: 1.5296 - val_loss: 1.6143 - val_mae: 1.0331 - val_mse: 1.6143\n",
      "Epoch 82/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5248 - mae: 1.0132 - mse: 1.5248 - val_loss: 1.6174 - val_mae: 1.0405 - val_mse: 1.6174\n",
      "Epoch 83/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5294 - mae: 1.0159 - mse: 1.5294 - val_loss: 1.6136 - val_mae: 1.0259 - val_mse: 1.6136\n",
      "Epoch 84/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5276 - mae: 1.0145 - mse: 1.5276 - val_loss: 1.6294 - val_mae: 1.0538 - val_mse: 1.6294\n",
      "Epoch 85/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5282 - mae: 1.0150 - mse: 1.5282 - val_loss: 1.6216 - val_mae: 1.0463 - val_mse: 1.6216\n",
      "Epoch 86/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5259 - mae: 1.0186 - mse: 1.5259 - val_loss: 1.6219 - val_mae: 1.0083 - val_mse: 1.6219\n",
      "Epoch 87/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5320 - mae: 1.0176 - mse: 1.5320 - val_loss: 1.6182 - val_mae: 1.0417 - val_mse: 1.6182\n",
      "Epoch 88/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5276 - mae: 1.0159 - mse: 1.5276 - val_loss: 1.6162 - val_mae: 1.0384 - val_mse: 1.6162\n",
      "Epoch 89/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5305 - mae: 1.0162 - mse: 1.5305 - val_loss: 1.6451 - val_mae: 1.0647 - val_mse: 1.6451\n",
      "Epoch 90/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5267 - mae: 1.0149 - mse: 1.5267 - val_loss: 1.6202 - val_mae: 1.0446 - val_mse: 1.6202\n",
      "Epoch 91/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5279 - mae: 1.0187 - mse: 1.5279 - val_loss: 1.6153 - val_mae: 1.0188 - val_mse: 1.6153\n",
      "Epoch 92/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5301 - mae: 1.0158 - mse: 1.5301 - val_loss: 1.6212 - val_mae: 1.0458 - val_mse: 1.6212\n",
      "Epoch 93/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5296 - mae: 1.0148 - mse: 1.5296 - val_loss: 1.6201 - val_mae: 1.0445 - val_mse: 1.6201\n",
      "Epoch 94/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5292 - mae: 1.0179 - mse: 1.5292 - val_loss: 1.6169 - val_mae: 1.0396 - val_mse: 1.6169\n",
      "Epoch 95/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5321 - mae: 1.0163 - mse: 1.5321 - val_loss: 1.6211 - val_mae: 1.0457 - val_mse: 1.6211\n",
      "Epoch 96/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5312 - mae: 1.0174 - mse: 1.5312 - val_loss: 1.6151 - val_mae: 1.0358 - val_mse: 1.6151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5233 - mae: 1.0135 - mse: 1.5233 - val_loss: 1.6185 - val_mae: 1.0422 - val_mse: 1.6185\n",
      "Epoch 98/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5339 - mae: 1.0185 - mse: 1.5339 - val_loss: 1.6149 - val_mae: 1.0351 - val_mse: 1.6149\n",
      "Epoch 99/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5309 - mae: 1.0178 - mse: 1.5309 - val_loss: 1.6139 - val_mae: 1.0313 - val_mse: 1.6139\n",
      "Epoch 100/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5276 - mae: 1.0170 - mse: 1.5276 - val_loss: 1.6253 - val_mae: 1.0502 - val_mse: 1.6253\n",
      "Epoch 101/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5327 - mae: 1.0173 - mse: 1.5327 - val_loss: 1.6136 - val_mae: 1.0252 - val_mse: 1.6136\n",
      "Epoch 102/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5307 - mae: 1.0170 - mse: 1.5307 - val_loss: 1.6260 - val_mae: 1.0509 - val_mse: 1.6260\n",
      "Epoch 103/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5326 - mae: 1.0168 - mse: 1.5326 - val_loss: 1.6253 - val_mae: 1.0502 - val_mse: 1.6253\n",
      "Epoch 104/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5298 - mae: 1.0185 - mse: 1.5298 - val_loss: 1.6135 - val_mae: 1.0277 - val_mse: 1.6135\n",
      "Epoch 105/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5322 - mae: 1.0162 - mse: 1.5322 - val_loss: 1.6144 - val_mae: 1.0338 - val_mse: 1.6144\n",
      "Epoch 106/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5259 - mae: 1.0142 - mse: 1.5259 - val_loss: 1.6136 - val_mae: 1.0291 - val_mse: 1.6136\n",
      "Epoch 107/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5250 - mae: 1.0163 - mse: 1.5250 - val_loss: 1.6409 - val_mae: 0.9928 - val_mse: 1.6409\n",
      "Epoch 108/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5328 - mae: 1.0163 - mse: 1.5328 - val_loss: 1.6246 - val_mae: 1.0054 - val_mse: 1.6246\n",
      "Epoch 109/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5275 - mae: 1.0153 - mse: 1.5275 - val_loss: 1.6146 - val_mae: 1.0206 - val_mse: 1.6146\n",
      "Epoch 110/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5263 - mae: 1.0141 - mse: 1.5263 - val_loss: 1.6149 - val_mae: 1.0353 - val_mse: 1.6149\n",
      "Epoch 111/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5329 - mae: 1.0174 - mse: 1.5329 - val_loss: 1.6162 - val_mae: 1.0383 - val_mse: 1.6162\n",
      "Epoch 112/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5276 - mae: 1.0165 - mse: 1.5276 - val_loss: 1.6319 - val_mae: 1.0558 - val_mse: 1.6319\n",
      "Epoch 113/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5356 - mae: 1.0181 - mse: 1.5356 - val_loss: 1.6179 - val_mae: 1.0413 - val_mse: 1.6179\n",
      "Epoch 114/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5263 - mae: 1.0170 - mse: 1.5263 - val_loss: 1.6158 - val_mae: 1.0174 - val_mse: 1.6158\n",
      "Epoch 115/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5325 - mae: 1.0163 - mse: 1.5325 - val_loss: 1.6174 - val_mae: 1.0405 - val_mse: 1.6174\n",
      "Epoch 116/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5282 - mae: 1.0152 - mse: 1.5282 - val_loss: 1.6160 - val_mae: 1.0378 - val_mse: 1.6160\n",
      "Epoch 117/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5250 - mae: 1.0136 - mse: 1.5250 - val_loss: 1.6506 - val_mae: 1.0678 - val_mse: 1.6506\n",
      "Epoch 118/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5329 - mae: 1.0195 - mse: 1.5329 - val_loss: 1.6152 - val_mae: 1.0360 - val_mse: 1.6152\n",
      "Epoch 119/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5330 - mae: 1.0147 - mse: 1.5330 - val_loss: 1.6304 - val_mae: 1.0547 - val_mse: 1.6304\n",
      "Epoch 120/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5279 - mae: 1.0148 - mse: 1.5279 - val_loss: 1.6292 - val_mae: 1.0537 - val_mse: 1.6292\n",
      "Epoch 121/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5279 - mae: 1.0161 - mse: 1.5279 - val_loss: 1.6150 - val_mae: 1.0355 - val_mse: 1.6150\n",
      "Epoch 122/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5268 - mae: 1.0153 - mse: 1.5268 - val_loss: 1.6290 - val_mae: 1.0535 - val_mse: 1.6290\n",
      "Epoch 123/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5310 - mae: 1.0198 - mse: 1.5310 - val_loss: 1.6197 - val_mae: 1.0440 - val_mse: 1.6197\n",
      "Epoch 124/450\n",
      "2174/2174 [==============================] - 0s 109us/sample - loss: 1.5315 - mae: 1.0172 - mse: 1.5315 - val_loss: 1.6185 - val_mae: 1.0422 - val_mse: 1.6185\n",
      "Epoch 125/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5275 - mae: 1.0144 - mse: 1.5275 - val_loss: 1.6135 - val_mae: 1.0278 - val_mse: 1.6135\n",
      "Epoch 126/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5301 - mae: 1.0166 - mse: 1.5301 - val_loss: 1.6150 - val_mae: 1.0196 - val_mse: 1.6150\n",
      "Epoch 127/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5281 - mae: 1.0141 - mse: 1.5281 - val_loss: 1.6183 - val_mae: 1.0419 - val_mse: 1.6183\n",
      "Epoch 128/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5297 - mae: 1.0187 - mse: 1.5297 - val_loss: 1.6144 - val_mae: 1.0335 - val_mse: 1.6144\n",
      "Epoch 129/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5323 - mae: 1.0180 - mse: 1.5323 - val_loss: 1.6145 - val_mae: 1.0208 - val_mse: 1.6145\n",
      "Epoch 130/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5300 - mae: 1.0156 - mse: 1.5300 - val_loss: 1.6301 - val_mae: 1.0544 - val_mse: 1.6301\n",
      "Epoch 131/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5318 - mae: 1.0172 - mse: 1.5318 - val_loss: 1.6369 - val_mae: 1.0595 - val_mse: 1.6369\n",
      "Epoch 132/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5268 - mae: 1.0172 - mse: 1.5268 - val_loss: 1.6147 - val_mae: 1.0345 - val_mse: 1.6147\n",
      "Epoch 133/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5311 - mae: 1.0151 - mse: 1.5311 - val_loss: 1.6192 - val_mae: 1.0433 - val_mse: 1.6192\n",
      "Epoch 134/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5282 - mae: 1.0159 - mse: 1.5282 - val_loss: 1.6137 - val_mae: 1.0248 - val_mse: 1.6137\n",
      "Epoch 135/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5255 - mae: 1.0138 - mse: 1.5255 - val_loss: 1.6179 - val_mae: 1.0413 - val_mse: 1.6179\n",
      "Epoch 136/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5284 - mae: 1.0174 - mse: 1.5284 - val_loss: 1.6205 - val_mae: 1.0449 - val_mse: 1.6205\n",
      "Epoch 137/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5295 - mae: 1.0165 - mse: 1.5295 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 138/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5329 - mae: 1.0166 - mse: 1.5329 - val_loss: 1.6345 - val_mae: 1.0578 - val_mse: 1.6345\n",
      "Epoch 139/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5311 - mae: 1.0182 - mse: 1.5311 - val_loss: 1.6264 - val_mae: 1.0512 - val_mse: 1.6264\n",
      "Epoch 140/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5256 - mae: 1.0160 - mse: 1.5256 - val_loss: 1.6205 - val_mae: 1.0099 - val_mse: 1.6205\n",
      "Epoch 141/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5360 - mae: 1.0181 - mse: 1.5360 - val_loss: 1.6173 - val_mae: 1.0146 - val_mse: 1.6173\n",
      "Epoch 142/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5252 - mae: 1.0135 - mse: 1.5252 - val_loss: 1.6167 - val_mae: 1.0392 - val_mse: 1.6167\n",
      "Epoch 143/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5268 - mae: 1.0147 - mse: 1.5268 - val_loss: 1.6136 - val_mae: 1.0257 - val_mse: 1.6136\n",
      "Epoch 144/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5282 - mae: 1.0148 - mse: 1.5282 - val_loss: 1.6580 - val_mae: 1.0716 - val_mse: 1.6580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5295 - mae: 1.0166 - mse: 1.5295 - val_loss: 1.6232 - val_mae: 1.0481 - val_mse: 1.6232\n",
      "Epoch 146/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5307 - mae: 1.0164 - mse: 1.5307 - val_loss: 1.6202 - val_mae: 1.0446 - val_mse: 1.6202\n",
      "Epoch 147/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5355 - mae: 1.0201 - mse: 1.5355 - val_loss: 1.6178 - val_mae: 1.0412 - val_mse: 1.6178\n",
      "Epoch 148/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5305 - mae: 1.0169 - mse: 1.5305 - val_loss: 1.6221 - val_mae: 1.0469 - val_mse: 1.6221\n",
      "Epoch 149/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5303 - mae: 1.0183 - mse: 1.5303 - val_loss: 1.6149 - val_mae: 1.0352 - val_mse: 1.6149\n",
      "Epoch 150/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5254 - mae: 1.0163 - mse: 1.5254 - val_loss: 1.6146 - val_mae: 1.0343 - val_mse: 1.6146\n",
      "Epoch 151/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5289 - mae: 1.0157 - mse: 1.5289 - val_loss: 1.6276 - val_mae: 1.0523 - val_mse: 1.6276\n",
      "Epoch 152/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5340 - mae: 1.0175 - mse: 1.5340 - val_loss: 1.6153 - val_mae: 1.0363 - val_mse: 1.6153\n",
      "Epoch 153/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5303 - mae: 1.0147 - mse: 1.5303 - val_loss: 1.6192 - val_mae: 1.0432 - val_mse: 1.6192\n",
      "Epoch 154/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5265 - mae: 1.0158 - mse: 1.5265 - val_loss: 1.6165 - val_mae: 1.0161 - val_mse: 1.6165\n",
      "Epoch 155/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5351 - mae: 1.0148 - mse: 1.5351 - val_loss: 1.6310 - val_mae: 1.0552 - val_mse: 1.6310\n",
      "Epoch 156/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5246 - mae: 1.0164 - mse: 1.5246 - val_loss: 1.6159 - val_mae: 1.0377 - val_mse: 1.6159\n",
      "Epoch 157/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5288 - mae: 1.0169 - mse: 1.5288 - val_loss: 1.6159 - val_mae: 1.0376 - val_mse: 1.6159\n",
      "Epoch 158/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5306 - mae: 1.0167 - mse: 1.5306 - val_loss: 1.6144 - val_mae: 1.0335 - val_mse: 1.6144\n",
      "Epoch 159/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5288 - mae: 1.0151 - mse: 1.5288 - val_loss: 1.6204 - val_mae: 1.0449 - val_mse: 1.6204\n",
      "Epoch 160/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5279 - mae: 1.0140 - mse: 1.5279 - val_loss: 1.6462 - val_mae: 1.0653 - val_mse: 1.6462\n",
      "Epoch 161/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5384 - mae: 1.0216 - mse: 1.5384 - val_loss: 1.6137 - val_mae: 1.0252 - val_mse: 1.6137\n",
      "Epoch 162/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5275 - mae: 1.0145 - mse: 1.5275 - val_loss: 1.6168 - val_mae: 1.0394 - val_mse: 1.6168\n",
      "Epoch 163/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5274 - mae: 1.0117 - mse: 1.5274 - val_loss: 1.6173 - val_mae: 1.0404 - val_mse: 1.6173\n",
      "Epoch 164/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5329 - mae: 1.0165 - mse: 1.5329 - val_loss: 1.6251 - val_mae: 1.0500 - val_mse: 1.6251\n",
      "Epoch 165/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5268 - mae: 1.0176 - mse: 1.5268 - val_loss: 1.6136 - val_mae: 1.0287 - val_mse: 1.6136\n",
      "Epoch 166/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5320 - mae: 1.0171 - mse: 1.5320 - val_loss: 1.6136 - val_mae: 1.0265 - val_mse: 1.6136\n",
      "Epoch 167/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5361 - mae: 1.0153 - mse: 1.5361 - val_loss: 1.6308 - val_mae: 1.0550 - val_mse: 1.6308\n",
      "Epoch 168/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5268 - mae: 1.0184 - mse: 1.5268 - val_loss: 1.6241 - val_mae: 1.0060 - val_mse: 1.6241\n",
      "Epoch 169/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5285 - mae: 1.0113 - mse: 1.5285 - val_loss: 1.6186 - val_mae: 1.0423 - val_mse: 1.6186\n",
      "Epoch 170/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5347 - mae: 1.0188 - mse: 1.5347 - val_loss: 1.6137 - val_mae: 1.0251 - val_mse: 1.6137\n",
      "Epoch 171/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5304 - mae: 1.0153 - mse: 1.5304 - val_loss: 1.6214 - val_mae: 1.0461 - val_mse: 1.6214\n",
      "Epoch 172/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5353 - mae: 1.0196 - mse: 1.5353 - val_loss: 1.6174 - val_mae: 1.0405 - val_mse: 1.6174\n",
      "Epoch 173/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5269 - mae: 1.0166 - mse: 1.5269 - val_loss: 1.6135 - val_mae: 1.0280 - val_mse: 1.6135\n",
      "Epoch 174/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5284 - mae: 1.0145 - mse: 1.5284 - val_loss: 1.6528 - val_mae: 1.0690 - val_mse: 1.6528\n",
      "Epoch 175/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5312 - mae: 1.0188 - mse: 1.5312 - val_loss: 1.6137 - val_mae: 1.0301 - val_mse: 1.6137\n",
      "Epoch 176/450\n",
      "2174/2174 [==============================] - 0s 66us/sample - loss: 1.5316 - mae: 1.0147 - mse: 1.5316 - val_loss: 1.6173 - val_mae: 1.0403 - val_mse: 1.6173\n",
      "Epoch 177/450\n",
      "2174/2174 [==============================] - 0s 79us/sample - loss: 1.5290 - mae: 1.0178 - mse: 1.5290 - val_loss: 1.6163 - val_mae: 1.0165 - val_mse: 1.6163\n",
      "Epoch 178/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5290 - mae: 1.0127 - mse: 1.5290 - val_loss: 1.6631 - val_mae: 1.0741 - val_mse: 1.6631\n",
      "Epoch 179/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5271 - mae: 1.0174 - mse: 1.5271 - val_loss: 1.6136 - val_mae: 1.0283 - val_mse: 1.6136\n",
      "Epoch 180/450\n",
      "2174/2174 [==============================] - 0s 91us/sample - loss: 1.5345 - mae: 1.0161 - mse: 1.5345 - val_loss: 1.6136 - val_mae: 1.0285 - val_mse: 1.6136\n",
      "Epoch 181/450\n",
      "2174/2174 [==============================] - 0s 83us/sample - loss: 1.5338 - mae: 1.0174 - mse: 1.5338 - val_loss: 1.6236 - val_mae: 1.0065 - val_mse: 1.6236\n",
      "Epoch 182/450\n",
      "2174/2174 [==============================] - 0s 81us/sample - loss: 1.5292 - mae: 1.0160 - mse: 1.5292 - val_loss: 1.6175 - val_mae: 1.0142 - val_mse: 1.6175\n",
      "Epoch 183/450\n",
      "2174/2174 [==============================] - 0s 74us/sample - loss: 1.5270 - mae: 1.0144 - mse: 1.5270 - val_loss: 1.6214 - val_mae: 1.0460 - val_mse: 1.6214\n",
      "Epoch 184/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5320 - mae: 1.0163 - mse: 1.5320 - val_loss: 1.6207 - val_mae: 1.0452 - val_mse: 1.6207\n",
      "Epoch 185/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5330 - mae: 1.0197 - mse: 1.5330 - val_loss: 1.6143 - val_mae: 1.0215 - val_mse: 1.6143\n",
      "Epoch 186/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5315 - mae: 1.0165 - mse: 1.5315 - val_loss: 1.6368 - val_mae: 1.0594 - val_mse: 1.6368\n",
      "Epoch 187/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5270 - mae: 1.0151 - mse: 1.5270 - val_loss: 1.6136 - val_mae: 1.0261 - val_mse: 1.6136\n",
      "Epoch 188/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5294 - mae: 1.0163 - mse: 1.5294 - val_loss: 1.6136 - val_mae: 1.0262 - val_mse: 1.6136\n",
      "Epoch 189/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5343 - mae: 1.0173 - mse: 1.5343 - val_loss: 1.6190 - val_mae: 1.0430 - val_mse: 1.6190\n",
      "Epoch 190/450\n",
      "2174/2174 [==============================] - 0s 74us/sample - loss: 1.5290 - mae: 1.0183 - mse: 1.5290 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 191/450\n",
      "2174/2174 [==============================] - 0s 75us/sample - loss: 1.5296 - mae: 1.0173 - mse: 1.5296 - val_loss: 1.6321 - val_mae: 0.9989 - val_mse: 1.6321\n",
      "Epoch 192/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5315 - mae: 1.0145 - mse: 1.5315 - val_loss: 1.6136 - val_mae: 1.0284 - val_mse: 1.6136\n",
      "Epoch 193/450\n",
      "2174/2174 [==============================] - 0s 83us/sample - loss: 1.5253 - mae: 1.0114 - mse: 1.5253 - val_loss: 1.6141 - val_mae: 1.0326 - val_mse: 1.6141\n",
      "Epoch 194/450\n",
      "2174/2174 [==============================] - 0s 86us/sample - loss: 1.5306 - mae: 1.0164 - mse: 1.5306 - val_loss: 1.6143 - val_mae: 1.0216 - val_mse: 1.6143\n",
      "Epoch 195/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5309 - mae: 1.0164 - mse: 1.5309 - val_loss: 1.6208 - val_mae: 1.0453 - val_mse: 1.6208\n",
      "Epoch 196/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5311 - mae: 1.0175 - mse: 1.5311 - val_loss: 1.6150 - val_mae: 1.0355 - val_mse: 1.6150\n",
      "Epoch 197/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5262 - mae: 1.0160 - mse: 1.5262 - val_loss: 1.6136 - val_mae: 1.0253 - val_mse: 1.6136\n",
      "Epoch 198/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5301 - mae: 1.0160 - mse: 1.5301 - val_loss: 1.6180 - val_mae: 1.0414 - val_mse: 1.6180\n",
      "Epoch 199/450\n",
      "2174/2174 [==============================] - 0s 87us/sample - loss: 1.5299 - mae: 1.0170 - mse: 1.5299 - val_loss: 1.6136 - val_mae: 1.0267 - val_mse: 1.6136\n",
      "Epoch 200/450\n",
      "2174/2174 [==============================] - 0s 63us/sample - loss: 1.5247 - mae: 1.0144 - mse: 1.5247 - val_loss: 1.6142 - val_mae: 1.0221 - val_mse: 1.6142\n",
      "Epoch 201/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5310 - mae: 1.0194 - mse: 1.5310 - val_loss: 1.6253 - val_mae: 1.0047 - val_mse: 1.6253\n",
      "Epoch 202/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5290 - mae: 1.0163 - mse: 1.5290 - val_loss: 1.6146 - val_mae: 1.0207 - val_mse: 1.6146\n",
      "Epoch 203/450\n",
      "2174/2174 [==============================] - 0s 64us/sample - loss: 1.5366 - mae: 1.0206 - mse: 1.5366 - val_loss: 1.6152 - val_mae: 1.0189 - val_mse: 1.6152\n",
      "Epoch 204/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5249 - mae: 1.0147 - mse: 1.5249 - val_loss: 1.6136 - val_mae: 1.0296 - val_mse: 1.6136\n",
      "Epoch 205/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5302 - mae: 1.0169 - mse: 1.5302 - val_loss: 1.6137 - val_mae: 1.0303 - val_mse: 1.6137\n",
      "Epoch 206/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5281 - mae: 1.0154 - mse: 1.5281 - val_loss: 1.6147 - val_mae: 1.0203 - val_mse: 1.6147\n",
      "Epoch 207/450\n",
      "2174/2174 [==============================] - 0s 80us/sample - loss: 1.5302 - mae: 1.0174 - mse: 1.5302 - val_loss: 1.6180 - val_mae: 1.0135 - val_mse: 1.6180\n",
      "Epoch 208/450\n",
      "2174/2174 [==============================] - 0s 74us/sample - loss: 1.5290 - mae: 1.0132 - mse: 1.5290 - val_loss: 1.6388 - val_mae: 1.0608 - val_mse: 1.6388\n",
      "Epoch 209/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5307 - mae: 1.0180 - mse: 1.5307 - val_loss: 1.6299 - val_mae: 1.0542 - val_mse: 1.6299\n",
      "Epoch 210/450\n",
      "2174/2174 [==============================] - 0s 75us/sample - loss: 1.5268 - mae: 1.0163 - mse: 1.5268 - val_loss: 1.6159 - val_mae: 1.0377 - val_mse: 1.6159\n",
      "Epoch 211/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5293 - mae: 1.0160 - mse: 1.5293 - val_loss: 1.6254 - val_mae: 1.0503 - val_mse: 1.6254\n",
      "Epoch 212/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5341 - mae: 1.0198 - mse: 1.5341 - val_loss: 1.6166 - val_mae: 1.0390 - val_mse: 1.6166\n",
      "Epoch 213/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5327 - mae: 1.0160 - mse: 1.5327 - val_loss: 1.6331 - val_mae: 1.0568 - val_mse: 1.6331\n",
      "Epoch 214/450\n",
      "2174/2174 [==============================] - 0s 62us/sample - loss: 1.5289 - mae: 1.0172 - mse: 1.5289 - val_loss: 1.6173 - val_mae: 1.0404 - val_mse: 1.6173\n",
      "Epoch 215/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5323 - mae: 1.0184 - mse: 1.5323 - val_loss: 1.6236 - val_mae: 1.0485 - val_mse: 1.6236\n",
      "Epoch 216/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5303 - mae: 1.0152 - mse: 1.5303 - val_loss: 1.6339 - val_mae: 1.0574 - val_mse: 1.6339\n",
      "Epoch 217/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5277 - mae: 1.0190 - mse: 1.5277 - val_loss: 1.6198 - val_mae: 1.0109 - val_mse: 1.6198\n",
      "Epoch 218/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5282 - mae: 1.0150 - mse: 1.5282 - val_loss: 1.6199 - val_mae: 1.0107 - val_mse: 1.6199\n",
      "Epoch 219/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5349 - mae: 1.0166 - mse: 1.5349 - val_loss: 1.6238 - val_mae: 1.0487 - val_mse: 1.6238\n",
      "Epoch 220/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5256 - mae: 1.0158 - mse: 1.5256 - val_loss: 1.6182 - val_mae: 1.0131 - val_mse: 1.6182\n",
      "Epoch 221/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5295 - mae: 1.0174 - mse: 1.5295 - val_loss: 1.6137 - val_mae: 1.0252 - val_mse: 1.6137\n",
      "Epoch 222/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5286 - mae: 1.0127 - mse: 1.5286 - val_loss: 1.6245 - val_mae: 1.0494 - val_mse: 1.6245\n",
      "Epoch 223/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5306 - mae: 1.0164 - mse: 1.5306 - val_loss: 1.6242 - val_mae: 1.0491 - val_mse: 1.6242\n",
      "Epoch 224/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5288 - mae: 1.0201 - mse: 1.5288 - val_loss: 1.6396 - val_mae: 0.9937 - val_mse: 1.6396\n",
      "Epoch 225/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5271 - mae: 1.0129 - mse: 1.5271 - val_loss: 1.6166 - val_mae: 1.0158 - val_mse: 1.6166\n",
      "Epoch 226/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5329 - mae: 1.0147 - mse: 1.5329 - val_loss: 1.6240 - val_mae: 1.0489 - val_mse: 1.6240\n",
      "Epoch 227/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5373 - mae: 1.0207 - mse: 1.5373 - val_loss: 1.6136 - val_mae: 1.0297 - val_mse: 1.6136\n",
      "Epoch 228/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5284 - mae: 1.0162 - mse: 1.5284 - val_loss: 1.6137 - val_mae: 1.0245 - val_mse: 1.6137\n",
      "Epoch 229/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5296 - mae: 1.0147 - mse: 1.5296 - val_loss: 1.6183 - val_mae: 1.0419 - val_mse: 1.6183\n",
      "Epoch 230/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5320 - mae: 1.0179 - mse: 1.5320 - val_loss: 1.6136 - val_mae: 1.0283 - val_mse: 1.6136\n",
      "Epoch 231/450\n",
      "2174/2174 [==============================] - 0s 66us/sample - loss: 1.5332 - mae: 1.0175 - mse: 1.5332 - val_loss: 1.6146 - val_mae: 1.0343 - val_mse: 1.6146\n",
      "Epoch 232/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5392 - mae: 1.0204 - mse: 1.5392 - val_loss: 1.6145 - val_mae: 1.0209 - val_mse: 1.6145\n",
      "Epoch 233/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5320 - mae: 1.0169 - mse: 1.5320 - val_loss: 1.6137 - val_mae: 1.0252 - val_mse: 1.6137\n",
      "Epoch 234/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5305 - mae: 1.0183 - mse: 1.5305 - val_loss: 1.6149 - val_mae: 1.0196 - val_mse: 1.6149\n",
      "Epoch 235/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5323 - mae: 1.0183 - mse: 1.5323 - val_loss: 1.6148 - val_mae: 1.0201 - val_mse: 1.6148\n",
      "Epoch 236/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5247 - mae: 1.0144 - mse: 1.5247 - val_loss: 1.6147 - val_mae: 1.0345 - val_mse: 1.6147\n",
      "Epoch 237/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5296 - mae: 1.0149 - mse: 1.5296 - val_loss: 1.6169 - val_mae: 1.0396 - val_mse: 1.6169\n",
      "Epoch 238/450\n",
      "2174/2174 [==============================] - 0s 67us/sample - loss: 1.5307 - mae: 1.0144 - mse: 1.5307 - val_loss: 1.6135 - val_mae: 1.0270 - val_mse: 1.6135\n",
      "Epoch 239/450\n",
      "2174/2174 [==============================] - 0s 76us/sample - loss: 1.5226 - mae: 1.0109 - mse: 1.5226 - val_loss: 1.6246 - val_mae: 1.0495 - val_mse: 1.6246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/450\n",
      "2174/2174 [==============================] - 0s 90us/sample - loss: 1.5233 - mae: 1.0136 - mse: 1.5233 - val_loss: 1.6187 - val_mae: 1.0426 - val_mse: 1.6187\n",
      "Epoch 241/450\n",
      "2174/2174 [==============================] - 0s 57us/sample - loss: 1.5304 - mae: 1.0161 - mse: 1.5304 - val_loss: 1.6183 - val_mae: 1.0419 - val_mse: 1.6183\n",
      "Epoch 242/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5265 - mae: 1.0156 - mse: 1.5265 - val_loss: 1.6314 - val_mae: 1.0555 - val_mse: 1.6314\n",
      "Epoch 243/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5312 - mae: 1.0172 - mse: 1.5312 - val_loss: 1.6137 - val_mae: 1.0249 - val_mse: 1.6137\n",
      "Epoch 244/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5355 - mae: 1.0149 - mse: 1.5355 - val_loss: 1.6391 - val_mae: 1.0610 - val_mse: 1.6391\n",
      "Epoch 245/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5312 - mae: 1.0187 - mse: 1.5312 - val_loss: 1.6191 - val_mae: 1.0431 - val_mse: 1.6191\n",
      "Epoch 246/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5298 - mae: 1.0178 - mse: 1.5298 - val_loss: 1.6267 - val_mae: 1.0515 - val_mse: 1.6267\n",
      "Epoch 247/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5310 - mae: 1.0175 - mse: 1.5310 - val_loss: 1.6146 - val_mae: 1.0342 - val_mse: 1.6146\n",
      "Epoch 248/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5347 - mae: 1.0170 - mse: 1.5347 - val_loss: 1.6186 - val_mae: 1.0424 - val_mse: 1.6186\n",
      "Epoch 249/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5267 - mae: 1.0148 - mse: 1.5267 - val_loss: 1.6390 - val_mae: 1.0609 - val_mse: 1.6390\n",
      "Epoch 250/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5294 - mae: 1.0193 - mse: 1.5294 - val_loss: 1.6326 - val_mae: 0.9985 - val_mse: 1.6326\n",
      "Epoch 251/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5306 - mae: 1.0160 - mse: 1.5306 - val_loss: 1.6150 - val_mae: 1.0195 - val_mse: 1.6150\n",
      "Epoch 252/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5283 - mae: 1.0125 - mse: 1.5283 - val_loss: 1.6391 - val_mae: 1.0609 - val_mse: 1.6391\n",
      "Epoch 253/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5261 - mae: 1.0149 - mse: 1.5261 - val_loss: 1.6137 - val_mae: 1.0301 - val_mse: 1.6137\n",
      "Epoch 254/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5313 - mae: 1.0154 - mse: 1.5313 - val_loss: 1.6145 - val_mae: 1.0338 - val_mse: 1.6145\n",
      "Epoch 255/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5299 - mae: 1.0163 - mse: 1.5299 - val_loss: 1.6285 - val_mae: 1.0531 - val_mse: 1.6285\n",
      "Epoch 256/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5278 - mae: 1.0179 - mse: 1.5278 - val_loss: 1.6238 - val_mae: 1.0062 - val_mse: 1.6238\n",
      "Epoch 257/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5294 - mae: 1.0130 - mse: 1.5294 - val_loss: 1.6137 - val_mae: 1.0245 - val_mse: 1.6137\n",
      "Epoch 258/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5288 - mae: 1.0142 - mse: 1.5288 - val_loss: 1.6319 - val_mae: 1.0558 - val_mse: 1.6319\n",
      "Epoch 259/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5268 - mae: 1.0169 - mse: 1.5268 - val_loss: 1.6162 - val_mae: 1.0382 - val_mse: 1.6162\n",
      "Epoch 260/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5299 - mae: 1.0163 - mse: 1.5299 - val_loss: 1.6298 - val_mae: 1.0542 - val_mse: 1.6298\n",
      "Epoch 261/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5336 - mae: 1.0134 - mse: 1.5336 - val_loss: 1.6379 - val_mae: 1.0601 - val_mse: 1.6379\n",
      "Epoch 262/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5321 - mae: 1.0159 - mse: 1.5321 - val_loss: 1.6135 - val_mae: 1.0268 - val_mse: 1.6135\n",
      "Epoch 263/450\n",
      "2174/2174 [==============================] - 0s 60us/sample - loss: 1.5300 - mae: 1.0166 - mse: 1.5300 - val_loss: 1.6139 - val_mae: 1.0236 - val_mse: 1.6139\n",
      "Epoch 264/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5248 - mae: 1.0135 - mse: 1.5248 - val_loss: 1.6535 - val_mae: 1.0694 - val_mse: 1.6535\n",
      "Epoch 265/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5281 - mae: 1.0164 - mse: 1.5281 - val_loss: 1.6592 - val_mae: 1.0722 - val_mse: 1.6592\n",
      "Epoch 266/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5228 - mae: 1.0159 - mse: 1.5228 - val_loss: 1.6205 - val_mae: 1.0100 - val_mse: 1.6205\n",
      "Epoch 267/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5316 - mae: 1.0142 - mse: 1.5316 - val_loss: 1.6155 - val_mae: 1.0182 - val_mse: 1.6155\n",
      "Epoch 268/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5275 - mae: 1.0140 - mse: 1.5275 - val_loss: 1.6484 - val_mae: 1.0666 - val_mse: 1.6484\n",
      "Epoch 269/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5319 - mae: 1.0179 - mse: 1.5319 - val_loss: 1.6153 - val_mae: 1.0362 - val_mse: 1.6153\n",
      "Epoch 270/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5314 - mae: 1.0173 - mse: 1.5314 - val_loss: 1.6142 - val_mae: 1.0328 - val_mse: 1.6142\n",
      "Epoch 271/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5260 - mae: 1.0114 - mse: 1.5260 - val_loss: 1.6680 - val_mae: 1.0764 - val_mse: 1.6680\n",
      "Epoch 272/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5293 - mae: 1.0168 - mse: 1.5293 - val_loss: 1.6504 - val_mae: 1.0677 - val_mse: 1.6504\n",
      "Epoch 273/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5301 - mae: 1.0182 - mse: 1.5301 - val_loss: 1.6230 - val_mae: 1.0479 - val_mse: 1.6230\n",
      "Epoch 274/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5252 - mae: 1.0155 - mse: 1.5252 - val_loss: 1.6136 - val_mae: 1.0253 - val_mse: 1.6136\n",
      "Epoch 275/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5301 - mae: 1.0163 - mse: 1.5301 - val_loss: 1.6136 - val_mae: 1.0287 - val_mse: 1.6136\n",
      "Epoch 276/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5287 - mae: 1.0167 - mse: 1.5287 - val_loss: 1.6136 - val_mae: 1.0297 - val_mse: 1.6136\n",
      "Epoch 277/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5335 - mae: 1.0166 - mse: 1.5335 - val_loss: 1.6353 - val_mae: 1.0584 - val_mse: 1.6353\n",
      "Epoch 278/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5275 - mae: 1.0167 - mse: 1.5275 - val_loss: 1.6203 - val_mae: 1.0447 - val_mse: 1.6203\n",
      "Epoch 279/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5309 - mae: 1.0151 - mse: 1.5309 - val_loss: 1.6303 - val_mae: 1.0546 - val_mse: 1.6303\n",
      "Epoch 280/450\n",
      "2174/2174 [==============================] - 0s 68us/sample - loss: 1.5285 - mae: 1.0158 - mse: 1.5285 - val_loss: 1.6299 - val_mae: 1.0543 - val_mse: 1.6299\n",
      "Epoch 281/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5337 - mae: 1.0180 - mse: 1.5337 - val_loss: 1.6141 - val_mae: 1.0224 - val_mse: 1.6141\n",
      "Epoch 282/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5282 - mae: 1.0154 - mse: 1.5282 - val_loss: 1.6151 - val_mae: 1.0356 - val_mse: 1.6151\n",
      "Epoch 283/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5300 - mae: 1.0158 - mse: 1.5300 - val_loss: 1.6135 - val_mae: 1.0273 - val_mse: 1.6135\n",
      "Epoch 284/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5281 - mae: 1.0168 - mse: 1.5281 - val_loss: 1.6236 - val_mae: 1.0065 - val_mse: 1.6236\n",
      "Epoch 285/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5303 - mae: 1.0147 - mse: 1.5303 - val_loss: 1.6144 - val_mae: 1.0214 - val_mse: 1.6144\n",
      "Epoch 286/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5317 - mae: 1.0173 - mse: 1.5317 - val_loss: 1.6157 - val_mae: 1.0178 - val_mse: 1.6157\n",
      "Epoch 287/450\n",
      "2174/2174 [==============================] - 0s 93us/sample - loss: 1.5255 - mae: 1.0132 - mse: 1.5255 - val_loss: 1.6436 - val_mae: 1.0638 - val_mse: 1.6436\n",
      "Epoch 288/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5269 - mae: 1.0151 - mse: 1.5269 - val_loss: 1.6179 - val_mae: 1.0414 - val_mse: 1.6179\n",
      "Epoch 289/450\n",
      "2174/2174 [==============================] - 0s 69us/sample - loss: 1.5271 - mae: 1.0151 - mse: 1.5271 - val_loss: 1.6340 - val_mae: 1.0574 - val_mse: 1.6340\n",
      "Epoch 290/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5303 - mae: 1.0157 - mse: 1.5303 - val_loss: 1.6182 - val_mae: 1.0418 - val_mse: 1.6182\n",
      "Epoch 291/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5250 - mae: 1.0164 - mse: 1.5250 - val_loss: 1.6138 - val_mae: 1.0310 - val_mse: 1.6138\n",
      "Epoch 292/450\n",
      "2174/2174 [==============================] - 0s 77us/sample - loss: 1.5336 - mae: 1.0174 - mse: 1.5336 - val_loss: 1.6146 - val_mae: 1.0207 - val_mse: 1.6146\n",
      "Epoch 293/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5249 - mae: 1.0129 - mse: 1.5249 - val_loss: 1.6316 - val_mae: 1.0556 - val_mse: 1.6316\n",
      "Epoch 294/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5326 - mae: 1.0191 - mse: 1.5326 - val_loss: 1.6149 - val_mae: 1.0353 - val_mse: 1.6149\n",
      "Epoch 295/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5306 - mae: 1.0154 - mse: 1.5306 - val_loss: 1.6186 - val_mae: 1.0424 - val_mse: 1.6186\n",
      "Epoch 296/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5264 - mae: 1.0164 - mse: 1.5264 - val_loss: 1.6136 - val_mae: 1.0297 - val_mse: 1.6136\n",
      "Epoch 297/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5320 - mae: 1.0170 - mse: 1.5320 - val_loss: 1.6140 - val_mae: 1.0229 - val_mse: 1.6140\n",
      "Epoch 298/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5262 - mae: 1.0141 - mse: 1.5262 - val_loss: 1.6150 - val_mae: 1.0355 - val_mse: 1.6150\n",
      "Epoch 299/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5277 - mae: 1.0137 - mse: 1.5277 - val_loss: 1.6148 - val_mae: 1.0348 - val_mse: 1.6148\n",
      "Epoch 300/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5276 - mae: 1.0166 - mse: 1.5276 - val_loss: 1.6159 - val_mae: 1.0376 - val_mse: 1.6159\n",
      "Epoch 301/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5349 - mae: 1.0186 - mse: 1.5349 - val_loss: 1.6236 - val_mae: 1.0485 - val_mse: 1.6236\n",
      "Epoch 302/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5298 - mae: 1.0147 - mse: 1.5298 - val_loss: 1.6446 - val_mae: 1.0644 - val_mse: 1.6446\n",
      "Epoch 303/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5325 - mae: 1.0182 - mse: 1.5325 - val_loss: 1.6141 - val_mae: 1.0326 - val_mse: 1.6141\n",
      "Epoch 304/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5273 - mae: 1.0164 - mse: 1.5273 - val_loss: 1.6153 - val_mae: 1.0188 - val_mse: 1.6153\n",
      "Epoch 305/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5326 - mae: 1.0168 - mse: 1.5326 - val_loss: 1.6137 - val_mae: 1.0250 - val_mse: 1.6137\n",
      "Epoch 306/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5274 - mae: 1.0140 - mse: 1.5274 - val_loss: 1.6180 - val_mae: 1.0414 - val_mse: 1.6180\n",
      "Epoch 307/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5295 - mae: 1.0156 - mse: 1.5295 - val_loss: 1.6341 - val_mae: 1.0575 - val_mse: 1.6341\n",
      "Epoch 308/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5286 - mae: 1.0175 - mse: 1.5286 - val_loss: 1.6155 - val_mae: 1.0367 - val_mse: 1.6155\n",
      "Epoch 309/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5261 - mae: 1.0159 - mse: 1.5261 - val_loss: 1.6216 - val_mae: 1.0086 - val_mse: 1.6216\n",
      "Epoch 310/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5324 - mae: 1.0132 - mse: 1.5324 - val_loss: 1.6233 - val_mae: 1.0481 - val_mse: 1.6233\n",
      "Epoch 311/450\n",
      "2174/2174 [==============================] - 0s 30us/sample - loss: 1.5289 - mae: 1.0184 - mse: 1.5289 - val_loss: 1.6136 - val_mae: 1.0267 - val_mse: 1.6136\n",
      "Epoch 312/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5315 - mae: 1.0161 - mse: 1.5315 - val_loss: 1.6136 - val_mae: 1.0263 - val_mse: 1.6136\n",
      "Epoch 313/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5292 - mae: 1.0154 - mse: 1.5292 - val_loss: 1.6300 - val_mae: 1.0543 - val_mse: 1.6300\n",
      "Epoch 314/450\n",
      "2174/2174 [==============================] - 0s 29us/sample - loss: 1.5301 - mae: 1.0189 - mse: 1.5301 - val_loss: 1.6189 - val_mae: 1.0427 - val_mse: 1.6189\n",
      "Epoch 315/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5314 - mae: 1.0182 - mse: 1.5314 - val_loss: 1.6136 - val_mae: 1.0261 - val_mse: 1.6136\n",
      "Epoch 316/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5308 - mae: 1.0177 - mse: 1.5308 - val_loss: 1.6167 - val_mae: 1.0157 - val_mse: 1.6167\n",
      "Epoch 317/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5256 - mae: 1.0144 - mse: 1.5256 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 318/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5314 - mae: 1.0139 - mse: 1.5314 - val_loss: 1.6237 - val_mae: 1.0485 - val_mse: 1.6237\n",
      "Epoch 319/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5291 - mae: 1.0153 - mse: 1.5291 - val_loss: 1.6158 - val_mae: 1.0375 - val_mse: 1.6158\n",
      "Epoch 320/450\n",
      "2174/2174 [==============================] - 0s 86us/sample - loss: 1.5303 - mae: 1.0153 - mse: 1.5303 - val_loss: 1.6228 - val_mae: 1.0476 - val_mse: 1.6228\n",
      "Epoch 321/450\n",
      "2174/2174 [==============================] - 0s 98us/sample - loss: 1.5363 - mae: 1.0191 - mse: 1.5363 - val_loss: 1.6136 - val_mae: 1.0295 - val_mse: 1.6136\n",
      "Epoch 322/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5335 - mae: 1.0185 - mse: 1.5335 - val_loss: 1.6135 - val_mae: 1.0273 - val_mse: 1.6135\n",
      "Epoch 323/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5275 - mae: 1.0161 - mse: 1.5275 - val_loss: 1.6198 - val_mae: 1.0108 - val_mse: 1.6198\n",
      "Epoch 324/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5332 - mae: 1.0145 - mse: 1.5332 - val_loss: 1.6315 - val_mae: 1.0555 - val_mse: 1.6315\n",
      "Epoch 325/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5271 - mae: 1.0150 - mse: 1.5271 - val_loss: 1.6263 - val_mae: 1.0511 - val_mse: 1.6263\n",
      "Epoch 326/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5303 - mae: 1.0160 - mse: 1.5303 - val_loss: 1.6286 - val_mae: 1.0532 - val_mse: 1.6286\n",
      "Epoch 327/450\n",
      "2174/2174 [==============================] - 0s 61us/sample - loss: 1.5382 - mae: 1.0185 - mse: 1.5382 - val_loss: 1.6320 - val_mae: 1.0559 - val_mse: 1.6320\n",
      "Epoch 328/450\n",
      "2174/2174 [==============================] - 0s 93us/sample - loss: 1.5269 - mae: 1.0163 - mse: 1.5269 - val_loss: 1.6206 - val_mae: 1.0451 - val_mse: 1.6206\n",
      "Epoch 329/450\n",
      "2174/2174 [==============================] - 0s 58us/sample - loss: 1.5276 - mae: 1.0171 - mse: 1.5276 - val_loss: 1.6183 - val_mae: 1.0420 - val_mse: 1.6183\n",
      "Epoch 330/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5348 - mae: 1.0178 - mse: 1.5348 - val_loss: 1.6144 - val_mae: 1.0337 - val_mse: 1.6144\n",
      "Epoch 331/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5320 - mae: 1.0173 - mse: 1.5320 - val_loss: 1.6145 - val_mae: 1.0341 - val_mse: 1.6145\n",
      "Epoch 332/450\n",
      "2174/2174 [==============================] - 0s 69us/sample - loss: 1.5286 - mae: 1.0169 - mse: 1.5286 - val_loss: 1.6136 - val_mae: 1.0294 - val_mse: 1.6136\n",
      "Epoch 333/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5327 - mae: 1.0164 - mse: 1.5327 - val_loss: 1.6500 - val_mae: 1.0675 - val_mse: 1.6500\n",
      "Epoch 334/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5351 - mae: 1.0207 - mse: 1.5351 - val_loss: 1.6139 - val_mae: 1.0236 - val_mse: 1.6139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5266 - mae: 1.0141 - mse: 1.5266 - val_loss: 1.6166 - val_mae: 1.0391 - val_mse: 1.6166\n",
      "Epoch 336/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5249 - mae: 1.0153 - mse: 1.5249 - val_loss: 1.6138 - val_mae: 1.0309 - val_mse: 1.6138\n",
      "Epoch 337/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5279 - mae: 1.0161 - mse: 1.5279 - val_loss: 1.6190 - val_mae: 1.0429 - val_mse: 1.6190\n",
      "Epoch 338/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5266 - mae: 1.0155 - mse: 1.5266 - val_loss: 1.6147 - val_mae: 1.0347 - val_mse: 1.6147\n",
      "Epoch 339/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5215 - mae: 1.0119 - mse: 1.5215 - val_loss: 1.6138 - val_mae: 1.0242 - val_mse: 1.6138\n",
      "Epoch 340/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5257 - mae: 1.0145 - mse: 1.5257 - val_loss: 1.6347 - val_mae: 1.0579 - val_mse: 1.6347\n",
      "Epoch 341/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5285 - mae: 1.0167 - mse: 1.5285 - val_loss: 1.6143 - val_mae: 1.0217 - val_mse: 1.6143\n",
      "Epoch 342/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5309 - mae: 1.0174 - mse: 1.5309 - val_loss: 1.6168 - val_mae: 1.0154 - val_mse: 1.6168\n",
      "Epoch 343/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5291 - mae: 1.0154 - mse: 1.5291 - val_loss: 1.6295 - val_mae: 1.0539 - val_mse: 1.6295\n",
      "Epoch 344/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5269 - mae: 1.0158 - mse: 1.5269 - val_loss: 1.6264 - val_mae: 1.0512 - val_mse: 1.6264\n",
      "Epoch 345/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5361 - mae: 1.0191 - mse: 1.5361 - val_loss: 1.6136 - val_mae: 1.0257 - val_mse: 1.6136\n",
      "Epoch 346/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5307 - mae: 1.0169 - mse: 1.5307 - val_loss: 1.6153 - val_mae: 1.0362 - val_mse: 1.6153\n",
      "Epoch 347/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5295 - mae: 1.0172 - mse: 1.5295 - val_loss: 1.6146 - val_mae: 1.0344 - val_mse: 1.6146\n",
      "Epoch 348/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5264 - mae: 1.0133 - mse: 1.5264 - val_loss: 1.6355 - val_mae: 1.0585 - val_mse: 1.6355\n",
      "Epoch 349/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5313 - mae: 1.0185 - mse: 1.5313 - val_loss: 1.6159 - val_mae: 1.0377 - val_mse: 1.6159\n",
      "Epoch 350/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5254 - mae: 1.0159 - mse: 1.5254 - val_loss: 1.6158 - val_mae: 1.0373 - val_mse: 1.6158\n",
      "Epoch 351/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5330 - mae: 1.0160 - mse: 1.5330 - val_loss: 1.6443 - val_mae: 1.0642 - val_mse: 1.6443\n",
      "Epoch 352/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5283 - mae: 1.0173 - mse: 1.5283 - val_loss: 1.6163 - val_mae: 1.0385 - val_mse: 1.6163\n",
      "Epoch 353/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5333 - mae: 1.0204 - mse: 1.5333 - val_loss: 1.6201 - val_mae: 1.0105 - val_mse: 1.6201\n",
      "Epoch 354/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5329 - mae: 1.0164 - mse: 1.5329 - val_loss: 1.6148 - val_mae: 1.0348 - val_mse: 1.6148\n",
      "Epoch 355/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5283 - mae: 1.0154 - mse: 1.5283 - val_loss: 1.6429 - val_mae: 1.0634 - val_mse: 1.6429\n",
      "Epoch 356/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5293 - mae: 1.0178 - mse: 1.5293 - val_loss: 1.6136 - val_mae: 1.0255 - val_mse: 1.6136\n",
      "Epoch 357/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5331 - mae: 1.0162 - mse: 1.5331 - val_loss: 1.6149 - val_mae: 1.0353 - val_mse: 1.6149\n",
      "Epoch 358/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5288 - mae: 1.0158 - mse: 1.5288 - val_loss: 1.6147 - val_mae: 1.0346 - val_mse: 1.6147\n",
      "Epoch 359/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5293 - mae: 1.0162 - mse: 1.5293 - val_loss: 1.6136 - val_mae: 1.0261 - val_mse: 1.6136\n",
      "Epoch 360/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5332 - mae: 1.0171 - mse: 1.5332 - val_loss: 1.6164 - val_mae: 1.0387 - val_mse: 1.6164\n",
      "Epoch 361/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5300 - mae: 1.0184 - mse: 1.5300 - val_loss: 1.6143 - val_mae: 1.0215 - val_mse: 1.6143\n",
      "Epoch 362/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5299 - mae: 1.0134 - mse: 1.5299 - val_loss: 1.6434 - val_mae: 1.0637 - val_mse: 1.6434\n",
      "Epoch 363/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5290 - mae: 1.0176 - mse: 1.5290 - val_loss: 1.6150 - val_mae: 1.0193 - val_mse: 1.6150\n",
      "Epoch 364/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5261 - mae: 1.0136 - mse: 1.5261 - val_loss: 1.6188 - val_mae: 1.0427 - val_mse: 1.6188\n",
      "Epoch 365/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5272 - mae: 1.0154 - mse: 1.5272 - val_loss: 1.6163 - val_mae: 1.0384 - val_mse: 1.6163\n",
      "Epoch 366/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5293 - mae: 1.0137 - mse: 1.5293 - val_loss: 1.6234 - val_mae: 1.0483 - val_mse: 1.6234\n",
      "Epoch 367/450\n",
      "2174/2174 [==============================] - 0s 73us/sample - loss: 1.5259 - mae: 1.0146 - mse: 1.5259 - val_loss: 1.6226 - val_mae: 1.0474 - val_mse: 1.6226\n",
      "Epoch 368/450\n",
      "2174/2174 [==============================] - 0s 80us/sample - loss: 1.5268 - mae: 1.0167 - mse: 1.5268 - val_loss: 1.6157 - val_mae: 1.0372 - val_mse: 1.6157\n",
      "Epoch 369/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5303 - mae: 1.0189 - mse: 1.5303 - val_loss: 1.6296 - val_mae: 1.0540 - val_mse: 1.6296\n",
      "Epoch 370/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5309 - mae: 1.0168 - mse: 1.5309 - val_loss: 1.6241 - val_mae: 1.0490 - val_mse: 1.6241\n",
      "Epoch 371/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5286 - mae: 1.0137 - mse: 1.5286 - val_loss: 1.6432 - val_mae: 1.0635 - val_mse: 1.6432\n",
      "Epoch 372/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5336 - mae: 1.0215 - mse: 1.5336 - val_loss: 1.6138 - val_mae: 1.0306 - val_mse: 1.6138\n",
      "Epoch 373/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5313 - mae: 1.0181 - mse: 1.5313 - val_loss: 1.6201 - val_mae: 1.0444 - val_mse: 1.6201\n",
      "Epoch 374/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5290 - mae: 1.0166 - mse: 1.5290 - val_loss: 1.6146 - val_mae: 1.0343 - val_mse: 1.6146\n",
      "Epoch 375/450\n",
      "2174/2174 [==============================] - 0s 79us/sample - loss: 1.5278 - mae: 1.0165 - mse: 1.5278 - val_loss: 1.6268 - val_mae: 1.0516 - val_mse: 1.6268\n",
      "Epoch 376/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5288 - mae: 1.0190 - mse: 1.5288 - val_loss: 1.6294 - val_mae: 1.0010 - val_mse: 1.6294\n",
      "Epoch 377/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5306 - mae: 1.0130 - mse: 1.5306 - val_loss: 1.6252 - val_mae: 1.0501 - val_mse: 1.6252\n",
      "Epoch 378/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5344 - mae: 1.0172 - mse: 1.5344 - val_loss: 1.6151 - val_mae: 1.0358 - val_mse: 1.6151\n",
      "Epoch 379/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5226 - mae: 1.0128 - mse: 1.5226 - val_loss: 1.6495 - val_mae: 1.0672 - val_mse: 1.6495\n",
      "Epoch 380/450\n",
      "2174/2174 [==============================] - 0s 51us/sample - loss: 1.5283 - mae: 1.0196 - mse: 1.5283 - val_loss: 1.6193 - val_mae: 1.0116 - val_mse: 1.6193\n",
      "Epoch 381/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5316 - mae: 1.0146 - mse: 1.5316 - val_loss: 1.6197 - val_mae: 1.0439 - val_mse: 1.6197\n",
      "Epoch 382/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5287 - mae: 1.0182 - mse: 1.5287 - val_loss: 1.6138 - val_mae: 1.0306 - val_mse: 1.6138\n",
      "Epoch 383/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5294 - mae: 1.0170 - mse: 1.5294 - val_loss: 1.6202 - val_mae: 1.0103 - val_mse: 1.6202\n",
      "Epoch 384/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5304 - mae: 1.0160 - mse: 1.5304 - val_loss: 1.6186 - val_mae: 1.0126 - val_mse: 1.6186\n",
      "Epoch 385/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5301 - mae: 1.0160 - mse: 1.5301 - val_loss: 1.6136 - val_mae: 1.0291 - val_mse: 1.6136\n",
      "Epoch 386/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5323 - mae: 1.0178 - mse: 1.5323 - val_loss: 1.6193 - val_mae: 1.0115 - val_mse: 1.6193\n",
      "Epoch 387/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5342 - mae: 1.0172 - mse: 1.5342 - val_loss: 1.6141 - val_mae: 1.0324 - val_mse: 1.6141\n",
      "Epoch 388/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5287 - mae: 1.0164 - mse: 1.5287 - val_loss: 1.6227 - val_mae: 1.0476 - val_mse: 1.6227\n",
      "Epoch 389/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5305 - mae: 1.0177 - mse: 1.5305 - val_loss: 1.6139 - val_mae: 1.0315 - val_mse: 1.6139\n",
      "Epoch 390/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5292 - mae: 1.0149 - mse: 1.5292 - val_loss: 1.6330 - val_mae: 1.0567 - val_mse: 1.6330\n",
      "Epoch 391/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5325 - mae: 1.0190 - mse: 1.5325 - val_loss: 1.6143 - val_mae: 1.0215 - val_mse: 1.6143\n",
      "Epoch 392/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5286 - mae: 1.0163 - mse: 1.5286 - val_loss: 1.6146 - val_mae: 1.0343 - val_mse: 1.6146\n",
      "Epoch 393/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5290 - mae: 1.0135 - mse: 1.5290 - val_loss: 1.6270 - val_mae: 1.0518 - val_mse: 1.6270\n",
      "Epoch 394/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5274 - mae: 1.0167 - mse: 1.5274 - val_loss: 1.6231 - val_mae: 1.0479 - val_mse: 1.6231\n",
      "Epoch 395/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5321 - mae: 1.0199 - mse: 1.5321 - val_loss: 1.6151 - val_mae: 1.0191 - val_mse: 1.6151\n",
      "Epoch 396/450\n",
      "2174/2174 [==============================] - 0s 44us/sample - loss: 1.5302 - mae: 1.0169 - mse: 1.5302 - val_loss: 1.6146 - val_mae: 1.0342 - val_mse: 1.6146\n",
      "Epoch 397/450\n",
      "2174/2174 [==============================] - 0s 38us/sample - loss: 1.5276 - mae: 1.0154 - mse: 1.5276 - val_loss: 1.6188 - val_mae: 1.0426 - val_mse: 1.6188\n",
      "Epoch 398/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5300 - mae: 1.0185 - mse: 1.5300 - val_loss: 1.6159 - val_mae: 1.0173 - val_mse: 1.6159\n",
      "Epoch 399/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5297 - mae: 1.0148 - mse: 1.5297 - val_loss: 1.6147 - val_mae: 1.0346 - val_mse: 1.6147\n",
      "Epoch 400/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5248 - mae: 1.0151 - mse: 1.5248 - val_loss: 1.6226 - val_mae: 1.0475 - val_mse: 1.6226\n",
      "Epoch 401/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5325 - mae: 1.0151 - mse: 1.5325 - val_loss: 1.6356 - val_mae: 1.0586 - val_mse: 1.6356\n",
      "Epoch 402/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5368 - mae: 1.0186 - mse: 1.5368 - val_loss: 1.6203 - val_mae: 1.0447 - val_mse: 1.6203\n",
      "Epoch 403/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5274 - mae: 1.0158 - mse: 1.5274 - val_loss: 1.6269 - val_mae: 1.0516 - val_mse: 1.6269\n",
      "Epoch 404/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5301 - mae: 1.0189 - mse: 1.5301 - val_loss: 1.6136 - val_mae: 1.0290 - val_mse: 1.6136\n",
      "Epoch 405/450\n",
      "2174/2174 [==============================] - 0s 55us/sample - loss: 1.5292 - mae: 1.0163 - mse: 1.5292 - val_loss: 1.6152 - val_mae: 1.0360 - val_mse: 1.6152\n",
      "Epoch 406/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5294 - mae: 1.0170 - mse: 1.5294 - val_loss: 1.6250 - val_mae: 1.0499 - val_mse: 1.6250\n",
      "Epoch 407/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5266 - mae: 1.0151 - mse: 1.5266 - val_loss: 1.6141 - val_mae: 1.0226 - val_mse: 1.6141\n",
      "Epoch 408/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5288 - mae: 1.0151 - mse: 1.5288 - val_loss: 1.6214 - val_mae: 1.0089 - val_mse: 1.6214\n",
      "Epoch 409/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5288 - mae: 1.0140 - mse: 1.5288 - val_loss: 1.6293 - val_mae: 1.0537 - val_mse: 1.6293\n",
      "Epoch 410/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5279 - mae: 1.0174 - mse: 1.5279 - val_loss: 1.6170 - val_mae: 1.0151 - val_mse: 1.6170\n",
      "Epoch 411/450\n",
      "2174/2174 [==============================] - 0s 54us/sample - loss: 1.5329 - mae: 1.0156 - mse: 1.5329 - val_loss: 1.6138 - val_mae: 1.0308 - val_mse: 1.6138\n",
      "Epoch 412/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5303 - mae: 1.0181 - mse: 1.5303 - val_loss: 1.6207 - val_mae: 1.0098 - val_mse: 1.6207\n",
      "Epoch 413/450\n",
      "2174/2174 [==============================] - 0s 50us/sample - loss: 1.5363 - mae: 1.0168 - mse: 1.5363 - val_loss: 1.6160 - val_mae: 1.0379 - val_mse: 1.6160\n",
      "Epoch 414/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5284 - mae: 1.0172 - mse: 1.5284 - val_loss: 1.6140 - val_mae: 1.0230 - val_mse: 1.6140\n",
      "Epoch 415/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5330 - mae: 1.0155 - mse: 1.5330 - val_loss: 1.6181 - val_mae: 1.0416 - val_mse: 1.6181\n",
      "Epoch 416/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5271 - mae: 1.0154 - mse: 1.5271 - val_loss: 1.6209 - val_mae: 1.0455 - val_mse: 1.6209\n",
      "Epoch 417/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5294 - mae: 1.0167 - mse: 1.5294 - val_loss: 1.6168 - val_mae: 1.0394 - val_mse: 1.6168\n",
      "Epoch 418/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5274 - mae: 1.0165 - mse: 1.5274 - val_loss: 1.6136 - val_mae: 1.0260 - val_mse: 1.6136\n",
      "Epoch 419/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5280 - mae: 1.0151 - mse: 1.5280 - val_loss: 1.6159 - val_mae: 1.0377 - val_mse: 1.6159\n",
      "Epoch 420/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5245 - mae: 1.0122 - mse: 1.5245 - val_loss: 1.6136 - val_mae: 1.0291 - val_mse: 1.6136\n",
      "Epoch 421/450\n",
      "2174/2174 [==============================] - 0s 56us/sample - loss: 1.5296 - mae: 1.0160 - mse: 1.5296 - val_loss: 1.6135 - val_mae: 1.0273 - val_mse: 1.6135\n",
      "Epoch 422/450\n",
      "2174/2174 [==============================] - 0s 42us/sample - loss: 1.5268 - mae: 1.0135 - mse: 1.5268 - val_loss: 1.6335 - val_mae: 1.0571 - val_mse: 1.6335\n",
      "Epoch 423/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5313 - mae: 1.0192 - mse: 1.5313 - val_loss: 1.6173 - val_mae: 1.0147 - val_mse: 1.6173\n",
      "Epoch 424/450\n",
      "2174/2174 [==============================] - 0s 47us/sample - loss: 1.5321 - mae: 1.0173 - mse: 1.5321 - val_loss: 1.6148 - val_mae: 1.0201 - val_mse: 1.6148\n",
      "Epoch 425/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5214 - mae: 1.0123 - mse: 1.5214 - val_loss: 1.6640 - val_mae: 1.0745 - val_mse: 1.6640\n",
      "Epoch 426/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5279 - mae: 1.0174 - mse: 1.5279 - val_loss: 1.6136 - val_mae: 1.0257 - val_mse: 1.6136\n",
      "Epoch 427/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5249 - mae: 1.0157 - mse: 1.5249 - val_loss: 1.6138 - val_mae: 1.0243 - val_mse: 1.6138\n",
      "Epoch 428/450\n",
      "2174/2174 [==============================] - 0s 59us/sample - loss: 1.5289 - mae: 1.0165 - mse: 1.5289 - val_loss: 1.6194 - val_mae: 1.0115 - val_mse: 1.6194\n",
      "Epoch 429/450\n",
      "2174/2174 [==============================] - 0s 49us/sample - loss: 1.5284 - mae: 1.0133 - mse: 1.5284 - val_loss: 1.6303 - val_mae: 1.0546 - val_mse: 1.6303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5299 - mae: 1.0167 - mse: 1.5299 - val_loss: 1.6154 - val_mae: 1.0366 - val_mse: 1.6154\n",
      "Epoch 431/450\n",
      "2174/2174 [==============================] - 0s 52us/sample - loss: 1.5306 - mae: 1.0144 - mse: 1.5306 - val_loss: 1.6306 - val_mae: 1.0548 - val_mse: 1.6306\n",
      "Epoch 432/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5303 - mae: 1.0181 - mse: 1.5303 - val_loss: 1.6135 - val_mae: 1.0278 - val_mse: 1.6135\n",
      "Epoch 433/450\n",
      "2174/2174 [==============================] - 0s 41us/sample - loss: 1.5244 - mae: 1.0140 - mse: 1.5244 - val_loss: 1.6215 - val_mae: 1.0462 - val_mse: 1.6215\n",
      "Epoch 434/450\n",
      "2174/2174 [==============================] - 0s 45us/sample - loss: 1.5363 - mae: 1.0213 - mse: 1.5363 - val_loss: 1.6198 - val_mae: 1.0109 - val_mse: 1.6198\n",
      "Epoch 435/450\n",
      "2174/2174 [==============================] - 0s 53us/sample - loss: 1.5326 - mae: 1.0159 - mse: 1.5326 - val_loss: 1.6250 - val_mae: 1.0499 - val_mse: 1.6250\n",
      "Epoch 436/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5347 - mae: 1.0156 - mse: 1.5347 - val_loss: 1.6667 - val_mae: 1.0758 - val_mse: 1.6667\n",
      "Epoch 437/450\n",
      "2174/2174 [==============================] - 0s 39us/sample - loss: 1.5312 - mae: 1.0188 - mse: 1.5312 - val_loss: 1.6280 - val_mae: 1.0526 - val_mse: 1.6280\n",
      "Epoch 438/450\n",
      "2174/2174 [==============================] - 0s 48us/sample - loss: 1.5300 - mae: 1.0180 - mse: 1.5300 - val_loss: 1.6257 - val_mae: 1.0044 - val_mse: 1.6257\n",
      "Epoch 439/450\n",
      "2174/2174 [==============================] - 0s 33us/sample - loss: 1.5293 - mae: 1.0113 - mse: 1.5293 - val_loss: 1.6205 - val_mae: 1.0450 - val_mse: 1.6205\n",
      "Epoch 440/450\n",
      "2174/2174 [==============================] - 0s 37us/sample - loss: 1.5271 - mae: 1.0159 - mse: 1.5271 - val_loss: 1.6152 - val_mae: 1.0359 - val_mse: 1.6152\n",
      "Epoch 441/450\n",
      "2174/2174 [==============================] - 0s 43us/sample - loss: 1.5340 - mae: 1.0188 - mse: 1.5340 - val_loss: 1.6164 - val_mae: 1.0387 - val_mse: 1.6164\n",
      "Epoch 442/450\n",
      "2174/2174 [==============================] - 0s 35us/sample - loss: 1.5254 - mae: 1.0146 - mse: 1.5254 - val_loss: 1.6650 - val_mae: 1.0750 - val_mse: 1.6650\n",
      "Epoch 443/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5322 - mae: 1.0166 - mse: 1.5322 - val_loss: 1.6395 - val_mae: 1.0612 - val_mse: 1.6395\n",
      "Epoch 444/450\n",
      "2174/2174 [==============================] - 0s 40us/sample - loss: 1.5297 - mae: 1.0167 - mse: 1.5297 - val_loss: 1.6200 - val_mae: 1.0443 - val_mse: 1.6200\n",
      "Epoch 445/450\n",
      "2174/2174 [==============================] - 0s 36us/sample - loss: 1.5292 - mae: 1.0181 - mse: 1.5292 - val_loss: 1.6136 - val_mae: 1.0287 - val_mse: 1.6136\n",
      "Epoch 446/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5277 - mae: 1.0164 - mse: 1.5277 - val_loss: 1.6136 - val_mae: 1.0265 - val_mse: 1.6136\n",
      "Epoch 447/450\n",
      "2174/2174 [==============================] - 0s 32us/sample - loss: 1.5306 - mae: 1.0171 - mse: 1.5306 - val_loss: 1.6143 - val_mae: 1.0334 - val_mse: 1.6143\n",
      "Epoch 448/450\n",
      "2174/2174 [==============================] - 0s 34us/sample - loss: 1.5347 - mae: 1.0154 - mse: 1.5347 - val_loss: 1.6317 - val_mae: 1.0557 - val_mse: 1.6317\n",
      "Epoch 449/450\n",
      "2174/2174 [==============================] - 0s 31us/sample - loss: 1.5327 - mae: 1.0185 - mse: 1.5327 - val_loss: 1.6189 - val_mae: 1.0429 - val_mse: 1.6189\n",
      "Epoch 450/450\n",
      "2174/2174 [==============================] - 0s 46us/sample - loss: 1.5291 - mae: 1.0167 - mse: 1.5291 - val_loss: 1.6143 - val_mae: 1.0218 - val_mse: 1.6143\n"
     ]
    }
   ],
   "source": [
    "trained_weight = ks.get_weights()[0]\n",
    "trained_bias = ks.get_weights()[1]\n",
    "\n",
    "EPOCHS = 450\n",
    "history = ks.fit(X_train,\n",
    "                 y_train,\n",
    "                 epochs = EPOCHS,\n",
    "                 batch_size = 128,\n",
    "                 validation_split = 0.2,\n",
    "                 verbose = 1)\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist['mse']\n",
    "epochs = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    plt.plot(feature.tolist(), label.tolist(), c='r')\n",
    "    # Render the scatter plot and the red line.\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.97, mse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature = X['all'].copy()\n",
    "#label = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_the_model(trained_weight, trained_bias, feature, label)\n",
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              loss         mae          mse    val_loss     val_mae  \\\n",
      "count   450.000000  450.000000   450.000000  450.000000  450.000000   \n",
      "mean      9.086165    1.050286     9.086165    1.632415    1.038954   \n",
      "std     160.149782    0.693368   160.149776    0.135470    0.034742   \n",
      "min       1.521420    1.009285     1.521420    1.612012    0.977886   \n",
      "25%       1.527667    1.015006     1.527667    1.614579    1.025582   \n",
      "50%       1.529606    1.016395     1.529606    1.617964    1.037376   \n",
      "75%       1.531973    1.017556     1.531973    1.625375    1.048965   \n",
      "max    3398.826308   15.723609  3398.826172    4.306231    1.638326   \n",
      "\n",
      "          val_mse  \n",
      "count  450.000000  \n",
      "mean     1.632415  \n",
      "std      0.135470  \n",
      "min      1.612012  \n",
      "25%      1.614579  \n",
      "50%      1.617964  \n",
      "75%      1.625375  \n",
      "max      4.306231  \n"
     ]
    }
   ],
   "source": [
    "print(hist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(a,b):\n",
    "    #print('pred :',a,'actual :',b)\n",
    "    if a == b:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.027766386623852446\n",
      "dt 0.09933525045903402\n",
      "rf 0.4936061703302301\n",
      "vr 0.2607999113428656\n",
      "ks 1.0093989\n"
     ]
    }
   ],
   "source": [
    "print('lr',lr.score(X_train, y_train))\n",
    "print('dt',dt.score(X_train, y_train))\n",
    "print('rf',rf.score(X_train, y_train))\n",
    "print('vr',vr.score(X_train, y_train))\n",
    "ks_test = ks.evaluate(X_train, y_train,verbose=0)\n",
    "print('ks',ks_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_pred_test(result,num):\n",
    "    score = check(result,y_test.loc[num])\n",
    "    return score\n",
    "\n",
    "def model_pred_test(model):\n",
    "    pred = []\n",
    "    numbers = X_test.index\n",
    "    random_nums = random.choices(numbers, k=50)\n",
    "    for i in random_nums:\n",
    "        p = X_test.loc[i].tolist()\n",
    "        result = model.predict([p]).flatten().round()\n",
    "        prediction = cycle_pred_test(result,i)\n",
    "        pred.append(prediction)\n",
    "    dz = pd.DataFrame(pred)\n",
    "    #df = pd.DataFrame(prob)\n",
    "    c = str(float(dz.sum().values / 50))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(index=range(10),columns=['vr','dt','rf','ks'])\n",
    "test_results = test_results.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    v_s = model_pred_test(vr)\n",
    "    test_results.at[i,'vr'] = v_s\n",
    "    d_s = model_pred_test(dt)\n",
    "    test_results.at[i,'dt'] = d_s\n",
    "    r_s = model_pred_test(rf)\n",
    "    test_results.at[i,'rf'] = r_s\n",
    "    k_s = model_pred_test(ks)\n",
    "    test_results.at[i,'ks'] = k_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vr</th>\n",
       "      <th>dt</th>\n",
       "      <th>rf</th>\n",
       "      <th>ks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.33200</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.08804</td>\n",
       "      <td>0.049396</td>\n",
       "      <td>0.069314</td>\n",
       "      <td>0.062004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.18000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.28500</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.33000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.36000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             vr         dt         rf         ks\n",
       "count  10.00000  10.000000  10.000000  10.000000\n",
       "mean    0.33200   0.298000   0.286000   0.330000\n",
       "std     0.08804   0.049396   0.069314   0.062004\n",
       "min     0.18000   0.200000   0.180000   0.240000\n",
       "25%     0.28500   0.270000   0.250000   0.285000\n",
       "50%     0.33000   0.320000   0.290000   0.330000\n",
       "75%     0.36000   0.320000   0.335000   0.375000\n",
       "max     0.50000   0.360000   0.380000   0.420000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.iloc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    e = e[0]\n",
    "    if e < 1:\n",
    "        e = 0\n",
    "    elif e < 2:\n",
    "        e = 1\n",
    "    return e\n",
    "\n",
    "def model_pred_test(model):\n",
    "    b = []\n",
    "    prob = []\n",
    "    random_nums = np.random.randint(low=1, high=58, size=(20))\n",
    "    for i in random_nums:\n",
    "        prob.append(cycle_prob_test(i,model))\n",
    "    df = pd.DataFrame(prob)\n",
    "    df = df.values\n",
    "    print('scores :\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d96eedabfe9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/cpl_score_regressor.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vr' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_score_regressor.sav'\n",
    "pickle.dump(vr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import cpl_main as cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forge FC Cavalry FC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todd/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "#model_pred_test(cpl_classifier_model)\n",
    "\n",
    "# home side\n",
    "q1 = schedule.iloc[0]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[0]['away']\n",
    "print(q1,q2)\n",
    "\n",
    "compare = cpl.get_team_comparison(results_brief,q1,q2)\n",
    "\n",
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)\n",
    "\n",
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "game_info\n",
    "\n",
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)\n",
    "\n",
    "home_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "away_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.33\n"
     ]
    }
   ],
   "source": [
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db\n",
    "\n",
    "q1_roster = get_overall_roster(home_roster)\n",
    "#print(q1_roster)\n",
    "\n",
    "q2_roster = get_overall_roster(away_roster)\n",
    "#print(q2_roster)\n",
    "\n",
    "def roster_regressor_pred(model,array):\n",
    "    prediction = model.predict([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    return df\n",
    "\n",
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)\n",
    "print(home_win, away_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n",
      "0.35\n",
      "\n",
      " Forge FC \n",
      "win probability:  0.34\n",
      "\n",
      " Cavalry FC \n",
      "win probability:  0.35\n",
      "\n",
      "Draw probability:  0.31\n"
     ]
    }
   ],
   "source": [
    "classifier = 'models/cpl_roster_classifier.sav'\n",
    "cpl_classifier_model = pickle.load(open(classifier, 'rb'))\n",
    "\n",
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(home_win_new)\n",
    "print(away_win_new)\n",
    "\n",
    "print('\\n',q1,'\\nwin probability: ', round(home_win_new,2))\n",
    "\n",
    "print('\\n',q2,'\\nwin probability: ', round(away_win_new,2))\n",
    "\n",
    "print('\\nDraw probability: ', round(draw_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/cpl_score_regressor.sav'\n",
    "cpl_score_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_score_prediction(model,q1_roster,q2_roster,home_win_new,away_win_new):\n",
    "\n",
    "    def roster_pred(model,array):\n",
    "        prediction = model.predict([array]).flatten()\n",
    "        return prediction\n",
    "\n",
    "    def final_score_fix(home_score,away_score,home_win_new,away_win_new):\n",
    "        if home_win_new > away_win_new and home_score < away_score: # fix the score prediction - if the probability of home win > away win and score doesn't reflect it\n",
    "            old_home = home_score\n",
    "            home_score = away_score # change the predicted score to reflect that\n",
    "            away_score = old_home\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score > away_score: # else the probability of home win < away win\n",
    "            old_away = away_score\n",
    "            away_score = home_score # change the predicted score to reflect that\n",
    "            home_score = away_score\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new > away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        else:\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "\n",
    "    def score(num): #improve this later for greater predictions\n",
    "        new_score = int(round(num,0)) # convert the float value to int and round it\n",
    "        return new_score\n",
    "\n",
    "    q1_pred = roster_pred(model,q1_roster)\n",
    "    q1_s = score(q1_pred[0])\n",
    "    q2_pred = roster_pred(model,q2_roster)\n",
    "    q2_s = score(q2_pred[0])\n",
    "    home_score, away_score, home_win_new, away_win_new = final_score_fix(q1_s, q2_s,home_win_new,away_win_new)\n",
    "    return home_score,away_score, home_win_new, away_win_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home 2 away 2\n"
     ]
    }
   ],
   "source": [
    "home_score, away_score, home_win_new, away_win_new = get_final_score_prediction(cpl_score_model,q1_roster,q2_roster,home_win_new,away_win_new)\n",
    "print('home',home_score,'away', away_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = q1_roster.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = l.values.tolist()\n",
    "l = l[0]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "average = statistics.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4421428516507149"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-136c17fc3cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "l.extend(average)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
