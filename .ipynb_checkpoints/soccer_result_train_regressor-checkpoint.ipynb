{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todd McCullough\n",
    "# 2020\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cpl_main as cpl\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  game      team    p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1  \\\n",
      "0   I1  Forge FC  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48   \n",
      "1   I1  York9 FC  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70   \n",
      "\n",
      "    p10  p11  p12  p13  r  s  \n",
      "0  0.42  0.0  0.0  0.0  2  1  \n",
      "1  0.47  0.0  0.0  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "results = pd.read_csv(f'datasets/soccer-nn-train.csv')\n",
    "print(results.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pump_it_up(db):\n",
    "    df = db.copy()\n",
    "    dc = df.copy()\n",
    "    m = df['p1'].copy()\n",
    "    n = df['p2'].copy()\n",
    "    o = df['p3'].copy()\n",
    "    p = df['p4'].copy()\n",
    "    q = df['p5'].copy()\n",
    "    r = df['p6'].copy()\n",
    "    df['p1'] = dc.pop('p8')\n",
    "    df['p2'] = dc.pop('p10')\n",
    "    df['p3'] = dc.pop('p12')\n",
    "    df['p4'] = dc.pop('p9')\n",
    "    df['p5'] = dc.pop('p11')\n",
    "    df['p6'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p8'] = n\n",
    "    df['p9'] = o\n",
    "    df['p10'] = p\n",
    "    df['p11'] = q\n",
    "    df['p12'] = r\n",
    "    df['p13'] = dc.pop('p7')\n",
    "    dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    df = dc.copy()\n",
    "    m = df['p13'].copy()\n",
    "    n = df['p12'].copy()\n",
    "    o = df['p11'].copy()\n",
    "    p = df['p10'].copy()\n",
    "    q = df['p9'].copy()\n",
    "    r = df['p8'].copy()\n",
    "    df['p13'] = dc.pop('p8')\n",
    "    df['p12'] = dc.pop('p10')\n",
    "    df['p11'] = dc.pop('p12')\n",
    "    df['p10'] = dc.pop('p9')\n",
    "    df['p9'] = dc.pop('p11')\n",
    "    df['p8'] = dc.pop('p13')\n",
    "    df['p7'] = m\n",
    "    df['p6'] = n\n",
    "    df['p5'] = o\n",
    "    df['p4'] = p\n",
    "    df['p3'] = q\n",
    "    df['p2'] = r\n",
    "    df['p1'] = dc.pop('p7')\n",
    "    #dc = df.copy()\n",
    "    db = pd.concat([db,df])\n",
    "    db = cpl.index_reset(db)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pump_it_up(results)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3883, 18)\n"
     ]
    }
   ],
   "source": [
    "print(db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.pop('game')\n",
    "db.pop('team')\n",
    "'''y = db.pop('s')\n",
    "db.pop('r')'''\n",
    "X = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
       "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
       "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
       "\n",
       "   p13  r  s  \n",
       "0  0.0  2  1  \n",
       "1  0.0  2  1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['all'] = round(X.sum(axis = 1, skipna = True) / 13,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
      "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
      "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
      "\n",
      "   p13  r  s  \n",
      "0  0.0  2  1  \n",
      "1  0.0  2  1  \n"
     ]
    }
   ],
   "source": [
    "print(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#importing libraries from sklearn\n",
    "from sklearn import tree\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler#,Imputer\n",
    "from sklearn import metrics\n",
    "# import algorithm modules\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  \\\n",
       "12  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0   \n",
       "43  0.79  0.91  0.75  0.37  0.33  0.87  0.66  0.66  0.62  0.73  0.64  0.0   \n",
       "\n",
       "    p12  p13  r  s  \n",
       "12  0.0  0.0  3  3  \n",
       "43  0.0  0.0  3  3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high = X[X['s'] > 2]\n",
    "high.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_y = high.pop('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p9.1</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1    p2    p3    p4    p5    p6    p7    p8    p9  p9.1   p10  p11  p12  \\\n",
       "0  0.71  0.89  0.74  0.71  0.48  0.89  0.77  0.52  0.50  0.48  0.42  0.0  0.0   \n",
       "1  0.93  0.92  0.91  0.86  0.41  0.78  0.65  0.64  0.46  0.70  0.47  0.0  0.0   \n",
       "\n",
       "   p13  r  s  \n",
       "0  0.0  2  1  \n",
       "1  0.0  2  1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low = X[X['s'] <= 2]\n",
    "low.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y = low.pop('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 16) (3152, 16)\n"
     ]
    }
   ],
   "source": [
    "print(high.shape,low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_X_train, h_X_test, h_y_train, h_y_test = train_test_split(high, h_y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_X_train, l_X_test, l_y_train, l_y_test = train_test_split(low, l_y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_X_train = pd.concat([l_X_train,h_X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_X_test = pd.concat([l_X_test,h_X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3105"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_X_train.shape[0] + l_X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y_train = pd.concat([l_y_train,h_y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y_test = pd.concat([l_y_test,h_y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_y_train.shape[0] + l_y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_y_test.shape[0] + l_X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = l_X_train, l_X_test, l_y_train, l_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3883, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3105, 15)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(778, 15)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3883"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model\n",
      "\n",
      "RMSE:  1.221500435672965\n",
      "\n",
      "Score 2.59\n",
      "\n",
      "Decision Tree Regression Model\n",
      "\n",
      "RMSE:  1.2548333744753983\n",
      "\n",
      "Score -2.8\n",
      "\n",
      "Random Forest Regression Model\n",
      "\n",
      "RMSE:  1.2318084161900413\n",
      "\n",
      "Score 0.94\n",
      "\n",
      "Voting Regressor Model\n",
      "\n",
      "RMSE:  1.2268773566254239\n",
      "\n",
      "Score 1.73\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression Model\n",
    "def linearRegression():\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "lr = linearRegression()\n",
    "\n",
    "print('\\nLinear Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,lr.predict(X_test))))\n",
    "print('\\nScore',round(lr.score(X_test, y_test)*100,2))\n",
    "\n",
    "#DecisionTreeRegressor\n",
    "def decisionTree():\n",
    "    model = DecisionTreeRegressor(criterion='mse', splitter='random', max_depth=8, max_features='log2')\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "dt = decisionTree()\n",
    "\n",
    "print('\\nDecision Tree Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test, dt.predict(X_test))))\n",
    "print('\\nScore',round(dt.score(X_test, y_test)*100,2))\n",
    "\n",
    "#Random Forest Regression\n",
    "def forestRegression():\n",
    "    model = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "rf = forestRegression()\n",
    "\n",
    "print('\\nRandom Forest Regression Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,rf.predict(X_test))))\n",
    "print('\\nScore',round(rf.score(X_test, y_test)*100,2))\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "vr = VotingRegressor(estimators=[('lr', lr), ('dt', dt), ('rf', rf)])\n",
    "vr = vr.fit(X_train, y_train)\n",
    "\n",
    "print('\\nVoting Regressor Model')\n",
    "print('\\nRMSE: ', sqrt(mean_squared_error(y_test,vr.predict(X_test))))\n",
    "print('\\nScore',round(vr.score(X_test, y_test)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasSequential():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[X_train.shape[1]]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = 'mse',\n",
    "                optimizer = tf.keras.optimizers.RMSprop(0.1),\n",
    "                metrics = ['mae', 'mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                960       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 5,185\n",
      "Trainable params: 5,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ks = kerasSequential()\n",
    "print(ks.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2484 samples, validate on 622 samples\n",
      "Epoch 1/450\n",
      "2484/2484 [==============================] - 1s 540us/sample - loss: 1913.2928 - mae: 10.9872 - mse: 1913.2931 - val_loss: 1.6441 - val_mae: 1.0205 - val_mse: 1.6441\n",
      "Epoch 2/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5355 - mae: 0.9923 - mse: 1.5355 - val_loss: 1.6693 - val_mae: 1.0036 - val_mse: 1.6693\n",
      "Epoch 3/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.4932 - mae: 0.9802 - mse: 1.4932 - val_loss: 1.7526 - val_mae: 0.9850 - val_mse: 1.7526\n",
      "Epoch 4/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5556 - mae: 0.9933 - mse: 1.5556 - val_loss: 1.6994 - val_mae: 0.9920 - val_mse: 1.6994\n",
      "Epoch 5/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 1.6003 - mae: 1.0109 - mse: 1.6003 - val_loss: 1.8131 - val_mae: 0.9860 - val_mse: 1.8131\n",
      "Epoch 6/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.6447 - mae: 1.0218 - mse: 1.6447 - val_loss: 1.6403 - val_mae: 1.0286 - val_mse: 1.6403\n",
      "Epoch 7/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5883 - mae: 1.0061 - mse: 1.5883 - val_loss: 1.6744 - val_mae: 1.0167 - val_mse: 1.6744\n",
      "Epoch 8/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5416 - mae: 1.0069 - mse: 1.5416 - val_loss: 1.7141 - val_mae: 0.9997 - val_mse: 1.7141\n",
      "Epoch 9/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.6794 - mae: 1.0356 - mse: 1.6794 - val_loss: 1.6546 - val_mae: 1.0517 - val_mse: 1.6546\n",
      "Epoch 10/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 1.5475 - mae: 1.0135 - mse: 1.5475 - val_loss: 1.6808 - val_mae: 1.0746 - val_mse: 1.6808\n",
      "Epoch 11/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5238 - mae: 1.0127 - mse: 1.5238 - val_loss: 1.6892 - val_mae: 1.0106 - val_mse: 1.6892\n",
      "Epoch 12/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5395 - mae: 1.0099 - mse: 1.5395 - val_loss: 1.6544 - val_mae: 1.0444 - val_mse: 1.6544\n",
      "Epoch 13/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5186 - mae: 1.0084 - mse: 1.5186 - val_loss: 1.6560 - val_mae: 1.0520 - val_mse: 1.6560\n",
      "Epoch 14/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5219 - mae: 1.0117 - mse: 1.5219 - val_loss: 1.6830 - val_mae: 1.0758 - val_mse: 1.6830\n",
      "Epoch 15/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5146 - mae: 1.0130 - mse: 1.5146 - val_loss: 1.6982 - val_mae: 1.0831 - val_mse: 1.6982\n",
      "Epoch 16/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5165 - mae: 1.0112 - mse: 1.5165 - val_loss: 1.6733 - val_mae: 1.0699 - val_mse: 1.6733\n",
      "Epoch 17/450\n",
      "2484/2484 [==============================] - 0s 101us/sample - loss: 1.5188 - mae: 1.0127 - mse: 1.5188 - val_loss: 1.6603 - val_mae: 1.0308 - val_mse: 1.6603\n",
      "Epoch 18/450\n",
      "2484/2484 [==============================] - 0s 93us/sample - loss: 1.5156 - mae: 1.0117 - mse: 1.5156 - val_loss: 1.6596 - val_mae: 1.0579 - val_mse: 1.6596\n",
      "Epoch 19/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 1.5134 - mae: 1.0104 - mse: 1.5134 - val_loss: 1.6559 - val_mae: 1.0519 - val_mse: 1.6559\n",
      "Epoch 20/450\n",
      "2484/2484 [==============================] - 0s 95us/sample - loss: 1.5107 - mae: 1.0115 - mse: 1.5107 - val_loss: 1.6581 - val_mae: 1.0559 - val_mse: 1.6581\n",
      "Epoch 21/450\n",
      "2484/2484 [==============================] - 0s 109us/sample - loss: 1.5193 - mae: 1.0141 - mse: 1.5193 - val_loss: 1.6681 - val_mae: 1.0233 - val_mse: 1.6681\n",
      "Epoch 22/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 1.5161 - mae: 1.0099 - mse: 1.5161 - val_loss: 1.6628 - val_mae: 1.0280 - val_mse: 1.6628\n",
      "Epoch 23/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5179 - mae: 1.0123 - mse: 1.5179 - val_loss: 1.6546 - val_mae: 1.0421 - val_mse: 1.6546\n",
      "Epoch 24/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5145 - mae: 1.0127 - mse: 1.5145 - val_loss: 1.6545 - val_mae: 1.0431 - val_mse: 1.6545\n",
      "Epoch 25/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5163 - mae: 1.0107 - mse: 1.5163 - val_loss: 1.6604 - val_mae: 1.0590 - val_mse: 1.6604\n",
      "Epoch 26/450\n",
      "2484/2484 [==============================] - 0s 96us/sample - loss: 1.5110 - mae: 1.0107 - mse: 1.5110 - val_loss: 1.6684 - val_mae: 1.0664 - val_mse: 1.6684\n",
      "Epoch 27/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 1.5207 - mae: 1.0145 - mse: 1.5207 - val_loss: 1.6682 - val_mae: 1.0663 - val_mse: 1.6682\n",
      "Epoch 28/450\n",
      "2484/2484 [==============================] - 0s 80us/sample - loss: 1.5142 - mae: 1.0136 - mse: 1.5142 - val_loss: 1.6578 - val_mae: 1.0555 - val_mse: 1.6578\n",
      "Epoch 29/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 1.5149 - mae: 1.0125 - mse: 1.5149 - val_loss: 1.6798 - val_mae: 1.0740 - val_mse: 1.6798\n",
      "Epoch 30/450\n",
      "2484/2484 [==============================] - 0s 86us/sample - loss: 1.5157 - mae: 1.0128 - mse: 1.5157 - val_loss: 1.7013 - val_mae: 1.0845 - val_mse: 1.7013\n",
      "Epoch 31/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5131 - mae: 1.0138 - mse: 1.5131 - val_loss: 1.6591 - val_mae: 1.0322 - val_mse: 1.6591\n",
      "Epoch 32/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 1.5171 - mae: 1.0137 - mse: 1.5171 - val_loss: 1.6626 - val_mae: 1.0282 - val_mse: 1.6626\n",
      "Epoch 33/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5123 - mae: 1.0109 - mse: 1.5123 - val_loss: 1.6547 - val_mae: 1.0417 - val_mse: 1.6547\n",
      "Epoch 34/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5198 - mae: 1.0140 - mse: 1.5198 - val_loss: 1.6546 - val_mae: 1.0468 - val_mse: 1.6546\n",
      "Epoch 35/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 1.5126 - mae: 1.0114 - mse: 1.5126 - val_loss: 1.6576 - val_mae: 1.0344 - val_mse: 1.6576\n",
      "Epoch 36/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5151 - mae: 1.0114 - mse: 1.5151 - val_loss: 1.6611 - val_mae: 1.0298 - val_mse: 1.6611\n",
      "Epoch 37/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5132 - mae: 1.0108 - mse: 1.5132 - val_loss: 1.6564 - val_mae: 1.0367 - val_mse: 1.6564\n",
      "Epoch 38/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5191 - mae: 1.0137 - mse: 1.5191 - val_loss: 1.6926 - val_mae: 1.0806 - val_mse: 1.6926\n",
      "Epoch 39/450\n",
      "2484/2484 [==============================] - 0s 94us/sample - loss: 1.5177 - mae: 1.0160 - mse: 1.5177 - val_loss: 1.6768 - val_mae: 1.0722 - val_mse: 1.6768\n",
      "Epoch 40/450\n",
      "2484/2484 [==============================] - 0s 78us/sample - loss: 1.5194 - mae: 1.0151 - mse: 1.5194 - val_loss: 1.6546 - val_mae: 1.0421 - val_mse: 1.6546\n",
      "Epoch 41/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 1.5125 - mae: 1.0109 - mse: 1.5125 - val_loss: 1.6666 - val_mae: 1.0650 - val_mse: 1.6666\n",
      "Epoch 42/450\n",
      "2484/2484 [==============================] - 0s 93us/sample - loss: 1.5125 - mae: 1.0107 - mse: 1.5125 - val_loss: 1.6620 - val_mae: 1.0288 - val_mse: 1.6620\n",
      "Epoch 43/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5151 - mae: 1.0123 - mse: 1.5151 - val_loss: 1.6553 - val_mae: 1.0395 - val_mse: 1.6553\n",
      "Epoch 44/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5181 - mae: 1.0129 - mse: 1.5181 - val_loss: 1.6573 - val_mae: 1.0546 - val_mse: 1.6573\n",
      "Epoch 45/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5166 - mae: 1.0132 - mse: 1.5166 - val_loss: 1.6657 - val_mae: 1.0642 - val_mse: 1.6657\n",
      "Epoch 46/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 1.5125 - mae: 1.0142 - mse: 1.5125 - val_loss: 1.6640 - val_mae: 1.0269 - val_mse: 1.6640\n",
      "Epoch 47/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5111 - mae: 1.0099 - mse: 1.5111 - val_loss: 1.6544 - val_mae: 1.0455 - val_mse: 1.6544\n",
      "Epoch 48/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5123 - mae: 1.0068 - mse: 1.5123 - val_loss: 1.6609 - val_mae: 1.0595 - val_mse: 1.6609\n",
      "Epoch 49/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5139 - mae: 1.0108 - mse: 1.5139 - val_loss: 1.6803 - val_mae: 1.0743 - val_mse: 1.6803\n",
      "Epoch 50/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5149 - mae: 1.0144 - mse: 1.5149 - val_loss: 1.6624 - val_mae: 1.0611 - val_mse: 1.6624\n",
      "Epoch 51/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5115 - mae: 1.0105 - mse: 1.5115 - val_loss: 1.6665 - val_mae: 1.0247 - val_mse: 1.6665\n",
      "Epoch 52/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5164 - mae: 1.0104 - mse: 1.5164 - val_loss: 1.6576 - val_mae: 1.0551 - val_mse: 1.6576\n",
      "Epoch 53/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5170 - mae: 1.0140 - mse: 1.5170 - val_loss: 1.6766 - val_mae: 1.0720 - val_mse: 1.6766\n",
      "Epoch 54/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5163 - mae: 1.0132 - mse: 1.5163 - val_loss: 1.6739 - val_mae: 1.0703 - val_mse: 1.6739\n",
      "Epoch 55/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5117 - mae: 1.0134 - mse: 1.5117 - val_loss: 1.6863 - val_mae: 1.0120 - val_mse: 1.6863\n",
      "Epoch 56/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5169 - mae: 1.0124 - mse: 1.5169 - val_loss: 1.6573 - val_mae: 1.0546 - val_mse: 1.6573\n",
      "Epoch 57/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5203 - mae: 1.0144 - mse: 1.5203 - val_loss: 1.6578 - val_mae: 1.0554 - val_mse: 1.6578\n",
      "Epoch 58/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5128 - mae: 1.0100 - mse: 1.5128 - val_loss: 1.6724 - val_mae: 1.0202 - val_mse: 1.6724\n",
      "Epoch 59/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5153 - mae: 1.0090 - mse: 1.5153 - val_loss: 1.6779 - val_mae: 1.0729 - val_mse: 1.6779\n",
      "Epoch 60/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5192 - mae: 1.0148 - mse: 1.5192 - val_loss: 1.6750 - val_mae: 1.0710 - val_mse: 1.6750\n",
      "Epoch 61/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5119 - mae: 1.0137 - mse: 1.5119 - val_loss: 1.6549 - val_mae: 1.0486 - val_mse: 1.6549\n",
      "Epoch 62/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5163 - mae: 1.0120 - mse: 1.5163 - val_loss: 1.6618 - val_mae: 1.0605 - val_mse: 1.6618\n",
      "Epoch 63/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5143 - mae: 1.0098 - mse: 1.5143 - val_loss: 1.7011 - val_mae: 1.0844 - val_mse: 1.7011\n",
      "Epoch 64/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5081 - mae: 1.0123 - mse: 1.5081 - val_loss: 1.6843 - val_mae: 1.0131 - val_mse: 1.6843\n",
      "Epoch 65/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5221 - mae: 1.0114 - mse: 1.5221 - val_loss: 1.6547 - val_mae: 1.0476 - val_mse: 1.6547\n",
      "Epoch 66/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5134 - mae: 1.0127 - mse: 1.5134 - val_loss: 1.6544 - val_mae: 1.0454 - val_mse: 1.6544\n",
      "Epoch 67/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5179 - mae: 1.0137 - mse: 1.5179 - val_loss: 1.6874 - val_mae: 1.0780 - val_mse: 1.6874\n",
      "Epoch 68/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5112 - mae: 1.0112 - mse: 1.5112 - val_loss: 1.6545 - val_mae: 1.0431 - val_mse: 1.6545\n",
      "Epoch 69/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5180 - mae: 1.0139 - mse: 1.5180 - val_loss: 1.6549 - val_mae: 1.0410 - val_mse: 1.6549\n",
      "Epoch 70/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5124 - mae: 1.0109 - mse: 1.5124 - val_loss: 1.6694 - val_mae: 1.0224 - val_mse: 1.6694\n",
      "Epoch 71/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5135 - mae: 1.0099 - mse: 1.5135 - val_loss: 1.6675 - val_mae: 1.0657 - val_mse: 1.6675\n",
      "Epoch 72/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5182 - mae: 1.0136 - mse: 1.5182 - val_loss: 1.6580 - val_mae: 1.0558 - val_mse: 1.6580\n",
      "Epoch 73/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5145 - mae: 1.0110 - mse: 1.5145 - val_loss: 1.6567 - val_mae: 1.0360 - val_mse: 1.6567\n",
      "Epoch 74/450\n",
      "2484/2484 [==============================] - 0s 140us/sample - loss: 1.5111 - mae: 1.0096 - mse: 1.5111 - val_loss: 1.6552 - val_mae: 1.0395 - val_mse: 1.6552\n",
      "Epoch 75/450\n",
      "2484/2484 [==============================] - 0s 99us/sample - loss: 1.5134 - mae: 1.0109 - mse: 1.5134 - val_loss: 1.6930 - val_mae: 1.0808 - val_mse: 1.6930\n",
      "Epoch 76/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 1.5124 - mae: 1.0101 - mse: 1.5124 - val_loss: 1.6577 - val_mae: 1.0552 - val_mse: 1.6577\n",
      "Epoch 77/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 1.5165 - mae: 1.0139 - mse: 1.5165 - val_loss: 1.6638 - val_mae: 1.0270 - val_mse: 1.6638\n",
      "Epoch 78/450\n",
      "2484/2484 [==============================] - 0s 88us/sample - loss: 1.5162 - mae: 1.0130 - mse: 1.5162 - val_loss: 1.6654 - val_mae: 1.0256 - val_mse: 1.6654\n",
      "Epoch 79/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 1.5131 - mae: 1.0108 - mse: 1.5131 - val_loss: 1.7249 - val_mae: 1.0934 - val_mse: 1.7249\n",
      "Epoch 80/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 1.5148 - mae: 1.0129 - mse: 1.5148 - val_loss: 1.6712 - val_mae: 1.0685 - val_mse: 1.6712\n",
      "Epoch 81/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 1.5161 - mae: 1.0145 - mse: 1.5161 - val_loss: 1.6569 - val_mae: 1.0356 - val_mse: 1.6569\n",
      "Epoch 82/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5166 - mae: 1.0119 - mse: 1.5166 - val_loss: 1.6575 - val_mae: 1.0549 - val_mse: 1.6575\n",
      "Epoch 83/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5140 - mae: 1.0120 - mse: 1.5140 - val_loss: 1.6548 - val_mae: 1.0413 - val_mse: 1.6548\n",
      "Epoch 84/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5079 - mae: 1.0084 - mse: 1.5079 - val_loss: 1.7749 - val_mae: 1.1084 - val_mse: 1.7749\n",
      "Epoch 85/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5171 - mae: 1.0133 - mse: 1.5171 - val_loss: 1.6557 - val_mae: 1.0512 - val_mse: 1.6557\n",
      "Epoch 86/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 1.5177 - mae: 1.0150 - mse: 1.5177 - val_loss: 1.6588 - val_mae: 1.0569 - val_mse: 1.6588\n",
      "Epoch 87/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5185 - mae: 1.0140 - mse: 1.5185 - val_loss: 1.6547 - val_mae: 1.0477 - val_mse: 1.6547\n",
      "Epoch 88/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5120 - mae: 1.0115 - mse: 1.5120 - val_loss: 1.6825 - val_mae: 1.0755 - val_mse: 1.6825\n",
      "Epoch 89/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5156 - mae: 1.0130 - mse: 1.5156 - val_loss: 1.6579 - val_mae: 1.0340 - val_mse: 1.6579\n",
      "Epoch 90/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5145 - mae: 1.0108 - mse: 1.5145 - val_loss: 1.6642 - val_mae: 1.0628 - val_mse: 1.6642\n",
      "Epoch 91/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5161 - mae: 1.0139 - mse: 1.5161 - val_loss: 1.6560 - val_mae: 1.0521 - val_mse: 1.6560\n",
      "Epoch 92/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5139 - mae: 1.0108 - mse: 1.5139 - val_loss: 1.7133 - val_mae: 1.0893 - val_mse: 1.7133\n",
      "Epoch 93/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 1.5131 - mae: 1.0105 - mse: 1.5131 - val_loss: 1.7134 - val_mae: 1.0893 - val_mse: 1.7134\n",
      "Epoch 94/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 1.5141 - mae: 1.0131 - mse: 1.5141 - val_loss: 1.6656 - val_mae: 1.0641 - val_mse: 1.6656\n",
      "Epoch 95/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5110 - mae: 1.0121 - mse: 1.5110 - val_loss: 1.6959 - val_mae: 1.0821 - val_mse: 1.6959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5194 - mae: 1.0147 - mse: 1.5194 - val_loss: 1.6556 - val_mae: 1.0509 - val_mse: 1.6556\n",
      "Epoch 97/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5162 - mae: 1.0130 - mse: 1.5162 - val_loss: 1.6557 - val_mae: 1.0384 - val_mse: 1.6557\n",
      "Epoch 98/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 1.5116 - mae: 1.0095 - mse: 1.5116 - val_loss: 1.6546 - val_mae: 1.0470 - val_mse: 1.6546\n",
      "Epoch 99/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 1.5139 - mae: 1.0131 - mse: 1.5139 - val_loss: 1.6677 - val_mae: 1.0236 - val_mse: 1.6677\n",
      "Epoch 100/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5097 - mae: 1.0075 - mse: 1.5097 - val_loss: 1.6545 - val_mae: 1.0439 - val_mse: 1.6545\n",
      "Epoch 101/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 1.5172 - mae: 1.0136 - mse: 1.5172 - val_loss: 1.7118 - val_mae: 1.0009 - val_mse: 1.7118\n",
      "Epoch 102/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 1.5171 - mae: 1.0095 - mse: 1.5171 - val_loss: 1.6825 - val_mae: 1.0141 - val_mse: 1.6825\n",
      "Epoch 103/450\n",
      "2484/2484 [==============================] - 0s 80us/sample - loss: 1.5168 - mae: 1.0098 - mse: 1.5168 - val_loss: 1.6548 - val_mae: 1.0485 - val_mse: 1.6548\n",
      "Epoch 104/450\n",
      "2484/2484 [==============================] - 0s 79us/sample - loss: 1.5214 - mae: 1.0154 - mse: 1.5214 - val_loss: 1.6602 - val_mae: 1.0587 - val_mse: 1.6602\n",
      "Epoch 105/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 1.5122 - mae: 1.0132 - mse: 1.5122 - val_loss: 1.6636 - val_mae: 1.0624 - val_mse: 1.6636\n",
      "Epoch 106/450\n",
      "2484/2484 [==============================] - 0s 81us/sample - loss: 1.5175 - mae: 1.0133 - mse: 1.5175 - val_loss: 1.6625 - val_mae: 1.0612 - val_mse: 1.6625\n",
      "Epoch 107/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5129 - mae: 1.0116 - mse: 1.5129 - val_loss: 1.6553 - val_mae: 1.0503 - val_mse: 1.6553\n",
      "Epoch 108/450\n",
      "2484/2484 [==============================] - 0s 78us/sample - loss: 1.5133 - mae: 1.0131 - mse: 1.5133 - val_loss: 1.6604 - val_mae: 1.0590 - val_mse: 1.6604\n",
      "Epoch 109/450\n",
      "2484/2484 [==============================] - 0s 74us/sample - loss: 1.5137 - mae: 1.0127 - mse: 1.5137 - val_loss: 1.6824 - val_mae: 1.0141 - val_mse: 1.6824\n",
      "Epoch 110/450\n",
      "2484/2484 [==============================] - 0s 78us/sample - loss: 1.5166 - mae: 1.0124 - mse: 1.5166 - val_loss: 1.6584 - val_mae: 1.0564 - val_mse: 1.6584\n",
      "Epoch 111/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5119 - mae: 1.0106 - mse: 1.5119 - val_loss: 1.6571 - val_mae: 1.0353 - val_mse: 1.6571\n",
      "Epoch 112/450\n",
      "2484/2484 [==============================] - 0s 85us/sample - loss: 1.5158 - mae: 1.0126 - mse: 1.5158 - val_loss: 1.6562 - val_mae: 1.0525 - val_mse: 1.6562\n",
      "Epoch 113/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5170 - mae: 1.0140 - mse: 1.5170 - val_loss: 1.6728 - val_mae: 1.0696 - val_mse: 1.6728\n",
      "Epoch 114/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5130 - mae: 1.0115 - mse: 1.5130 - val_loss: 1.6544 - val_mae: 1.0443 - val_mse: 1.6544\n",
      "Epoch 115/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5169 - mae: 1.0141 - mse: 1.5169 - val_loss: 1.6558 - val_mae: 1.0516 - val_mse: 1.6558\n",
      "Epoch 116/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5148 - mae: 1.0127 - mse: 1.5148 - val_loss: 1.6550 - val_mae: 1.0403 - val_mse: 1.6550\n",
      "Epoch 117/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5150 - mae: 1.0126 - mse: 1.5150 - val_loss: 1.6546 - val_mae: 1.0422 - val_mse: 1.6546\n",
      "Epoch 118/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5143 - mae: 1.0123 - mse: 1.5143 - val_loss: 1.6775 - val_mae: 1.0726 - val_mse: 1.6775\n",
      "Epoch 119/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5140 - mae: 1.0120 - mse: 1.5140 - val_loss: 1.6629 - val_mae: 1.0279 - val_mse: 1.6629\n",
      "Epoch 120/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5096 - mae: 1.0078 - mse: 1.5096 - val_loss: 1.7320 - val_mae: 1.0958 - val_mse: 1.7320\n",
      "Epoch 121/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5213 - mae: 1.0160 - mse: 1.5213 - val_loss: 1.6549 - val_mae: 1.0488 - val_mse: 1.6549\n",
      "Epoch 122/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5123 - mae: 1.0128 - mse: 1.5123 - val_loss: 1.7015 - val_mae: 1.0050 - val_mse: 1.7015\n",
      "Epoch 123/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5215 - mae: 1.0098 - mse: 1.5215 - val_loss: 1.6560 - val_mae: 1.0374 - val_mse: 1.6560\n",
      "Epoch 124/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5137 - mae: 1.0103 - mse: 1.5137 - val_loss: 1.6552 - val_mae: 1.0398 - val_mse: 1.6552\n",
      "Epoch 125/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5151 - mae: 1.0097 - mse: 1.5151 - val_loss: 1.6952 - val_mae: 1.0818 - val_mse: 1.6952\n",
      "Epoch 126/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5101 - mae: 1.0106 - mse: 1.5101 - val_loss: 1.7213 - val_mae: 1.0922 - val_mse: 1.7213\n",
      "Epoch 127/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5179 - mae: 1.0157 - mse: 1.5179 - val_loss: 1.6867 - val_mae: 1.0118 - val_mse: 1.6867\n",
      "Epoch 128/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5131 - mae: 1.0086 - mse: 1.5131 - val_loss: 1.6544 - val_mae: 1.0446 - val_mse: 1.6544\n",
      "Epoch 129/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 1.5181 - mae: 1.0152 - mse: 1.5181 - val_loss: 1.6545 - val_mae: 1.0456 - val_mse: 1.6545\n",
      "Epoch 130/450\n",
      "2484/2484 [==============================] - 0s 93us/sample - loss: 1.5128 - mae: 1.0116 - mse: 1.5128 - val_loss: 1.6593 - val_mae: 1.0576 - val_mse: 1.6593\n",
      "Epoch 131/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5148 - mae: 1.0127 - mse: 1.5148 - val_loss: 1.6604 - val_mae: 1.0589 - val_mse: 1.6604\n",
      "Epoch 132/450\n",
      "2484/2484 [==============================] - 0s 95us/sample - loss: 1.5117 - mae: 1.0097 - mse: 1.5117 - val_loss: 1.6685 - val_mae: 1.0665 - val_mse: 1.6685\n",
      "Epoch 133/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5168 - mae: 1.0144 - mse: 1.5168 - val_loss: 1.6730 - val_mae: 1.0198 - val_mse: 1.6730\n",
      "Epoch 134/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5156 - mae: 1.0087 - mse: 1.5156 - val_loss: 1.6614 - val_mae: 1.0600 - val_mse: 1.6614\n",
      "Epoch 135/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 1.5120 - mae: 1.0128 - mse: 1.5120 - val_loss: 1.6604 - val_mae: 1.0306 - val_mse: 1.6604\n",
      "Epoch 136/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5152 - mae: 1.0112 - mse: 1.5152 - val_loss: 1.6707 - val_mae: 1.0214 - val_mse: 1.6707\n",
      "Epoch 137/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5148 - mae: 1.0098 - mse: 1.5148 - val_loss: 1.7379 - val_mae: 1.0977 - val_mse: 1.7379\n",
      "Epoch 138/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5210 - mae: 1.0176 - mse: 1.5210 - val_loss: 1.6602 - val_mae: 1.0587 - val_mse: 1.6602\n",
      "Epoch 139/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5151 - mae: 1.0123 - mse: 1.5151 - val_loss: 1.6573 - val_mae: 1.0349 - val_mse: 1.6573\n",
      "Epoch 140/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5127 - mae: 1.0112 - mse: 1.5127 - val_loss: 1.6813 - val_mae: 1.0748 - val_mse: 1.6813\n",
      "Epoch 141/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5135 - mae: 1.0130 - mse: 1.5135 - val_loss: 1.6600 - val_mae: 1.0311 - val_mse: 1.6600\n",
      "Epoch 142/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 1.5149 - mae: 1.0095 - mse: 1.5149 - val_loss: 1.6551 - val_mae: 1.0399 - val_mse: 1.6551\n",
      "Epoch 143/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5146 - mae: 1.0111 - mse: 1.5146 - val_loss: 1.6548 - val_mae: 1.0414 - val_mse: 1.6548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 1.5135 - mae: 1.0122 - mse: 1.5135 - val_loss: 1.6544 - val_mae: 1.0440 - val_mse: 1.6544\n",
      "Epoch 145/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5119 - mae: 1.0127 - mse: 1.5119 - val_loss: 1.6729 - val_mae: 1.0199 - val_mse: 1.6729\n",
      "Epoch 146/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5149 - mae: 1.0109 - mse: 1.5149 - val_loss: 1.6596 - val_mae: 1.0316 - val_mse: 1.6596\n",
      "Epoch 147/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5131 - mae: 1.0097 - mse: 1.5131 - val_loss: 1.6545 - val_mae: 1.0465 - val_mse: 1.6545\n",
      "Epoch 148/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5128 - mae: 1.0100 - mse: 1.5128 - val_loss: 1.6944 - val_mae: 1.0814 - val_mse: 1.6944\n",
      "Epoch 149/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5200 - mae: 1.0152 - mse: 1.5200 - val_loss: 1.6648 - val_mae: 1.0634 - val_mse: 1.6648\n",
      "Epoch 150/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 1.5101 - mae: 1.0121 - mse: 1.5101 - val_loss: 1.6561 - val_mae: 1.0523 - val_mse: 1.6561\n",
      "Epoch 151/450\n",
      "2484/2484 [==============================] - 0s 68us/sample - loss: 1.5111 - mae: 1.0091 - mse: 1.5111 - val_loss: 1.7274 - val_mae: 1.0943 - val_mse: 1.7274\n",
      "Epoch 152/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5138 - mae: 1.0137 - mse: 1.5138 - val_loss: 1.6711 - val_mae: 1.0211 - val_mse: 1.6711\n",
      "Epoch 153/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5167 - mae: 1.0098 - mse: 1.5167 - val_loss: 1.6647 - val_mae: 1.0633 - val_mse: 1.6647\n",
      "Epoch 154/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5144 - mae: 1.0118 - mse: 1.5144 - val_loss: 1.6579 - val_mae: 1.0556 - val_mse: 1.6579\n",
      "Epoch 155/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5194 - mae: 1.0135 - mse: 1.5194 - val_loss: 1.6588 - val_mae: 1.0569 - val_mse: 1.6588\n",
      "Epoch 156/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5106 - mae: 1.0119 - mse: 1.5106 - val_loss: 1.6692 - val_mae: 1.0225 - val_mse: 1.6692\n",
      "Epoch 157/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5093 - mae: 1.0055 - mse: 1.5093 - val_loss: 1.7348 - val_mae: 1.0967 - val_mse: 1.7348\n",
      "Epoch 158/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5143 - mae: 1.0130 - mse: 1.5143 - val_loss: 1.6700 - val_mae: 1.0676 - val_mse: 1.6700\n",
      "Epoch 159/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5164 - mae: 1.0128 - mse: 1.5164 - val_loss: 1.6575 - val_mae: 1.0347 - val_mse: 1.6575\n",
      "Epoch 160/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5163 - mae: 1.0115 - mse: 1.5163 - val_loss: 1.6672 - val_mae: 1.0240 - val_mse: 1.6672\n",
      "Epoch 161/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5179 - mae: 1.0115 - mse: 1.5179 - val_loss: 1.6546 - val_mae: 1.0425 - val_mse: 1.6546\n",
      "Epoch 162/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5071 - mae: 1.0089 - mse: 1.5071 - val_loss: 1.6924 - val_mae: 1.0091 - val_mse: 1.6924\n",
      "Epoch 163/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 1.5201 - mae: 1.0104 - mse: 1.5201 - val_loss: 1.6596 - val_mae: 1.0580 - val_mse: 1.6596\n",
      "Epoch 164/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5154 - mae: 1.0121 - mse: 1.5154 - val_loss: 1.6694 - val_mae: 1.0672 - val_mse: 1.6694\n",
      "Epoch 165/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5136 - mae: 1.0149 - mse: 1.5136 - val_loss: 1.6851 - val_mae: 1.0127 - val_mse: 1.6851\n",
      "Epoch 166/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5129 - mae: 1.0095 - mse: 1.5129 - val_loss: 1.6550 - val_mae: 1.0492 - val_mse: 1.6550\n",
      "Epoch 167/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5150 - mae: 1.0134 - mse: 1.5150 - val_loss: 1.6709 - val_mae: 1.0683 - val_mse: 1.6709\n",
      "Epoch 168/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 1.5124 - mae: 1.0123 - mse: 1.5124 - val_loss: 1.6705 - val_mae: 1.0680 - val_mse: 1.6705\n",
      "Epoch 169/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5130 - mae: 1.0129 - mse: 1.5130 - val_loss: 1.6544 - val_mae: 1.0448 - val_mse: 1.6544\n",
      "Epoch 170/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 1.5141 - mae: 1.0122 - mse: 1.5141 - val_loss: 1.6584 - val_mae: 1.0333 - val_mse: 1.6584\n",
      "Epoch 171/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5120 - mae: 1.0101 - mse: 1.5120 - val_loss: 1.6589 - val_mae: 1.0570 - val_mse: 1.6589\n",
      "Epoch 172/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5113 - mae: 1.0124 - mse: 1.5113 - val_loss: 1.6588 - val_mae: 1.0568 - val_mse: 1.6588\n",
      "Epoch 173/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5163 - mae: 1.0146 - mse: 1.5163 - val_loss: 1.6557 - val_mae: 1.0513 - val_mse: 1.6557\n",
      "Epoch 174/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5176 - mae: 1.0136 - mse: 1.5176 - val_loss: 1.6654 - val_mae: 1.0640 - val_mse: 1.6654\n",
      "Epoch 175/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5158 - mae: 1.0113 - mse: 1.5158 - val_loss: 1.6563 - val_mae: 1.0369 - val_mse: 1.6563\n",
      "Epoch 176/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5167 - mae: 1.0125 - mse: 1.5167 - val_loss: 1.6646 - val_mae: 1.0633 - val_mse: 1.6646\n",
      "Epoch 177/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5160 - mae: 1.0142 - mse: 1.5160 - val_loss: 1.6544 - val_mae: 1.0446 - val_mse: 1.6544\n",
      "Epoch 178/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5163 - mae: 1.0114 - mse: 1.5163 - val_loss: 1.6766 - val_mae: 1.0721 - val_mse: 1.6766\n",
      "Epoch 179/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5177 - mae: 1.0152 - mse: 1.5177 - val_loss: 1.6640 - val_mae: 1.0627 - val_mse: 1.6640\n",
      "Epoch 180/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5153 - mae: 1.0117 - mse: 1.5153 - val_loss: 1.6564 - val_mae: 1.0366 - val_mse: 1.6564\n",
      "Epoch 181/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5142 - mae: 1.0116 - mse: 1.5142 - val_loss: 1.6876 - val_mae: 1.0782 - val_mse: 1.6876\n",
      "Epoch 182/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5193 - mae: 1.0143 - mse: 1.5193 - val_loss: 1.6544 - val_mae: 1.0454 - val_mse: 1.6544\n",
      "Epoch 183/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5133 - mae: 1.0095 - mse: 1.5133 - val_loss: 1.6647 - val_mae: 1.0634 - val_mse: 1.6647\n",
      "Epoch 184/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5145 - mae: 1.0124 - mse: 1.5145 - val_loss: 1.6568 - val_mae: 1.0538 - val_mse: 1.6568\n",
      "Epoch 185/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5137 - mae: 1.0145 - mse: 1.5137 - val_loss: 1.6556 - val_mae: 1.0385 - val_mse: 1.6556\n",
      "Epoch 186/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5179 - mae: 1.0133 - mse: 1.5179 - val_loss: 1.6582 - val_mae: 1.0561 - val_mse: 1.6582\n",
      "Epoch 187/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5085 - mae: 1.0100 - mse: 1.5085 - val_loss: 1.6555 - val_mae: 1.0507 - val_mse: 1.6555\n",
      "Epoch 188/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5216 - mae: 1.0155 - mse: 1.5216 - val_loss: 1.6545 - val_mae: 1.0458 - val_mse: 1.6545\n",
      "Epoch 189/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5136 - mae: 1.0114 - mse: 1.5136 - val_loss: 1.6579 - val_mae: 1.0556 - val_mse: 1.6579\n",
      "Epoch 190/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5147 - mae: 1.0114 - mse: 1.5147 - val_loss: 1.6606 - val_mae: 1.0592 - val_mse: 1.6606\n",
      "Epoch 191/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 83us/sample - loss: 1.5113 - mae: 1.0111 - mse: 1.5113 - val_loss: 1.6556 - val_mae: 1.0386 - val_mse: 1.6556\n",
      "Epoch 192/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5191 - mae: 1.0149 - mse: 1.5191 - val_loss: 1.6554 - val_mae: 1.0390 - val_mse: 1.6554\n",
      "Epoch 193/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 1.5121 - mae: 1.0095 - mse: 1.5121 - val_loss: 1.6837 - val_mae: 1.0761 - val_mse: 1.6837\n",
      "Epoch 194/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5163 - mae: 1.0139 - mse: 1.5163 - val_loss: 1.6548 - val_mae: 1.0481 - val_mse: 1.6548\n",
      "Epoch 195/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 1.5195 - mae: 1.0121 - mse: 1.5195 - val_loss: 1.6544 - val_mae: 1.0453 - val_mse: 1.6544\n",
      "Epoch 196/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5133 - mae: 1.0119 - mse: 1.5133 - val_loss: 1.6547 - val_mae: 1.0479 - val_mse: 1.6547\n",
      "Epoch 197/450\n",
      "2484/2484 [==============================] - 0s 91us/sample - loss: 1.5132 - mae: 1.0110 - mse: 1.5132 - val_loss: 1.7227 - val_mae: 1.0927 - val_mse: 1.7227\n",
      "Epoch 198/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 1.5176 - mae: 1.0141 - mse: 1.5176 - val_loss: 1.6829 - val_mae: 1.0757 - val_mse: 1.6829\n",
      "Epoch 199/450\n",
      "2484/2484 [==============================] - 0s 110us/sample - loss: 1.5131 - mae: 1.0136 - mse: 1.5131 - val_loss: 1.6605 - val_mae: 1.0305 - val_mse: 1.6605\n",
      "Epoch 200/450\n",
      "2484/2484 [==============================] - 0s 108us/sample - loss: 1.5125 - mae: 1.0098 - mse: 1.5125 - val_loss: 1.6672 - val_mae: 1.0655 - val_mse: 1.6672\n",
      "Epoch 201/450\n",
      "2484/2484 [==============================] - 0s 111us/sample - loss: 1.5166 - mae: 1.0107 - mse: 1.5166 - val_loss: 1.6549 - val_mae: 1.0409 - val_mse: 1.6549\n",
      "Epoch 202/450\n",
      "2484/2484 [==============================] - 0s 107us/sample - loss: 1.5083 - mae: 1.0103 - mse: 1.5083 - val_loss: 1.6549 - val_mae: 1.0406 - val_mse: 1.6549\n",
      "Epoch 203/450\n",
      "2484/2484 [==============================] - 0s 118us/sample - loss: 1.5152 - mae: 1.0127 - mse: 1.5152 - val_loss: 1.6571 - val_mae: 1.0353 - val_mse: 1.6571\n",
      "Epoch 204/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 1.5125 - mae: 1.0115 - mse: 1.5125 - val_loss: 1.6834 - val_mae: 1.0136 - val_mse: 1.6834\n",
      "Epoch 205/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 1.5147 - mae: 1.0113 - mse: 1.5147 - val_loss: 1.6898 - val_mae: 1.0103 - val_mse: 1.6898\n",
      "Epoch 206/450\n",
      "2484/2484 [==============================] - 0s 94us/sample - loss: 1.5166 - mae: 1.0112 - mse: 1.5166 - val_loss: 1.6638 - val_mae: 1.0271 - val_mse: 1.6638\n",
      "Epoch 207/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 1.5102 - mae: 1.0102 - mse: 1.5102 - val_loss: 1.6545 - val_mae: 1.0465 - val_mse: 1.6545\n",
      "Epoch 208/450\n",
      "2484/2484 [==============================] - 0s 101us/sample - loss: 1.5182 - mae: 1.0120 - mse: 1.5182 - val_loss: 1.6569 - val_mae: 1.0357 - val_mse: 1.6569\n",
      "Epoch 209/450\n",
      "2484/2484 [==============================] - 0s 87us/sample - loss: 1.5076 - mae: 1.0110 - mse: 1.5076 - val_loss: 1.7166 - val_mae: 0.9991 - val_mse: 1.7166\n",
      "Epoch 210/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 1.5171 - mae: 1.0071 - mse: 1.5171 - val_loss: 1.6913 - val_mae: 1.0800 - val_mse: 1.6913\n",
      "Epoch 211/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5204 - mae: 1.0151 - mse: 1.5204 - val_loss: 1.6555 - val_mae: 1.0508 - val_mse: 1.6555\n",
      "Epoch 212/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5141 - mae: 1.0136 - mse: 1.5141 - val_loss: 1.6907 - val_mae: 1.0099 - val_mse: 1.6907\n",
      "Epoch 213/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5228 - mae: 1.0135 - mse: 1.5228 - val_loss: 1.6544 - val_mae: 1.0442 - val_mse: 1.6544\n",
      "Epoch 214/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5120 - mae: 1.0119 - mse: 1.5120 - val_loss: 1.6604 - val_mae: 1.0307 - val_mse: 1.6604\n",
      "Epoch 215/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5170 - mae: 1.0120 - mse: 1.5170 - val_loss: 1.6655 - val_mae: 1.0255 - val_mse: 1.6655\n",
      "Epoch 216/450\n",
      "2484/2484 [==============================] - 0s 76us/sample - loss: 1.5125 - mae: 1.0090 - mse: 1.5125 - val_loss: 1.6567 - val_mae: 1.0361 - val_mse: 1.6567\n",
      "Epoch 217/450\n",
      "2484/2484 [==============================] - 0s 83us/sample - loss: 1.5105 - mae: 1.0121 - mse: 1.5105 - val_loss: 1.7288 - val_mae: 0.9948 - val_mse: 1.7288\n",
      "Epoch 218/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5108 - mae: 1.0035 - mse: 1.5108 - val_loss: 1.6573 - val_mae: 1.0546 - val_mse: 1.6573\n",
      "Epoch 219/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 1.5121 - mae: 1.0108 - mse: 1.5121 - val_loss: 1.6556 - val_mae: 1.0511 - val_mse: 1.6556\n",
      "Epoch 220/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 1.5138 - mae: 1.0110 - mse: 1.5138 - val_loss: 1.6765 - val_mae: 1.0720 - val_mse: 1.6765\n",
      "Epoch 221/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5108 - mae: 1.0105 - mse: 1.5108 - val_loss: 1.6698 - val_mae: 1.0675 - val_mse: 1.6698\n",
      "Epoch 222/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5112 - mae: 1.0107 - mse: 1.5112 - val_loss: 1.6558 - val_mae: 1.0516 - val_mse: 1.6558\n",
      "Epoch 223/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5165 - mae: 1.0136 - mse: 1.5165 - val_loss: 1.6717 - val_mae: 1.0207 - val_mse: 1.6717\n",
      "Epoch 224/450\n",
      "2484/2484 [==============================] - 0s 67us/sample - loss: 1.5154 - mae: 1.0096 - mse: 1.5154 - val_loss: 1.6590 - val_mae: 1.0323 - val_mse: 1.6590\n",
      "Epoch 225/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5129 - mae: 1.0105 - mse: 1.5129 - val_loss: 1.6544 - val_mae: 1.0451 - val_mse: 1.6544\n",
      "Epoch 226/450\n",
      "2484/2484 [==============================] - 0s 84us/sample - loss: 1.5150 - mae: 1.0127 - mse: 1.5150 - val_loss: 1.6574 - val_mae: 1.0347 - val_mse: 1.6574\n",
      "Epoch 227/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5130 - mae: 1.0118 - mse: 1.5130 - val_loss: 1.6584 - val_mae: 1.0563 - val_mse: 1.6584\n",
      "Epoch 228/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5193 - mae: 1.0154 - mse: 1.5193 - val_loss: 1.6545 - val_mae: 1.0438 - val_mse: 1.6545\n",
      "Epoch 229/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5156 - mae: 1.0128 - mse: 1.5156 - val_loss: 1.6680 - val_mae: 1.0662 - val_mse: 1.6680\n",
      "Epoch 230/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5108 - mae: 1.0118 - mse: 1.5108 - val_loss: 1.6704 - val_mae: 1.0216 - val_mse: 1.6704\n",
      "Epoch 231/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5223 - mae: 1.0132 - mse: 1.5223 - val_loss: 1.6556 - val_mae: 1.0510 - val_mse: 1.6556\n",
      "Epoch 232/450\n",
      "2484/2484 [==============================] - 0s 75us/sample - loss: 1.5138 - mae: 1.0096 - mse: 1.5138 - val_loss: 1.6972 - val_mae: 1.0827 - val_mse: 1.6972\n",
      "Epoch 233/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5149 - mae: 1.0155 - mse: 1.5149 - val_loss: 1.6549 - val_mae: 1.0408 - val_mse: 1.6549\n",
      "Epoch 234/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5128 - mae: 1.0098 - mse: 1.5128 - val_loss: 1.6566 - val_mae: 1.0532 - val_mse: 1.6566\n",
      "Epoch 235/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5175 - mae: 1.0109 - mse: 1.5175 - val_loss: 1.6570 - val_mae: 1.0541 - val_mse: 1.6570\n",
      "Epoch 236/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 1.5160 - mae: 1.0120 - mse: 1.5160 - val_loss: 1.6554 - val_mae: 1.0390 - val_mse: 1.6554\n",
      "Epoch 237/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5173 - mae: 1.0108 - mse: 1.5173 - val_loss: 1.6590 - val_mae: 1.0324 - val_mse: 1.6590\n",
      "Epoch 238/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5151 - mae: 1.0113 - mse: 1.5151 - val_loss: 1.6587 - val_mae: 1.0568 - val_mse: 1.6587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5162 - mae: 1.0117 - mse: 1.5162 - val_loss: 1.6880 - val_mae: 1.0784 - val_mse: 1.6880\n",
      "Epoch 240/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5188 - mae: 1.0148 - mse: 1.5188 - val_loss: 1.6556 - val_mae: 1.0511 - val_mse: 1.6556\n",
      "Epoch 241/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5156 - mae: 1.0120 - mse: 1.5156 - val_loss: 1.6833 - val_mae: 1.0759 - val_mse: 1.6833\n",
      "Epoch 242/450\n",
      "2484/2484 [==============================] - 0s 69us/sample - loss: 1.5190 - mae: 1.0145 - mse: 1.5190 - val_loss: 1.6544 - val_mae: 1.0452 - val_mse: 1.6544\n",
      "Epoch 243/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5135 - mae: 1.0106 - mse: 1.5135 - val_loss: 1.6586 - val_mae: 1.0329 - val_mse: 1.6586\n",
      "Epoch 244/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5173 - mae: 1.0125 - mse: 1.5173 - val_loss: 1.6546 - val_mae: 1.0422 - val_mse: 1.6546\n",
      "Epoch 245/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5124 - mae: 1.0116 - mse: 1.5124 - val_loss: 1.6740 - val_mae: 1.0704 - val_mse: 1.6740\n",
      "Epoch 246/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5147 - mae: 1.0134 - mse: 1.5147 - val_loss: 1.6555 - val_mae: 1.0389 - val_mse: 1.6555\n",
      "Epoch 247/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5146 - mae: 1.0115 - mse: 1.5146 - val_loss: 1.6691 - val_mae: 1.0669 - val_mse: 1.6691\n",
      "Epoch 248/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5153 - mae: 1.0129 - mse: 1.5153 - val_loss: 1.6549 - val_mae: 1.0489 - val_mse: 1.6549\n",
      "Epoch 249/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5162 - mae: 1.0119 - mse: 1.5162 - val_loss: 1.6555 - val_mae: 1.0506 - val_mse: 1.6555\n",
      "Epoch 250/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5115 - mae: 1.0120 - mse: 1.5115 - val_loss: 1.6659 - val_mae: 1.0644 - val_mse: 1.6659\n",
      "Epoch 251/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5133 - mae: 1.0112 - mse: 1.5133 - val_loss: 1.6621 - val_mae: 1.0608 - val_mse: 1.6621\n",
      "Epoch 252/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5123 - mae: 1.0086 - mse: 1.5123 - val_loss: 1.6549 - val_mae: 1.0409 - val_mse: 1.6549\n",
      "Epoch 253/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5161 - mae: 1.0111 - mse: 1.5161 - val_loss: 1.6547 - val_mae: 1.0417 - val_mse: 1.6547\n",
      "Epoch 254/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5166 - mae: 1.0128 - mse: 1.5166 - val_loss: 1.7068 - val_mae: 1.0867 - val_mse: 1.7068\n",
      "Epoch 255/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5143 - mae: 1.0135 - mse: 1.5143 - val_loss: 1.6590 - val_mae: 1.0324 - val_mse: 1.6590\n",
      "Epoch 256/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5168 - mae: 1.0120 - mse: 1.5168 - val_loss: 1.6569 - val_mae: 1.0357 - val_mse: 1.6569\n",
      "Epoch 257/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5112 - mae: 1.0106 - mse: 1.5112 - val_loss: 1.6606 - val_mae: 1.0304 - val_mse: 1.6606\n",
      "Epoch 258/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5117 - mae: 1.0092 - mse: 1.5117 - val_loss: 1.6919 - val_mae: 1.0803 - val_mse: 1.6919\n",
      "Epoch 259/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5196 - mae: 1.0131 - mse: 1.5196 - val_loss: 1.6837 - val_mae: 1.0762 - val_mse: 1.6837\n",
      "Epoch 260/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5145 - mae: 1.0130 - mse: 1.5145 - val_loss: 1.6600 - val_mae: 1.0584 - val_mse: 1.6600\n",
      "Epoch 261/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5168 - mae: 1.0118 - mse: 1.5168 - val_loss: 1.6900 - val_mae: 1.0794 - val_mse: 1.6900\n",
      "Epoch 262/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5202 - mae: 1.0164 - mse: 1.5202 - val_loss: 1.6718 - val_mae: 1.0206 - val_mse: 1.6718\n",
      "Epoch 263/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5110 - mae: 1.0074 - mse: 1.5110 - val_loss: 1.6570 - val_mae: 1.0354 - val_mse: 1.6570\n",
      "Epoch 264/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5171 - mae: 1.0103 - mse: 1.5171 - val_loss: 1.6617 - val_mae: 1.0291 - val_mse: 1.6617\n",
      "Epoch 265/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5155 - mae: 1.0115 - mse: 1.5155 - val_loss: 1.6719 - val_mae: 1.0690 - val_mse: 1.6719\n",
      "Epoch 266/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5082 - mae: 1.0110 - mse: 1.5082 - val_loss: 1.6728 - val_mae: 1.0199 - val_mse: 1.6728\n",
      "Epoch 267/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5140 - mae: 1.0111 - mse: 1.5140 - val_loss: 1.6769 - val_mae: 1.0173 - val_mse: 1.6769\n",
      "Epoch 268/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 1.5130 - mae: 1.0091 - mse: 1.5130 - val_loss: 1.6602 - val_mae: 1.0309 - val_mse: 1.6602\n",
      "Epoch 269/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5068 - mae: 1.0078 - mse: 1.5068 - val_loss: 1.6547 - val_mae: 1.0418 - val_mse: 1.6547\n",
      "Epoch 270/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 1.5117 - mae: 1.0091 - mse: 1.5117 - val_loss: 1.6586 - val_mae: 1.0566 - val_mse: 1.6586\n",
      "Epoch 271/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5101 - mae: 1.0066 - mse: 1.5101 - val_loss: 1.6545 - val_mae: 1.0465 - val_mse: 1.6545\n",
      "Epoch 272/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5139 - mae: 1.0115 - mse: 1.5139 - val_loss: 1.6613 - val_mae: 1.0600 - val_mse: 1.6613\n",
      "Epoch 273/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5163 - mae: 1.0139 - mse: 1.5163 - val_loss: 1.6544 - val_mae: 1.0446 - val_mse: 1.6544\n",
      "Epoch 274/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5140 - mae: 1.0119 - mse: 1.5140 - val_loss: 1.6559 - val_mae: 1.0378 - val_mse: 1.6559\n",
      "Epoch 275/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5144 - mae: 1.0115 - mse: 1.5144 - val_loss: 1.6958 - val_mae: 1.0820 - val_mse: 1.6958\n",
      "Epoch 276/450\n",
      "2484/2484 [==============================] - 0s 85us/sample - loss: 1.5114 - mae: 1.0111 - mse: 1.5114 - val_loss: 1.6568 - val_mae: 1.0536 - val_mse: 1.6568\n",
      "Epoch 277/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5188 - mae: 1.0134 - mse: 1.5188 - val_loss: 1.6556 - val_mae: 1.0386 - val_mse: 1.6556\n",
      "Epoch 278/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5096 - mae: 1.0103 - mse: 1.5096 - val_loss: 1.6954 - val_mae: 1.0077 - val_mse: 1.6954\n",
      "Epoch 279/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5230 - mae: 1.0125 - mse: 1.5230 - val_loss: 1.6553 - val_mae: 1.0395 - val_mse: 1.6553\n",
      "Epoch 280/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5094 - mae: 1.0122 - mse: 1.5094 - val_loss: 1.6731 - val_mae: 1.0198 - val_mse: 1.6731\n",
      "Epoch 281/450\n",
      "2484/2484 [==============================] - 0s 37us/sample - loss: 1.5216 - mae: 1.0096 - mse: 1.5216 - val_loss: 1.6674 - val_mae: 1.0657 - val_mse: 1.6674\n",
      "Epoch 282/450\n",
      "2484/2484 [==============================] - 0s 53us/sample - loss: 1.5096 - mae: 1.0126 - mse: 1.5096 - val_loss: 1.6572 - val_mae: 1.0351 - val_mse: 1.6572\n",
      "Epoch 283/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5134 - mae: 1.0099 - mse: 1.5134 - val_loss: 1.6545 - val_mae: 1.0466 - val_mse: 1.6545\n",
      "Epoch 284/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5095 - mae: 1.0106 - mse: 1.5095 - val_loss: 1.6554 - val_mae: 1.0504 - val_mse: 1.6554\n",
      "Epoch 285/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5122 - mae: 1.0107 - mse: 1.5122 - val_loss: 1.6556 - val_mae: 1.0386 - val_mse: 1.6556\n",
      "Epoch 286/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 71us/sample - loss: 1.5166 - mae: 1.0122 - mse: 1.5166 - val_loss: 1.6563 - val_mae: 1.0369 - val_mse: 1.6563\n",
      "Epoch 287/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5213 - mae: 1.0121 - mse: 1.5213 - val_loss: 1.6577 - val_mae: 1.0553 - val_mse: 1.6577\n",
      "Epoch 288/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5178 - mae: 1.0127 - mse: 1.5178 - val_loss: 1.6560 - val_mae: 1.0375 - val_mse: 1.6560\n",
      "Epoch 289/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5174 - mae: 1.0127 - mse: 1.5174 - val_loss: 1.6545 - val_mae: 1.0465 - val_mse: 1.6545\n",
      "Epoch 290/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5123 - mae: 1.0103 - mse: 1.5123 - val_loss: 1.6544 - val_mae: 1.0441 - val_mse: 1.6544\n",
      "Epoch 291/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5180 - mae: 1.0120 - mse: 1.5180 - val_loss: 1.6552 - val_mae: 1.0498 - val_mse: 1.6552\n",
      "Epoch 292/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5169 - mae: 1.0163 - mse: 1.5169 - val_loss: 1.6545 - val_mae: 1.0436 - val_mse: 1.6545\n",
      "Epoch 293/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5133 - mae: 1.0117 - mse: 1.5133 - val_loss: 1.6562 - val_mae: 1.0525 - val_mse: 1.6562\n",
      "Epoch 294/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5142 - mae: 1.0099 - mse: 1.5142 - val_loss: 1.6547 - val_mae: 1.0480 - val_mse: 1.6547\n",
      "Epoch 295/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5140 - mae: 1.0118 - mse: 1.5140 - val_loss: 1.6576 - val_mae: 1.0552 - val_mse: 1.6576\n",
      "Epoch 296/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5131 - mae: 1.0114 - mse: 1.5131 - val_loss: 1.6820 - val_mae: 1.0752 - val_mse: 1.6820\n",
      "Epoch 297/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5160 - mae: 1.0120 - mse: 1.5160 - val_loss: 1.7189 - val_mae: 1.0913 - val_mse: 1.7189\n",
      "Epoch 298/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5134 - mae: 1.0108 - mse: 1.5134 - val_loss: 1.7267 - val_mae: 1.0940 - val_mse: 1.7267\n",
      "Epoch 299/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5153 - mae: 1.0116 - mse: 1.5153 - val_loss: 1.6834 - val_mae: 1.0760 - val_mse: 1.6834\n",
      "Epoch 300/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5162 - mae: 1.0137 - mse: 1.5162 - val_loss: 1.6544 - val_mae: 1.0452 - val_mse: 1.6544\n",
      "Epoch 301/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5191 - mae: 1.0130 - mse: 1.5191 - val_loss: 1.6620 - val_mae: 1.0288 - val_mse: 1.6620\n",
      "Epoch 302/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5112 - mae: 1.0103 - mse: 1.5112 - val_loss: 1.6700 - val_mae: 1.0676 - val_mse: 1.6700\n",
      "Epoch 303/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5130 - mae: 1.0127 - mse: 1.5130 - val_loss: 1.6859 - val_mae: 1.0123 - val_mse: 1.6859\n",
      "Epoch 304/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5214 - mae: 1.0106 - mse: 1.5214 - val_loss: 1.6602 - val_mae: 1.0587 - val_mse: 1.6602\n",
      "Epoch 305/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 1.5134 - mae: 1.0123 - mse: 1.5134 - val_loss: 1.6797 - val_mae: 1.0156 - val_mse: 1.6797\n",
      "Epoch 306/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5153 - mae: 1.0110 - mse: 1.5153 - val_loss: 1.6544 - val_mae: 1.0455 - val_mse: 1.6544\n",
      "Epoch 307/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5178 - mae: 1.0120 - mse: 1.5178 - val_loss: 1.6636 - val_mae: 1.0623 - val_mse: 1.6636\n",
      "Epoch 308/450\n",
      "2484/2484 [==============================] - 0s 31us/sample - loss: 1.5162 - mae: 1.0136 - mse: 1.5162 - val_loss: 1.6545 - val_mae: 1.0467 - val_mse: 1.6545\n",
      "Epoch 309/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5135 - mae: 1.0121 - mse: 1.5135 - val_loss: 1.6553 - val_mae: 1.0503 - val_mse: 1.6553\n",
      "Epoch 310/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5139 - mae: 1.0143 - mse: 1.5139 - val_loss: 1.6557 - val_mae: 1.0514 - val_mse: 1.6557\n",
      "Epoch 311/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5134 - mae: 1.0112 - mse: 1.5134 - val_loss: 1.6567 - val_mae: 1.0535 - val_mse: 1.6567\n",
      "Epoch 312/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5128 - mae: 1.0109 - mse: 1.5128 - val_loss: 1.6546 - val_mae: 1.0422 - val_mse: 1.6546\n",
      "Epoch 313/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5145 - mae: 1.0104 - mse: 1.5145 - val_loss: 1.6635 - val_mae: 1.0623 - val_mse: 1.6635\n",
      "Epoch 314/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5143 - mae: 1.0122 - mse: 1.5143 - val_loss: 1.6904 - val_mae: 1.0795 - val_mse: 1.6904\n",
      "Epoch 315/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5134 - mae: 1.0150 - mse: 1.5134 - val_loss: 1.6664 - val_mae: 1.0649 - val_mse: 1.6664\n",
      "Epoch 316/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5184 - mae: 1.0141 - mse: 1.5184 - val_loss: 1.6557 - val_mae: 1.0514 - val_mse: 1.6557\n",
      "Epoch 317/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5189 - mae: 1.0136 - mse: 1.5189 - val_loss: 1.6547 - val_mae: 1.0418 - val_mse: 1.6547\n",
      "Epoch 318/450\n",
      "2484/2484 [==============================] - 0s 31us/sample - loss: 1.5120 - mae: 1.0061 - mse: 1.5120 - val_loss: 1.6571 - val_mae: 1.0354 - val_mse: 1.6571\n",
      "Epoch 319/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5120 - mae: 1.0113 - mse: 1.5120 - val_loss: 1.6695 - val_mae: 1.0223 - val_mse: 1.6695\n",
      "Epoch 320/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5229 - mae: 1.0109 - mse: 1.5229 - val_loss: 1.6558 - val_mae: 1.0516 - val_mse: 1.6558\n",
      "Epoch 321/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5105 - mae: 1.0103 - mse: 1.5105 - val_loss: 1.6932 - val_mae: 1.0809 - val_mse: 1.6932\n",
      "Epoch 322/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5167 - mae: 1.0109 - mse: 1.5167 - val_loss: 1.6545 - val_mae: 1.0458 - val_mse: 1.6545\n",
      "Epoch 323/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 1.5196 - mae: 1.0116 - mse: 1.5196 - val_loss: 1.6545 - val_mae: 1.0466 - val_mse: 1.6545\n",
      "Epoch 324/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5133 - mae: 1.0125 - mse: 1.5133 - val_loss: 1.6552 - val_mae: 1.0396 - val_mse: 1.6552\n",
      "Epoch 325/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5146 - mae: 1.0119 - mse: 1.5146 - val_loss: 1.6737 - val_mae: 1.0702 - val_mse: 1.6737\n",
      "Epoch 326/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5110 - mae: 1.0100 - mse: 1.5110 - val_loss: 1.6746 - val_mae: 1.0187 - val_mse: 1.6746\n",
      "Epoch 327/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5183 - mae: 1.0108 - mse: 1.5183 - val_loss: 1.6548 - val_mae: 1.0484 - val_mse: 1.6548\n",
      "Epoch 328/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5105 - mae: 1.0090 - mse: 1.5105 - val_loss: 1.6626 - val_mae: 1.0282 - val_mse: 1.6626\n",
      "Epoch 329/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5126 - mae: 1.0098 - mse: 1.5126 - val_loss: 1.6582 - val_mae: 1.0560 - val_mse: 1.6582\n",
      "Epoch 330/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5144 - mae: 1.0100 - mse: 1.5144 - val_loss: 1.6609 - val_mae: 1.0596 - val_mse: 1.6609\n",
      "Epoch 331/450\n",
      "2484/2484 [==============================] - 0s 37us/sample - loss: 1.5152 - mae: 1.0109 - mse: 1.5152 - val_loss: 1.6730 - val_mae: 1.0698 - val_mse: 1.6730\n",
      "Epoch 332/450\n",
      "2484/2484 [==============================] - 0s 37us/sample - loss: 1.5137 - mae: 1.0131 - mse: 1.5137 - val_loss: 1.6660 - val_mae: 1.0251 - val_mse: 1.6660\n",
      "Epoch 333/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5119 - mae: 1.0094 - mse: 1.5119 - val_loss: 1.7019 - val_mae: 1.0847 - val_mse: 1.7019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5178 - mae: 1.0131 - mse: 1.5178 - val_loss: 1.6576 - val_mae: 1.0344 - val_mse: 1.6576\n",
      "Epoch 335/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5143 - mae: 1.0101 - mse: 1.5143 - val_loss: 1.7039 - val_mae: 1.0856 - val_mse: 1.7039\n",
      "Epoch 336/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5158 - mae: 1.0162 - mse: 1.5158 - val_loss: 1.6818 - val_mae: 1.0145 - val_mse: 1.6818\n",
      "Epoch 337/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5159 - mae: 1.0099 - mse: 1.5159 - val_loss: 1.6786 - val_mae: 1.0163 - val_mse: 1.6786\n",
      "Epoch 338/450\n",
      "2484/2484 [==============================] - 0s 37us/sample - loss: 1.5169 - mae: 1.0121 - mse: 1.5169 - val_loss: 1.6584 - val_mae: 1.0564 - val_mse: 1.6584\n",
      "Epoch 339/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5116 - mae: 1.0122 - mse: 1.5116 - val_loss: 1.6689 - val_mae: 1.0227 - val_mse: 1.6689\n",
      "Epoch 340/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5163 - mae: 1.0100 - mse: 1.5163 - val_loss: 1.6779 - val_mae: 1.0167 - val_mse: 1.6779\n",
      "Epoch 341/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 1.5149 - mae: 1.0091 - mse: 1.5149 - val_loss: 1.6576 - val_mae: 1.0551 - val_mse: 1.6576\n",
      "Epoch 342/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5154 - mae: 1.0130 - mse: 1.5154 - val_loss: 1.6634 - val_mae: 1.0274 - val_mse: 1.6634\n",
      "Epoch 343/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5137 - mae: 1.0095 - mse: 1.5137 - val_loss: 1.6601 - val_mae: 1.0310 - val_mse: 1.6601\n",
      "Epoch 344/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5143 - mae: 1.0096 - mse: 1.5143 - val_loss: 1.6553 - val_mae: 1.0394 - val_mse: 1.6553\n",
      "Epoch 345/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5103 - mae: 1.0101 - mse: 1.5103 - val_loss: 1.6664 - val_mae: 1.0247 - val_mse: 1.6664\n",
      "Epoch 346/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5098 - mae: 1.0107 - mse: 1.5098 - val_loss: 1.6813 - val_mae: 1.0147 - val_mse: 1.6813\n",
      "Epoch 347/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5169 - mae: 1.0124 - mse: 1.5169 - val_loss: 1.6971 - val_mae: 1.0827 - val_mse: 1.6971\n",
      "Epoch 348/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5112 - mae: 1.0085 - mse: 1.5112 - val_loss: 1.6638 - val_mae: 1.0625 - val_mse: 1.6638\n",
      "Epoch 349/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5121 - mae: 1.0118 - mse: 1.5121 - val_loss: 1.6548 - val_mae: 1.0483 - val_mse: 1.6548\n",
      "Epoch 350/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5180 - mae: 1.0121 - mse: 1.5180 - val_loss: 1.6904 - val_mae: 1.0795 - val_mse: 1.6904\n",
      "Epoch 351/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5175 - mae: 1.0154 - mse: 1.5175 - val_loss: 1.6615 - val_mae: 1.0294 - val_mse: 1.6615\n",
      "Epoch 352/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5134 - mae: 1.0105 - mse: 1.5134 - val_loss: 1.6550 - val_mae: 1.0492 - val_mse: 1.6550\n",
      "Epoch 353/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5147 - mae: 1.0133 - mse: 1.5147 - val_loss: 1.6617 - val_mae: 1.0291 - val_mse: 1.6617\n",
      "Epoch 354/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5106 - mae: 1.0083 - mse: 1.5106 - val_loss: 1.6547 - val_mae: 1.0478 - val_mse: 1.6547\n",
      "Epoch 355/450\n",
      "2484/2484 [==============================] - 0s 99us/sample - loss: 1.5131 - mae: 1.0113 - mse: 1.5131 - val_loss: 1.6599 - val_mae: 1.0584 - val_mse: 1.6599\n",
      "Epoch 356/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5112 - mae: 1.0103 - mse: 1.5112 - val_loss: 1.6610 - val_mae: 1.0596 - val_mse: 1.6610\n",
      "Epoch 357/450\n",
      "2484/2484 [==============================] - 0s 65us/sample - loss: 1.5166 - mae: 1.0124 - mse: 1.5166 - val_loss: 1.6577 - val_mae: 1.0553 - val_mse: 1.6577\n",
      "Epoch 358/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5176 - mae: 1.0136 - mse: 1.5176 - val_loss: 1.6740 - val_mae: 1.0704 - val_mse: 1.6740\n",
      "Epoch 359/450\n",
      "2484/2484 [==============================] - 0s 61us/sample - loss: 1.5149 - mae: 1.0133 - mse: 1.5149 - val_loss: 1.6574 - val_mae: 1.0348 - val_mse: 1.6574\n",
      "Epoch 360/450\n",
      "2484/2484 [==============================] - 0s 90us/sample - loss: 1.5124 - mae: 1.0107 - mse: 1.5124 - val_loss: 1.6581 - val_mae: 1.0336 - val_mse: 1.6581\n",
      "Epoch 361/450\n",
      "2484/2484 [==============================] - 0s 78us/sample - loss: 1.5131 - mae: 1.0108 - mse: 1.5131 - val_loss: 1.6958 - val_mae: 1.0821 - val_mse: 1.6958\n",
      "Epoch 362/450\n",
      "2484/2484 [==============================] - 0s 63us/sample - loss: 1.5170 - mae: 1.0117 - mse: 1.5170 - val_loss: 1.6594 - val_mae: 1.0576 - val_mse: 1.6594\n",
      "Epoch 363/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5150 - mae: 1.0131 - mse: 1.5150 - val_loss: 1.6577 - val_mae: 1.0553 - val_mse: 1.6577\n",
      "Epoch 364/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5186 - mae: 1.0139 - mse: 1.5186 - val_loss: 1.6576 - val_mae: 1.0345 - val_mse: 1.6576\n",
      "Epoch 365/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5148 - mae: 1.0125 - mse: 1.5148 - val_loss: 1.6703 - val_mae: 1.0217 - val_mse: 1.6703\n",
      "Epoch 366/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5132 - mae: 1.0100 - mse: 1.5132 - val_loss: 1.6582 - val_mae: 1.0560 - val_mse: 1.6582\n",
      "Epoch 367/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5065 - mae: 1.0064 - mse: 1.5065 - val_loss: 1.7702 - val_mae: 1.1071 - val_mse: 1.7702\n",
      "Epoch 368/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5210 - mae: 1.0138 - mse: 1.5210 - val_loss: 1.6548 - val_mae: 1.0413 - val_mse: 1.6548\n",
      "Epoch 369/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5127 - mae: 1.0104 - mse: 1.5127 - val_loss: 1.6572 - val_mae: 1.0352 - val_mse: 1.6572\n",
      "Epoch 370/450\n",
      "2484/2484 [==============================] - 0s 73us/sample - loss: 1.5117 - mae: 1.0104 - mse: 1.5117 - val_loss: 1.6551 - val_mae: 1.0494 - val_mse: 1.6551\n",
      "Epoch 371/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5104 - mae: 1.0103 - mse: 1.5104 - val_loss: 1.6692 - val_mae: 1.0671 - val_mse: 1.6692\n",
      "Epoch 372/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5159 - mae: 1.0121 - mse: 1.5159 - val_loss: 1.6766 - val_mae: 1.0721 - val_mse: 1.6766\n",
      "Epoch 373/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5112 - mae: 1.0104 - mse: 1.5112 - val_loss: 1.7639 - val_mae: 1.1054 - val_mse: 1.7639\n",
      "Epoch 374/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5188 - mae: 1.0159 - mse: 1.5188 - val_loss: 1.6600 - val_mae: 1.0584 - val_mse: 1.6600\n",
      "Epoch 375/450\n",
      "2484/2484 [==============================] - 0s 49us/sample - loss: 1.5172 - mae: 1.0132 - mse: 1.5172 - val_loss: 1.6547 - val_mae: 1.0476 - val_mse: 1.6547\n",
      "Epoch 376/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5174 - mae: 1.0136 - mse: 1.5174 - val_loss: 1.6656 - val_mae: 1.0254 - val_mse: 1.6656\n",
      "Epoch 377/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5149 - mae: 1.0103 - mse: 1.5149 - val_loss: 1.6715 - val_mae: 1.0687 - val_mse: 1.6715\n",
      "Epoch 378/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5154 - mae: 1.0131 - mse: 1.5154 - val_loss: 1.6562 - val_mae: 1.0371 - val_mse: 1.6562\n",
      "Epoch 379/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5172 - mae: 1.0126 - mse: 1.5172 - val_loss: 1.6641 - val_mae: 1.0267 - val_mse: 1.6641\n",
      "Epoch 380/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5135 - mae: 1.0097 - mse: 1.5135 - val_loss: 1.6717 - val_mae: 1.0689 - val_mse: 1.6717\n",
      "Epoch 381/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5171 - mae: 1.0087 - mse: 1.5171 - val_loss: 1.6609 - val_mae: 1.0595 - val_mse: 1.6609\n",
      "Epoch 382/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5117 - mae: 1.0117 - mse: 1.5117 - val_loss: 1.6553 - val_mae: 1.0393 - val_mse: 1.6553\n",
      "Epoch 383/450\n",
      "2484/2484 [==============================] - 0s 45us/sample - loss: 1.5165 - mae: 1.0121 - mse: 1.5165 - val_loss: 1.6546 - val_mae: 1.0473 - val_mse: 1.6546\n",
      "Epoch 384/450\n",
      "2484/2484 [==============================] - 0s 48us/sample - loss: 1.5143 - mae: 1.0118 - mse: 1.5143 - val_loss: 1.6553 - val_mae: 1.0393 - val_mse: 1.6553\n",
      "Epoch 385/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5129 - mae: 1.0119 - mse: 1.5129 - val_loss: 1.6704 - val_mae: 1.0216 - val_mse: 1.6704\n",
      "Epoch 386/450\n",
      "2484/2484 [==============================] - 0s 52us/sample - loss: 1.5139 - mae: 1.0082 - mse: 1.5139 - val_loss: 1.7076 - val_mae: 1.0871 - val_mse: 1.7076\n",
      "Epoch 387/450\n",
      "2484/2484 [==============================] - 0s 57us/sample - loss: 1.5172 - mae: 1.0141 - mse: 1.5172 - val_loss: 1.6545 - val_mae: 1.0462 - val_mse: 1.6545\n",
      "Epoch 388/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5132 - mae: 1.0118 - mse: 1.5132 - val_loss: 1.7102 - val_mae: 1.0881 - val_mse: 1.7102\n",
      "Epoch 389/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5185 - mae: 1.0175 - mse: 1.5185 - val_loss: 1.6546 - val_mae: 1.0473 - val_mse: 1.6546\n",
      "Epoch 390/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5138 - mae: 1.0116 - mse: 1.5138 - val_loss: 1.6627 - val_mae: 1.0614 - val_mse: 1.6627\n",
      "Epoch 391/450\n",
      "2484/2484 [==============================] - 0s 71us/sample - loss: 1.5168 - mae: 1.0133 - mse: 1.5168 - val_loss: 1.6912 - val_mae: 1.0799 - val_mse: 1.6912\n",
      "Epoch 392/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5149 - mae: 1.0120 - mse: 1.5149 - val_loss: 1.6550 - val_mae: 1.0492 - val_mse: 1.6550\n",
      "Epoch 393/450\n",
      "2484/2484 [==============================] - 0s 77us/sample - loss: 1.5152 - mae: 1.0128 - mse: 1.5152 - val_loss: 1.6761 - val_mae: 1.0178 - val_mse: 1.6761\n",
      "Epoch 394/450\n",
      "2484/2484 [==============================] - 0s 64us/sample - loss: 1.5167 - mae: 1.0093 - mse: 1.5167 - val_loss: 1.6579 - val_mae: 1.0555 - val_mse: 1.6579\n",
      "Epoch 395/450\n",
      "2484/2484 [==============================] - 0s 66us/sample - loss: 1.5133 - mae: 1.0116 - mse: 1.5133 - val_loss: 1.7113 - val_mae: 1.0885 - val_mse: 1.7113\n",
      "Epoch 396/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5135 - mae: 1.0107 - mse: 1.5135 - val_loss: 1.6961 - val_mae: 1.0822 - val_mse: 1.6961\n",
      "Epoch 397/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5189 - mae: 1.0156 - mse: 1.5189 - val_loss: 1.6558 - val_mae: 1.0380 - val_mse: 1.6558\n",
      "Epoch 398/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5156 - mae: 1.0128 - mse: 1.5156 - val_loss: 1.6614 - val_mae: 1.0295 - val_mse: 1.6614\n",
      "Epoch 399/450\n",
      "2484/2484 [==============================] - 0s 72us/sample - loss: 1.5132 - mae: 1.0085 - mse: 1.5132 - val_loss: 1.7114 - val_mae: 1.0885 - val_mse: 1.7114\n",
      "Epoch 400/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5174 - mae: 1.0125 - mse: 1.5174 - val_loss: 1.6589 - val_mae: 1.0326 - val_mse: 1.6589\n",
      "Epoch 401/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5083 - mae: 1.0082 - mse: 1.5083 - val_loss: 1.6649 - val_mae: 1.0635 - val_mse: 1.6649\n",
      "Epoch 402/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5180 - mae: 1.0144 - mse: 1.5180 - val_loss: 1.6763 - val_mae: 1.0177 - val_mse: 1.6763\n",
      "Epoch 403/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5171 - mae: 1.0113 - mse: 1.5171 - val_loss: 1.6689 - val_mae: 1.0668 - val_mse: 1.6689\n",
      "Epoch 404/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5137 - mae: 1.0140 - mse: 1.5137 - val_loss: 1.6593 - val_mae: 1.0319 - val_mse: 1.6593\n",
      "Epoch 405/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5173 - mae: 1.0136 - mse: 1.5173 - val_loss: 1.6604 - val_mae: 1.0589 - val_mse: 1.6604\n",
      "Epoch 406/450\n",
      "2484/2484 [==============================] - 0s 31us/sample - loss: 1.5127 - mae: 1.0145 - mse: 1.5127 - val_loss: 1.6882 - val_mae: 1.0111 - val_mse: 1.6882\n",
      "Epoch 407/450\n",
      "2484/2484 [==============================] - 0s 42us/sample - loss: 1.5123 - mae: 1.0095 - mse: 1.5123 - val_loss: 1.6553 - val_mae: 1.0502 - val_mse: 1.6553\n",
      "Epoch 408/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5117 - mae: 1.0123 - mse: 1.5117 - val_loss: 1.6545 - val_mae: 1.0462 - val_mse: 1.6545\n",
      "Epoch 409/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5178 - mae: 1.0119 - mse: 1.5178 - val_loss: 1.6554 - val_mae: 1.0390 - val_mse: 1.6554\n",
      "Epoch 410/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5155 - mae: 1.0112 - mse: 1.5155 - val_loss: 1.6560 - val_mae: 1.0522 - val_mse: 1.6560\n",
      "Epoch 411/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5089 - mae: 1.0096 - mse: 1.5089 - val_loss: 1.6558 - val_mae: 1.0515 - val_mse: 1.6558\n",
      "Epoch 412/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5136 - mae: 1.0118 - mse: 1.5136 - val_loss: 1.6591 - val_mae: 1.0322 - val_mse: 1.6591\n",
      "Epoch 413/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5142 - mae: 1.0124 - mse: 1.5142 - val_loss: 1.6786 - val_mae: 1.0163 - val_mse: 1.6786\n",
      "Epoch 414/450\n",
      "2484/2484 [==============================] - 0s 55us/sample - loss: 1.5146 - mae: 1.0108 - mse: 1.5146 - val_loss: 1.6593 - val_mae: 1.0320 - val_mse: 1.6593\n",
      "Epoch 415/450\n",
      "2484/2484 [==============================] - 0s 41us/sample - loss: 1.5203 - mae: 1.0122 - mse: 1.5203 - val_loss: 1.6699 - val_mae: 1.0676 - val_mse: 1.6699\n",
      "Epoch 416/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5103 - mae: 1.0134 - mse: 1.5103 - val_loss: 1.6630 - val_mae: 1.0278 - val_mse: 1.6630\n",
      "Epoch 417/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5177 - mae: 1.0104 - mse: 1.5177 - val_loss: 1.6956 - val_mae: 1.0820 - val_mse: 1.6956\n",
      "Epoch 418/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 1.5185 - mae: 1.0133 - mse: 1.5185 - val_loss: 1.6555 - val_mae: 1.0509 - val_mse: 1.6555\n",
      "Epoch 419/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5136 - mae: 1.0123 - mse: 1.5136 - val_loss: 1.6994 - val_mae: 1.0059 - val_mse: 1.6994\n",
      "Epoch 420/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 1.5144 - mae: 1.0098 - mse: 1.5144 - val_loss: 1.6668 - val_mae: 1.0244 - val_mse: 1.6668\n",
      "Epoch 421/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5163 - mae: 1.0121 - mse: 1.5163 - val_loss: 1.6576 - val_mae: 1.0344 - val_mse: 1.6576\n",
      "Epoch 422/450\n",
      "2484/2484 [==============================] - 0s 50us/sample - loss: 1.5151 - mae: 1.0096 - mse: 1.5151 - val_loss: 1.6638 - val_mae: 1.0270 - val_mse: 1.6638\n",
      "Epoch 423/450\n",
      "2484/2484 [==============================] - 0s 36us/sample - loss: 1.5159 - mae: 1.0111 - mse: 1.5159 - val_loss: 1.6605 - val_mae: 1.0591 - val_mse: 1.6605\n",
      "Epoch 424/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5152 - mae: 1.0113 - mse: 1.5152 - val_loss: 1.6698 - val_mae: 1.0675 - val_mse: 1.6698\n",
      "Epoch 425/450\n",
      "2484/2484 [==============================] - 0s 54us/sample - loss: 1.5135 - mae: 1.0131 - mse: 1.5135 - val_loss: 1.6574 - val_mae: 1.0547 - val_mse: 1.6574\n",
      "Epoch 426/450\n",
      "2484/2484 [==============================] - 0s 62us/sample - loss: 1.5152 - mae: 1.0128 - mse: 1.5152 - val_loss: 1.6548 - val_mae: 1.0414 - val_mse: 1.6548\n",
      "Epoch 427/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5113 - mae: 1.0100 - mse: 1.5113 - val_loss: 1.6550 - val_mae: 1.0493 - val_mse: 1.6550\n",
      "Epoch 428/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5174 - mae: 1.0111 - mse: 1.5174 - val_loss: 1.6620 - val_mae: 1.0608 - val_mse: 1.6620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5133 - mae: 1.0106 - mse: 1.5133 - val_loss: 1.6640 - val_mae: 1.0627 - val_mse: 1.6640\n",
      "Epoch 430/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5144 - mae: 1.0128 - mse: 1.5144 - val_loss: 1.6557 - val_mae: 1.0512 - val_mse: 1.6557\n",
      "Epoch 431/450\n",
      "2484/2484 [==============================] - 0s 37us/sample - loss: 1.5159 - mae: 1.0116 - mse: 1.5159 - val_loss: 1.6569 - val_mae: 1.0540 - val_mse: 1.6569\n",
      "Epoch 432/450\n",
      "2484/2484 [==============================] - 0s 46us/sample - loss: 1.5172 - mae: 1.0136 - mse: 1.5172 - val_loss: 1.6592 - val_mae: 1.0322 - val_mse: 1.6592\n",
      "Epoch 433/450\n",
      "2484/2484 [==============================] - 0s 44us/sample - loss: 1.5129 - mae: 1.0126 - mse: 1.5129 - val_loss: 1.6867 - val_mae: 1.0118 - val_mse: 1.6867\n",
      "Epoch 434/450\n",
      "2484/2484 [==============================] - 0s 40us/sample - loss: 1.5178 - mae: 1.0103 - mse: 1.5178 - val_loss: 1.6571 - val_mae: 1.0354 - val_mse: 1.6571\n",
      "Epoch 435/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5139 - mae: 1.0113 - mse: 1.5139 - val_loss: 1.6803 - val_mae: 1.0743 - val_mse: 1.6803\n",
      "Epoch 436/450\n",
      "2484/2484 [==============================] - 0s 39us/sample - loss: 1.5184 - mae: 1.0123 - mse: 1.5184 - val_loss: 1.6852 - val_mae: 1.0126 - val_mse: 1.6852\n",
      "Epoch 437/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5154 - mae: 1.0094 - mse: 1.5154 - val_loss: 1.6666 - val_mae: 1.0246 - val_mse: 1.6666\n",
      "Epoch 438/450\n",
      "2484/2484 [==============================] - 0s 70us/sample - loss: 1.5170 - mae: 1.0124 - mse: 1.5170 - val_loss: 1.6680 - val_mae: 1.0234 - val_mse: 1.6680\n",
      "Epoch 439/450\n",
      "2484/2484 [==============================] - 0s 58us/sample - loss: 1.5121 - mae: 1.0109 - mse: 1.5121 - val_loss: 1.6682 - val_mae: 1.0233 - val_mse: 1.6682\n",
      "Epoch 440/450\n",
      "2484/2484 [==============================] - 0s 79us/sample - loss: 1.5204 - mae: 1.0111 - mse: 1.5204 - val_loss: 1.6862 - val_mae: 1.0775 - val_mse: 1.6862\n",
      "Epoch 441/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5157 - mae: 1.0131 - mse: 1.5157 - val_loss: 1.6982 - val_mae: 1.0831 - val_mse: 1.6982\n",
      "Epoch 442/450\n",
      "2484/2484 [==============================] - 0s 43us/sample - loss: 1.5164 - mae: 1.0140 - mse: 1.5164 - val_loss: 1.6552 - val_mae: 1.0499 - val_mse: 1.6552\n",
      "Epoch 443/450\n",
      "2484/2484 [==============================] - 0s 32us/sample - loss: 1.5100 - mae: 1.0089 - mse: 1.5100 - val_loss: 1.7095 - val_mae: 1.0878 - val_mse: 1.7095\n",
      "Epoch 444/450\n",
      "2484/2484 [==============================] - 0s 60us/sample - loss: 1.5228 - mae: 1.0161 - mse: 1.5228 - val_loss: 1.6552 - val_mae: 1.0499 - val_mse: 1.6552\n",
      "Epoch 445/450\n",
      "2484/2484 [==============================] - 0s 51us/sample - loss: 1.5081 - mae: 1.0072 - mse: 1.5081 - val_loss: 1.6920 - val_mae: 1.0803 - val_mse: 1.6920\n",
      "Epoch 446/450\n",
      "2484/2484 [==============================] - 0s 38us/sample - loss: 1.5135 - mae: 1.0135 - mse: 1.5135 - val_loss: 1.6559 - val_mae: 1.0377 - val_mse: 1.6559\n",
      "Epoch 447/450\n",
      "2484/2484 [==============================] - 0s 47us/sample - loss: 1.5136 - mae: 1.0108 - mse: 1.5136 - val_loss: 1.6544 - val_mae: 1.0444 - val_mse: 1.6544\n",
      "Epoch 448/450\n",
      "2484/2484 [==============================] - 0s 59us/sample - loss: 1.5158 - mae: 1.0133 - mse: 1.5158 - val_loss: 1.6696 - val_mae: 1.0222 - val_mse: 1.6696\n",
      "Epoch 449/450\n",
      "2484/2484 [==============================] - 0s 56us/sample - loss: 1.5146 - mae: 1.0109 - mse: 1.5146 - val_loss: 1.6793 - val_mae: 1.0159 - val_mse: 1.6793\n",
      "Epoch 450/450\n",
      "2484/2484 [==============================] - 0s 34us/sample - loss: 1.5170 - mae: 1.0128 - mse: 1.5170 - val_loss: 1.6587 - val_mae: 1.0327 - val_mse: 1.6587\n"
     ]
    }
   ],
   "source": [
    "trained_weight = ks.get_weights()[0]\n",
    "trained_bias = ks.get_weights()[1]\n",
    "\n",
    "EPOCHS = 450\n",
    "history = ks.fit(X_train,\n",
    "                 y_train,\n",
    "                 epochs = EPOCHS,\n",
    "                 batch_size = 128,\n",
    "                 validation_split = 0.2,\n",
    "                 verbose = 1)\n",
    "\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist['mse']\n",
    "epochs = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "    \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "    # Label the axes.\n",
    "    plt.xlabel(\"feature\")\n",
    "    plt.ylabel(\"label\")\n",
    "    # Plot the feature values vs. label values.\n",
    "    plt.scatter(feature, label)\n",
    "    # Create a red line representing the model. The red line starts\n",
    "    # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "    plt.plot(feature.tolist(), label.tolist(), c='r')\n",
    "    # Render the scatter plot and the red line.\n",
    "    plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.97, mse.max()])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature = X['all'].copy()\n",
    "#label = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_the_model(trained_weight, trained_bias, feature, label)\n",
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              loss         mae          mse    val_loss     val_mae  \\\n",
      "count   450.000000  450.000000   450.000000  450.000000  450.000000   \n",
      "mean      5.764526    1.033864     5.764526    1.668625    1.048493   \n",
      "std      90.122018    0.470257    90.122031    0.020291    0.021683   \n",
      "min       1.493179    0.980247     1.493179    1.640283    0.985009   \n",
      "25%       1.512879    1.010549     1.512879    1.655655    1.034440   \n",
      "50%       1.514793    1.011831     1.514794    1.660323    1.048182   \n",
      "75%       1.517012    1.013086     1.517012    1.673043    1.062642   \n",
      "max    1913.292818   10.987158  1913.293091    1.813143    1.108396   \n",
      "\n",
      "          val_mse  \n",
      "count  450.000000  \n",
      "mean     1.668625  \n",
      "std      0.020291  \n",
      "min      1.640283  \n",
      "25%      1.655655  \n",
      "50%      1.660323  \n",
      "75%      1.673043  \n",
      "max      1.813143  \n"
     ]
    }
   ],
   "source": [
    "print(hist.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(a,b):\n",
    "    #print('pred :',a,'actual :',b)\n",
    "    if a == b:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.02681707664798261\n",
      "dt 0.07940946441270735\n",
      "rf 0.49071921897746024\n",
      "vr 0.27200788477068794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ks_test = ks.evaluate(X_train, y_train,verbose=0)\\nprint('ks',ks_test[1])\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('lr',lr.score(X_train, y_train))\n",
    "print('dt',dt.score(X_train, y_train))\n",
    "print('rf',rf.score(X_train, y_train))\n",
    "print('vr',vr.score(X_train, y_train))\n",
    "'''ks_test = ks.evaluate(X_train, y_train,verbose=0)\n",
    "print('ks',ks_test[1])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_pred_test(result,num):\n",
    "    score = check(result,y_test.loc[num])\n",
    "    return score\n",
    "\n",
    "def model_pred_test(model):\n",
    "    pred = []\n",
    "    numbers = X_test.index\n",
    "    random_nums = random.choices(numbers, k=50)\n",
    "    for i in random_nums:\n",
    "        p = X_test.loc[i].tolist()\n",
    "        result = model.predict([p]).flatten().round()\n",
    "        prediction = cycle_pred_test(result,i)\n",
    "        pred.append(prediction)\n",
    "    dz = pd.DataFrame(pred)\n",
    "    #df = pd.DataFrame(prob)\n",
    "    c = str(float(dz.sum().values / 50))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(index=range(10),columns=['vr','dt','rf','ks'])\n",
    "test_results = test_results.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    v_s = model_pred_test(vr)\n",
    "    test_results.at[i,'vr'] = v_s\n",
    "    d_s = model_pred_test(dt)\n",
    "    test_results.at[i,'dt'] = d_s\n",
    "    r_s = model_pred_test(rf)\n",
    "    test_results.at[i,'rf'] = r_s\n",
    "    k_s = model_pred_test(ks)\n",
    "    test_results.at[i,'ks'] = k_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vr</th>\n",
       "      <th>dt</th>\n",
       "      <th>rf</th>\n",
       "      <th>ks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.069314</td>\n",
       "      <td>0.055176</td>\n",
       "      <td>0.029889</td>\n",
       "      <td>0.049171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              vr         dt         rf         ks\n",
       "count  10.000000  10.000000  10.000000  10.000000\n",
       "mean    0.346000   0.310000   0.266000   0.352000\n",
       "std     0.069314   0.055176   0.029889   0.049171\n",
       "min     0.260000   0.240000   0.220000   0.300000\n",
       "25%     0.285000   0.265000   0.240000   0.320000\n",
       "50%     0.340000   0.310000   0.270000   0.340000\n",
       "75%     0.395000   0.355000   0.295000   0.375000\n",
       "max     0.440000   0.400000   0.300000   0.440000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_prob_test(num,model):\n",
    "    p = X_test.iloc[num].tolist()\n",
    "    e = model.predict([p]).flatten()\n",
    "    e = e[0]\n",
    "    if e < 1:\n",
    "        e = 0\n",
    "    elif e < 2:\n",
    "        e = 1\n",
    "    return e\n",
    "\n",
    "def model_pred_test(model):\n",
    "    b = []\n",
    "    prob = []\n",
    "    random_nums = np.random.randint(low=1, high=58, size=(20))\n",
    "    for i in random_nums:\n",
    "        prob.append(cycle_prob_test(i,model))\n",
    "    df = pd.DataFrame(prob)\n",
    "    df = df.values\n",
    "    print('scores :\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'models/cpl_score_regressor.sav'\n",
    "pickle.dump(vr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import cpl_main as cpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2020'\n",
    "team_ref = pd.read_csv('datasets/teams.csv')\n",
    "results = pd.read_csv(f'datasets/{year}/cpl-{year}-results.csv')\n",
    "stats = pd.read_csv(f'datasets/{year}/cpl-{year}-stats.csv')\n",
    "player_info = pd.read_csv(f'datasets/{year}/player-{year}-info.csv')\n",
    "results_brief = pd.read_csv(f'datasets/{year}/cpl-{year}-results_brief.csv')\n",
    "team_stats = pd.read_csv(f'datasets/{year}/cpl-{year}-team_stats.csv')\n",
    "schedule = pd.read_csv(f'datasets/{year}/cpl-{year}-schedule.csv')\n",
    "rated_forwards = pd.read_csv(f'datasets/{year}/cpl-{year}-forwards.csv')\n",
    "rated_midfielders = pd.read_csv(f'datasets/{year}/cpl-{year}-midfielders.csv')\n",
    "rated_defenders = pd.read_csv(f'datasets/{year}/cpl-{year}-defenders.csv')\n",
    "rated_keepers = pd.read_csv(f'datasets/{year}/cpl-{year}-keepers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forge FC Cavalry FC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todd/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  res_values = method(rvalues)\n"
     ]
    }
   ],
   "source": [
    "#model_pred_test(cpl_classifier_model)\n",
    "\n",
    "# home side\n",
    "q1 = schedule.iloc[0]['home']\n",
    "# away side\n",
    "q2 = schedule.iloc[0]['away']\n",
    "print(q1,q2)\n",
    "\n",
    "compare = cpl.get_team_comparison(results_brief,q1,q2)\n",
    "\n",
    "t1_x, t1_y = cpl.get_NB_data(compare,q1)\n",
    "t2_x, t2_y = cpl.get_NB_data(compare,q2)\n",
    "\n",
    "game_info = schedule[schedule['home'] == q1]\n",
    "game_info = game_info[game_info['away'] == q2]\n",
    "game_info\n",
    "\n",
    "game = game_info.iloc[0]['game']\n",
    "game_h = cpl.get_home_away_comparison(stats,game,q1)\n",
    "game_a = cpl.get_home_away_comparison(stats,game,q2)\n",
    "\n",
    "home_roster = cpl.get_compare_roster(results,q1,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)\n",
    "away_roster = cpl.get_compare_roster(results,q2,team_stats,team_ref,rated_forwards,rated_midfielders,rated_defenders,rated_keepers,player_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.33\n"
     ]
    }
   ],
   "source": [
    "def get_overall_roster(game_roster):\n",
    "    b = []\n",
    "    for i in range(game_roster.shape[0]):\n",
    "        b.append(game_roster.iloc[i]['overall']) # get the player overall score for each player in the game\n",
    "    if len(b) < 16:\n",
    "        i = int(16 - len(b))\n",
    "        for j in range(0,i):\n",
    "            b.append(0)\n",
    "    db = pd.DataFrame(b[0:14])\n",
    "    db = db.T\n",
    "    return db\n",
    "\n",
    "q1_roster = get_overall_roster(home_roster)\n",
    "#print(q1_roster)\n",
    "\n",
    "q2_roster = get_overall_roster(away_roster)\n",
    "#print(q2_roster)\n",
    "\n",
    "def roster_regressor_pred(model,array):\n",
    "    prediction = model.predict([array]).flatten()\n",
    "    df = pd.DataFrame(prediction)\n",
    "    return df\n",
    "\n",
    "home_win, draw, away_win = cpl.get_match_prediction(q1,q2,t1_x,t1_y,t2_x,t2_y)\n",
    "print(home_win, away_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34\n",
      "0.35\n",
      "\n",
      " Forge FC \n",
      "win probability:  0.34\n",
      "\n",
      " Cavalry FC \n",
      "win probability:  0.35\n",
      "\n",
      "Draw probability:  0.31\n"
     ]
    }
   ],
   "source": [
    "classifier = 'models/cpl_roster_classifier.sav'\n",
    "cpl_classifier_model = pickle.load(open(classifier, 'rb'))\n",
    "\n",
    "home_win_new, away_win_new, draw_new = cpl.get_final_game_prediction(cpl_classifier_model,q1_roster,q2_roster,home_win,away_win,draw)\n",
    "\n",
    "print(home_win_new)\n",
    "print(away_win_new)\n",
    "\n",
    "print('\\n',q1,'\\nwin probability: ', round(home_win_new,2))\n",
    "\n",
    "print('\\n',q2,'\\nwin probability: ', round(away_win_new,2))\n",
    "\n",
    "print('\\nDraw probability: ', round(draw_new,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/cpl_score_regressor.sav'\n",
    "cpl_score_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_score_prediction(model,q1_roster,q2_roster,home_win_new,away_win_new):\n",
    "\n",
    "    def roster_pred(model,array):\n",
    "        prediction = model.predict([array]).flatten()\n",
    "        return prediction\n",
    "\n",
    "    def final_score_fix(home_score,away_score,home_win_new,away_win_new):\n",
    "        if home_win_new > away_win_new and home_score < away_score: # fix the score prediction - if the probability of home win > away win and score doesn't reflect it\n",
    "            old_home = home_score\n",
    "            home_score = away_score # change the predicted score to reflect that\n",
    "            away_score = old_home\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score > away_score: # else the probability of home win < away win\n",
    "            old_away = away_score\n",
    "            away_score = home_score # change the predicted score to reflect that\n",
    "            home_score = away_score\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new < away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        elif home_win_new > away_win_new and home_score == away_score:\n",
    "            home_win_new = away_win_new\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "        else:\n",
    "            return home_score,away_score,home_win_new,away_win_new\n",
    "\n",
    "    def score(num): #improve this later for greater predictions\n",
    "        new_score = int(round(num,0)) # convert the float value to int and round it\n",
    "        return new_score\n",
    "\n",
    "    q1_pred = roster_pred(model,q1_roster)\n",
    "    q1_s = score(q1_pred[0])\n",
    "    q2_pred = roster_pred(model,q2_roster)\n",
    "    q2_s = score(q2_pred[0])\n",
    "    home_score, away_score, home_win_new, away_win_new = final_score_fix(q1_s, q2_s,home_win_new,away_win_new)\n",
    "    return home_score,away_score, home_win_new, away_win_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home 2 away 2\n"
     ]
    }
   ],
   "source": [
    "home_score, away_score, home_win_new, away_win_new = get_final_score_prediction(cpl_score_model,q1_roster,q2_roster,home_win_new,away_win_new)\n",
    "print('home',home_score,'away', away_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = q1_roster.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = l.values.tolist()\n",
    "l = l[0]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "average = statistics.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4421428516507149"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7099999785423279,\n",
       " 0.8899999856948853,\n",
       " 0.7400000095367432,\n",
       " 0.7099999785423279,\n",
       " 0.33000001311302185,\n",
       " 0.7699999809265137,\n",
       " 0.5199999809265137,\n",
       " 0.5,\n",
       " 0.47999998927116394,\n",
       " 0.33000001311302185,\n",
       " 0.20999999344348907,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-136c17fc3cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "l.extend(average)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
